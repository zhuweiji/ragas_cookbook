Question
----------------------------------------
AMI ID System Manager?

Documents
----------------------------------------
You can programmatically retrieve the Amazon Machine Image \(AMI\) ID for Amazon EKS optimized AMIs by querying the AWS Systems Manager Parameter Store API\. This parameter eliminates the need for you to manually look up Amazon EKS optimized AMI IDs\. For more information about the Systems Manager Parameter Store API, see [GetParameter](https://docs.aws.amazon.com/systems-manager/latest/APIReference/API_GetParameter.html)\. The [IAM principal](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html) that you use must have the `ssm:GetParameter` IAM permission to retrieve the Amazon EKS optimized AMI metadata\.  
You can retrieve the image ID of the latest recommended Amazon EKS optimized Windows AMI with the following command by using the sub\-parameter `image_id`\. You can replace `1.29` with any supported Amazon EKS version and can replace `region-code` with an [Amazon EKS supported Region](https://docs.aws.amazon.com/general/latest/gr/eks.html) for which you want the AMI ID\. Replace `Core` with `Full` to see the Windows Server full AMI ID\. For Kubernetes version `1.24` or later, you can replace `2019` with `2022` to see the Windows Server 2022 AMI ID\.  
```
aws ssm get-parameter --name /aws/service/ami-windows-latest/Windows_Server-2019-English-Core-EKS_Optimized-1.29/image_id --region region-code --query "Parameter.Value" --output text
```  
An example output is as follows\.  
```
ami-1234567890abcdef0
```

AWS Identity and Access Management \(IAM\) is an AWS service that helps an administrator securely control access to AWS resources\. IAM administrators control who can be *authenticated* \(signed in\) and *authorized* \(have permissions\) to use Amazon EKS resources\. IAM is an AWS service that you can use with no additional charge\.

Amazon EKS maintains relationships with a network of partners that offer support for compatible OIDC identity providers\. Refer to the following partners' documentation for details on how to integrate the identity provider with Amazon EKS\.  
|  Partner  |  Product  |  Documentation  |
| --- | --- | --- |
|  PingIdentity  |  [PingOne for Enterprise](https://docs.pingidentity.com/r/en-us/pingoneforenterprise/p14e_landing)  |  [Installation instructions](https://docs.pingidentity.com/r/en-us/solution-guides/htg_config_oidc_authn_aws_eks_custers)  |  
Amazon EKS aims to give you a wide selection of options to cover all use cases\. If you develop a commercially supported OIDC compatible identity provider that is not listed here, then contact our partner team at [aws\-container\-partners@amazon\.com](mailto:aws-container-partners@amazon.com) for more information\.

In addition to the previous list of Amazon EKS add\-ons, you can also add a wide selection of operational software Amazon EKS add\-ons from independent software vendors\. Choose an add\-on to learn more about it and its installation requirements\.  
[![AWS Videos](http://img.youtube.com/vi/https://www.youtube.com/embed/IIPj119mspc/0.jpg)](http://www.youtube.com/watch?v=https://www.youtube.com/embed/IIPj119mspc)

Answer
----------------------------------------
The IAM (Identity and Access Management) system manager in the context provided is used to programmatically retrieve the Amazon Machine Image (AMI) ID for Amazon EKS optimized AMIs. The IAM principal needs the `ssm:GetParameter` permission to retrieve the Amazon EKS optimized AMI metadata.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not directly provide the requested information, which is the meaning or explanation of "AMI ID System Manager." Instead, it describes the IAM system manager's role in retrieving the Amazon Machine Image (AMI) ID, which might be a partial answer but does not fully address the question.

Evaluation_score
----------------------------------------
2

Ground truth answer
The Amazon EKS optimized AMI IDs can be retrieved using the AWS Systems Manager Parameter Store API.

====================================================================================================

Question
----------------------------------------
IAM permission AMI retrieval?

Documents
----------------------------------------
An [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) is an entity within your AWS account that has specific permissions\.

You can programmatically retrieve the Amazon Machine Image \(AMI\) ID for Amazon EKS optimized AMIs by querying the AWS Systems Manager Parameter Store API\. This parameter eliminates the need for you to manually look up Amazon EKS optimized AMI IDs\. For more information about the Systems Manager Parameter Store API, see [GetParameter](https://docs.aws.amazon.com/systems-manager/latest/APIReference/API_GetParameter.html)\. The [IAM principal](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html) that you use must have the `ssm:GetParameter` IAM permission to retrieve the Amazon EKS optimized AMI metadata\.  
You can retrieve the image ID of the latest recommended Amazon EKS optimized Windows AMI with the following command by using the sub\-parameter `image_id`\. You can replace `1.29` with any supported Amazon EKS version and can replace `region-code` with an [Amazon EKS supported Region](https://docs.aws.amazon.com/general/latest/gr/eks.html) for which you want the AMI ID\. Replace `Core` with `Full` to see the Windows Server full AMI ID\. For Kubernetes version `1.24` or later, you can replace `2019` with `2022` to see the Windows Server 2022 AMI ID\.  
```
aws ssm get-parameter --name /aws/service/ami-windows-latest/Windows_Server-2019-English-Core-EKS_Optimized-1.29/image_id --region region-code --query "Parameter.Value" --output text
```  
An example output is as follows\.  
```
ami-1234567890abcdef0
```

+ **Publisher** – Rafay
+ **Name** – `rafay-systems_rafay-operator`
+ **Namespace** – `rafay-system`
+ **Service account name** – A service account isn't used with this add\-on\.
+ **AWS managed IAM policy** – A managed policy isn't used with this add\-on\.
+ **Custom IAM permissions** – Custom permissions aren't used with this add\-on\.
+ **Setup and usage instructions** – See [Installing the Rafay Amazon EKS Add\-on](https://docs.rafay.co/clusters/import/eksaddon/) in the Rafay documentation\.

The Amazon EBS CSI plugin requires IAM permissions to make calls to AWS APIs on your behalf\. For more information, see [Set up driver permission](https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/install.md#set-up-driver-permissions) on GitHub\.  
**Note**
Pods will have access to the permissions that are assigned to the IAM role unless you block access to IMDS\. For more information, see [Security best practices for Amazon EKS](security-best-practices.md)\.  
**Prerequisites**
+ An existing cluster\.
+ An existing AWS Identity and Access Management \(IAM\) OpenID Connect \(OIDC\) provider for your cluster\. To determine whether you already have one, or to create one, see [Create an IAM OIDC provider for your cluster](enable-iam-roles-for-service-accounts.md)\.  
The following procedure shows you how to create an IAM role and attach the AWS managed policy to it\. You can use `eksctl`, the AWS Management Console, or the AWS CLI\.  
**Note**
The specific steps in this procedure are written for using the driver as an Amazon EKS add\-on\. Different steps are needed to use the driver as a self\-managed add\-on\. For more information, see [Set up driver permissions](https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/install.md#set-up-driver-permissions) on GitHub\.  
------

Answer
----------------------------------------
The IAM permission required for AMI retrieval is `ssm:GetParameter`. This permission is needed to retrieve the Amazon EKS optimized AMI metadata from the AWS Systems Manager Parameter Store API.

Evaluation
----------------------------------------
Score (1-5): 4
Answered: Yes.

The answer does provide the correct IAM permission required for AMI retrieval, but it also includes extra information about the model and token usage which is not relevant to the question.

Evaluation_score
----------------------------------------
4

Ground truth answer
To retrieve the Amazon EKS optimized AMI metadata, you must be logged into the AWS CLI using an IAM principal that has the `ssm:GetParameter` IAM permission.

====================================================================================================

Question
----------------------------------------
AWS CLI command AMI?

Documents
----------------------------------------
The [AWS CLI](https://aws.amazon.com/cli/) is a command line tool for working with AWS services, including Amazon EKS\. It is also used to authenticate IAM users or roles for access to the Amazon EKS cluster and other AWS resources from your local machine\. To provision resources in AWS from the command line, you need to obtain an AWS access key ID and secret key to use in the command line\. Then you need to configure these credentials in the AWS CLI\. If you haven't already installed the AWS CLI, see [Install or update the latest version of the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) in the *AWS Command Line Interface User Guide*\.

**Prerequisites**
+ Version `2.12.3` or later or version `1.27.160` or later of the AWS Command Line Interface \(AWS CLI\) installed and configured on your device or AWS CloudShell\. To check your current version, use `aws --version | cut -d / -f2 | cut -d ' ' -f1`\. Package managers such `yum`, `apt-get`, or Homebrew for macOS are often several versions behind the latest version of the AWS CLI\. To install the latest version, see [Installing, updating, and uninstalling the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) and [Quick configuration with aws configure](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-config) in the *AWS Command Line Interface User Guide*\. The AWS CLI version that is installed in AWS CloudShell might also be several versions behind the latest version\. To update it, see [Installing AWS CLI to your home directory](https://docs.aws.amazon.com/cloudshell/latest/userguide/vm-specs.html#install-cli-software) in the *AWS CloudShell User Guide*\.
+ Permission to use the `eks:DescribeCluster` API action for the cluster that you specify\. For more information, see [Amazon EKS identity\-based policy examples](security_iam_id-based-policy-examples.md)\.  
**To create your `kubeconfig` file with the AWS CLI**  
1. Create or update a `kubeconfig` file for your cluster\. Replace *region\-code* with the AWS Region that your cluster is in and replace *my\-cluster* with the name of your cluster\.  
```
aws eks update-kubeconfig --region region-code --name my-cluster
```  
By default, the resulting configuration file is created at the default `kubeconfig` path \(`.kube`\) in your home directory or merged with an existing `config` file at that location\. You can specify another path with the **\-\-kubeconfig** option\.  
You can specify an IAM role ARN with the **\-\-role\-arn** option to use for authentication when you issue `kubectl` commands\. Otherwise, the [IAM principal](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html) in your default AWS CLI or SDK credential chain is used\. You can view your default AWS CLI or SDK identity by running the `aws sts get-caller-identity` command\.  
For all available options, run the `aws eks update-kubeconfig help` command or see [https://docs.aws.amazon.com/cli/latest/reference/eks/update-kubeconfig.html](https://docs.aws.amazon.com/cli/latest/reference/eks/update-kubeconfig.html) in the *AWS CLI Command Reference*\.  
1. Test your configuration\.  
```
kubectl get svc
```  
An example output is as follows\.  
```
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
svc/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   1m
```  
If you receive any authorization or resource type errors, see [Unauthorized or access denied \(`kubectl`\)](troubleshooting.md#unauthorized) in the troubleshooting topic\.

You can programmatically retrieve the Amazon Machine Image \(AMI\) ID for Amazon EKS optimized AMIs by querying the AWS Systems Manager Parameter Store API\. This parameter eliminates the need for you to manually look up Amazon EKS optimized AMI IDs\. For more information about the Systems Manager Parameter Store API, see [GetParameter](https://docs.aws.amazon.com/systems-manager/latest/APIReference/API_GetParameter.html)\.  
**To retrieve an AMI ID for Amazon EKS optimized AMIs using the AWS CLI**  
1. Determine the region your node instance will be deployed in, such as `us-east-1`\.  
1. Determine the type of AMI you need\. For information about the types of Amazon EC2 instances, see [Instance Types](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html)\.
+ `amazon-linux-2` is for Amazon Linux 2 \(AL2\) `x86` based instances\.
+ `amazon-linux-2-arm64` is for AL2 ARM instances, such as [AWS Graviton](https://aws.amazon.com/ec2/graviton/) based instances\.
+ `amazon-linux-2-gpu` is for AL2 [GPU accelerated instances](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html#gpu-instances)\.
+ `amazon-linux-2023/x86_64/standard` is for Amazon Linux 2023 \(AL2023\) `x86` based instances\.
+ `amazon-linux-2023/arm64/standard` is for AL2023 ARM instances\.  
1. Determine the Kubernetes version of the cluster your node will be attached to, such as 1\.29\.  
1. Run the following AWS CLI command to retrieve the appropriate AMI ID\. Replace the AWS Region, Kubernetes version, and platform as appropriate\. You must be logged into the AWS CLI using an [IAM principal](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html) that has the `ssm:GetParameter` IAM permission to retrieve the Amazon EKS optimized AMI metadata\.  
```
aws ssm get-parameter --name /aws/service/eks/optimized-ami/1.29/amazon-linux-2/recommended/image_id \
--region region-code --query "Parameter.Value" --output text
```  
An example output is as follows\.  
```
ami-1234567890abcdef0
```

You can create the node IAM role with the AWS Management Console or the AWS CLI\.  
------

Answer
----------------------------------------
Context is irrelevant to the question.

or

I do not know the answer. Reason: The provided context does not contain information about an AWS CLI command for AMI.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not provide the AWS CLI command for working with Amazon Machine Images (AMIs).

Evaluation_score
----------------------------------------
2

Ground truth answer
The AWS CLI command for retrieving the AMI ID is `aws ssm get-parameter --name /aws/service/eks/optimized-ami/1.29/amazon-linux-2/recommended/image_id --region region-code --query "Parameter.Value" --output text`.

====================================================================================================

Question
----------------------------------------
EKS AMI types?

Documents
----------------------------------------
You can deploy nodes with pre\-built Amazon EKS optimized [Amazon Machine Images](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html) \(AMIs\) or your own custom AMIs\. For information about each type of Amazon EKS optimized AMI, see one of the following topics\. For instructions on how to create your own custom AMI, see [Amazon EKS optimized Amazon Linux AMI build script](eks-ami-build-scripts.md)\.  
**Topics**
+ [Amazon EKS ended support for `Dockershim`](dockershim-deprecation.md)
+ [Amazon EKS optimized Amazon Linux AMIs](eks-optimized-ami.md)
+ [Amazon EKS optimized Bottlerocket AMIs](eks-optimized-ami-bottlerocket.md)
+ [Amazon EKS optimized Ubuntu Linux AMIs](eks-partner-amis.md)
+ [Amazon EKS optimized Windows AMIs](eks-optimized-windows-ami.md)

The following admission controllers are enabled for all `1.25` platform versions: `CertificateApproval`, `CertificateSigning`, `CertificateSubjectRestriction`, `DefaultIngressClass`, `DefaultStorageClass`, `DefaultTolerationSeconds`, `ExtendedResourceToleration`, `LimitRanger`, `MutatingAdmissionWebhook`, `NamespaceLifecycle`, `NodeRestriction`, `PersistentVolumeClaimResize`, `Priority`, `PodSecurity`, `ResourceQuota`, `RuntimeClass`, `ServiceAccount`, `StorageObjectInUseProtection`, `TaintNodesByCondition`, and `ValidatingAdmissionWebhook`\.  
|  Kubernetes version  |  Amazon EKS platform version  |  Release notes  |  Release date  |
| --- | --- | --- | --- |
|  `1.25.11`  |  `eks-local-outposts.6`  |  New platform version with security fixes and enhancements\. `kube-proxy` updated to `v1.25.11`\. Amazon VPC CNI plugin for Kubernetes updated to `v1.13.2`\.   | July 14, 2023 |
|  `1.25.9`  |  `eks-local-outposts.5`  |  New platform version with security fixes and enhancements\.  | July 13, 2023 |
|  `1.25.6`  |  `eks-local-outposts.4`  |  Updated Bottlerocket version to `1.13.2`  | May 2, 2023 |
|  `1.25.6`  |  `eks-local-outposts.3`  |  Amazon EKS control plane instance operating system updated to Bottlerocket version `v1.13.1` and Amazon VPC CNI plugin for Kubernetes updated to version `v1.12.6`\.  | April 14, 2023 |
|  `1.25.6`  |  `eks-local-outposts.2`  |  Improved diagnostics collection for Kubernetes control plane instances\.  | March 8, 2023 |
|  `1.25.6`  |  `eks-local-outposts.1`  | Initial release of Kubernetes version `1.25` for local Amazon EKS clusters on Outposts\. | March 1, 2023 |

To use EKS Pod Identities, the cluster must have a platform version that is the same or later than the version listed in the following table, or a Kubernetes version that is later than the versions listed in the table\.  
| Kubernetes version | Platform version |
| --- | --- |
| 1\.28 | eks\.4 |
| 1\.27 | eks\.8 |
| 1\.26 | eks\.9 |
| 1\.25 | eks\.10 |
| 1\.24 | eks\.13 |

This topic covers some common errors that you may see while using Amazon EKS with IAM and how to work around them\.

Answer
----------------------------------------
Answer: The EKS AMI types are Amazon EKS optimized Amazon Linux AMIs, Amazon EKS optimized Bottlerocket AMIs, Amazon EKS optimized Ubuntu Linux AMIs, and Amazon EKS optimized Windows AMIs. For more information, see the following topics:
- Amazon EKS optimized Amazon Linux AMIs
- Amazon EKS optimized Bottlerocket AMIs
- Amazon EKS optimized Ubuntu Linux AMIs
- Amazon EKS optimized Windows AMIs.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The answer provides a clear and accurate response to the question about EKS AMI types. It includes specific examples of each type of AMI, which demonstrates a strong understanding of the topic. The format and tone of the answer are also professional and helpful. Overall, this answer meets all the criteria for a complete and accurate response.

Evaluation_score
----------------------------------------
5

Ground truth answer
The types of Amazon EKS optimized AMIs include `amazon-linux-2` for Amazon Linux 2 (AL2) x86 based instances, `amazon-linux-2-arm64` for AL2 ARM instances, `amazon-linux-2-gpu` for AL2 GPU accelerated instances, and `amazon-linux-2023/x86_64/standard` and `amazon-linux-2023/arm64/standard` for Amazon Linux 2023 (AL2023) instances.

====================================================================================================

Question
----------------------------------------
eks subnet tags?

Documents
----------------------------------------
You can use *tags* to help you manage your Amazon EKS resources\. This topic provides an overview of the tags function and shows how you can create tags\.  
**Topics**
+ [Tag basics](#tag-basics)
+ [Tagging your resources](#tag-resources)
+ [Tag restrictions](#tag-restrictions)
+ [Tagging your resources for billing](#tag-resources-for-billing)
+ [Working with tags using the console](#tag-resources-console)
+ [Working with tags using the CLI, API, or `eksctl`](#tag-resources-api-sdk)  
**Note**
Tags are a type of metadata that's separate from Kubernetes labels and annotations\. For more information about these other metadata types, see the following sections in the Kubernetes documentation:
[Labels and Selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
[Annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/)

Several networking add\-ons are available for your Amazon EKS cluster\.

Amazon EKS aligns with the general cluster architecture of Kubernetes\. For more information, see [Kubernetes Components](https://kubernetes.io/docs/concepts/overview/components/) in the Kubernetes documentation\. The following sections summarize some extra architecture details for Amazon EKS\.

When you create a cluster, you specify a [VPC](https://docs.aws.amazon.com/vpc/latest/userguide/configure-your-vpc.html) and at least two subnets that are in different Availability Zones\. This topic provides an overview of Amazon EKS specific requirements and considerations for the VPC and subnets that you use with your cluster\. If you don't have a VPC to use with Amazon EKS, you can [create one using an Amazon EKS provided AWS CloudFormation template](creating-a-vpc.md)\. If you're creating a local or extended cluster on AWS Outposts, see [Amazon EKS local cluster VPC and subnet requirements and considerations](eks-outposts-vpc-subnet-requirements.md) instead of this topic\.

Answer
----------------------------------------
Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide any useful information related to the question about EKS subnet tags.

Evaluation_score
----------------------------------------
1

Ground truth answer
Private subnets must be tagged with `kubernetes.io/role/internal-elb: 1` and public subnets with `kubernetes.io/role/elb: 1` for successful auto discovery by the AWS Load Balancer Controller in an Amazon EKS cluster. However, it is recommended to explicitly add the private or public role tags rather than relying on examining the route table of the cluster VPC subnets.

====================================================================================================

Question
----------------------------------------
lb controller version?

Documents
----------------------------------------
This topic describes how to migrate from deprecated controller versions\. More specifically, it describes how to remove deprecated versions of the AWS Load Balancer Controller\.
+ Deprecated versions cannot be upgraded\. They must be removed and a current version of the LBC installed\.
+ <a name="lbc-deprecated-list"></a>Deprecated versions include:
+ AWS ALB Ingress Controller for Kubernetes \("Ingress Controller"\), a predecessor to the AWS Load Balancer Controller\.
+ Any `0.1.x` version of the AWS Load Balancer Controller

This topic describes how to install the controller by downloading and applying Kubernetes manifests\. You can view the full [documentation](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/) for the controller on GitHub\.  
In the following steps, replace the `example values` with your own values\.

The following Kubernetes versions are currently available in Amazon EKS standard support:
+ `1.29`
+ `1.28`
+ `1.27`
+ `1.26`
+ `1.25`  
For important changes to be aware of for each version in standard support, see [Release notes for standard support versions](kubernetes-versions-standard.md)\.

**Note**
Amazon EKS accelerated AMIs based on AL2023 will be available at a later date\. If you have accelerated workloads, you should continue to use the AL2 accelerated AMI or Bottlerocket\.  
The Amazon EKS optimized accelerated Amazon Linux AMI is built on top of the standard Amazon EKS optimized Amazon Linux AMI\. It's configured to serve as an optional image for Amazon EKS nodes to support GPU, [Inferentia](https://aws.amazon.com/machine-learning/inferentia/), and [Trainium](https://aws.amazon.com/machine-learning/trainium/) based workloads\.  
In addition to the standard Amazon EKS optimized AMI configuration, the accelerated AMI includes the following:
+ NVIDIA drivers
+ The `nvidia-container-runtime` \(as the default runtime\)
+ AWS Neuron container runtime  
For a list of the latest components included in the accelerated AMI, see the `amazon-eks-ami` [Releases](https://github.com/awslabs/amazon-eks-ami/releases) on GitHub\.  
**Note**
The Amazon EKS optimized accelerated AMI only supports GPU and Inferentia based instance types\. Make sure to specify these instance types in your node AWS CloudFormation template\. By using the Amazon EKS optimized accelerated AMI, you agree to [NVIDIA's user license agreement \(EULA\)](https://www.nvidia.com/en-us/drivers/nvidia-license/)\.
The Amazon EKS optimized accelerated AMI was previously referred to as the *Amazon EKS optimized AMI with GPU support*\.
Previous versions of the Amazon EKS optimized accelerated AMI installed the `nvidia-docker` repository\. The repository is no longer included in Amazon EKS AMI version `v20200529` and later\.  
**To enable GPU based workloads**  
The following procedure describes how to run a workload on a GPU based instance with the Amazon EKS optimized accelerated AMI\. For other options, see the following references:
+ For more information about using Inferentia based workloads, see [Machine learning inference using AWS Inferentia](inferentia-support.md)\.
+ For more information about using Neuron, see [Containers \- Kubernetes \- Getting Started](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/containers/kubernetes-getting-started.html) in the *AWS Neuron Documentation*\.  
1. After your GPU nodes join your cluster, you must apply the [NVIDIA device plugin for Kubernetes](https://github.com/NVIDIA/k8s-device-plugin) as a DaemonSet on your cluster\. Replace `vX.X.X` with your desired [NVIDIA/k8s\-device\-plugin](https://github.com/NVIDIA/k8s-device-plugin/releases) version before running the following command\.  
```
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/vX.X.X/nvidia-device-plugin.yml
```  
1. You can verify that your nodes have allocatable GPUs with the following command\.  
```
kubectl get nodes "-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu"
```  
**To deploy a Pod to test that your GPU nodes are configured properly**  
1. Create a file named `nvidia-smi.yaml` with the following contents\. Replace `tag` with your desired tag for [https://hub.docker.com/r/nvidia/cuda/tags](https://hub.docker.com/r/nvidia/cuda/tags)\. This manifest launches an [https://developer.nvidia.com/cuda-zone](https://developer.nvidia.com/cuda-zone) container that runs `nvidia-smi` on a node\.  
```
apiVersion: v1
kind: Pod
metadata:
name: nvidia-smi
spec:
restartPolicy: OnFailure
containers:
- name: nvidia-smi
image: nvidia/cuda:tag
args:
- "nvidia-smi"
resources:
limits:
nvidia.com/gpu: 1
```  
1. Apply the manifest with the following command\.  
```
kubectl apply -f nvidia-smi.yaml
```  
1. After the Pod has finished running, view its logs with the following command\.  
```
kubectl logs nvidia-smi
```  
An example output is as follows\.  
```
Mon Aug  6 20:23:31 20XX
+-----------------------------------------------------------------------------+
| NVIDIA-SMI XXX.XX                 Driver Version: XXX.XX                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |
| N/A   46C    P0    47W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

Answer
----------------------------------------
The context provided discusses deprecated versions of the AWS Load Balancer Controller, Kubernetes versions in Amazon EKS standard support, and steps to enable GPU-based workloads on Amazon EKS. However, it does not mention the version of the lb controller. Therefore, the context is irrelevant to the question.

Reason: Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide the lb controller version, nor does it point to where in the context the information can be found. It simply states that the context does not mention the version.

Evaluation_score
----------------------------------------
1

Ground truth answer
It is recommended to use version 2.7.2 or later of the AWS Load Balancer Controller on your Amazon EKS cluster for deploying the controller.

====================================================================================================

Question
----------------------------------------
Ingress group order

Documents
----------------------------------------
You can modify some of the configurations of a managed node group\.  
**To edit a node group configuration**  
1. Open the Amazon EKS console at [https://console\.aws\.amazon\.com/eks/home\#/clusters](https://console.aws.amazon.com/eks/home#/clusters)\.  
1. Choose the cluster that contains the node group to edit\.  
1. Select the **Compute** tab\.  
1. Select the node group to edit, and then choose **Edit**\.  
1. \(Optional\) On the **Edit node group** page, do the following:  
1. Edit the **Node group scaling configuration**\.
+ **Desired size** – Specify the current number of nodes that the managed node group should maintain\.
+ **Minimum size** – Specify the minimum number of nodes that the managed node group can scale in to\.
+ **Maximum size** – Specify the maximum number of nodes that the managed node group can scale out to\. For the maximum number of nodes supported in a node group, see [Amazon EKS service quotas](service-quotas.md)\.  
1. \(Optional\) Add or remove **Kubernetes labels** to the nodes in your node group\. The labels shown here are only the labels that you have applied with Amazon EKS\. Other labels may exist on your nodes that aren't shown here\.  
1. \(Optional\) Add or remove **Kubernetes taints** to the nodes in your node group\. Added taints can have the effect of either `NoSchedule`, `NoExecute`, or `PreferNoSchedule`\. For more information, see [Node taints on managed node groups](node-taints-managed-node-groups.md)\.  
1. \(Optional\) Add or remove **Tags** from your node group resource\. These tags are only applied to the Amazon EKS node group\. They don't propagate to other resources, such as subnets or Amazon EC2 instances in the node group\.  
1. \(Optional\) Edit the **Node Group update configuration**\. Select either **Number** or **Percentage**\.
+ **Number** – Select and specify the number of nodes in your node group that can be updated in parallel\. These nodes will be unavailable during update\.
+ **Percentage** – Select and specify the percentage of nodes in your node group that can be updated in parallel\. These nodes will be unavailable during update\. This is useful if you have many nodes in your node group\.  
1. When you're finished editing, choose **Save changes**\.

EKS Pod Identity can't add additional custom tags to the `AssumeRole` action that it performs\. However, tags that you apply to the IAM role are always available though the same format: `${aws:PrincipalTag/` followed by the key, for example `${aws:PrincipalTag/MyCustomTag}`\.  
**Note**
Tags added to the session through the `sts:AssumeRole` request take precedence in the case of conflict\. For example, assume that Amazon EKS adds a key `eks-cluster-name` and value `my-cluster` to the session when EKS assume the customer role\. You has also added an `eks-cluster-name` tag to the IAM role with value `my-own-cluster`\. In this case, the former takes precedence and value for the `eks-cluster-name` tag will be `my-cluster`\.

+ Before deploying security groups for Pods, consider the following limitations and conditions:
+ Security groups for Pods can't be used with Windows nodes\.
+ Security groups for Pods can be used with clusters configured for the `IPv6` family that contain Amazon EC2 nodes by using version 1\.16\.0 or later of the Amazon VPC CNI plugin\. You can use security groups for Pods with clusters configure `IPv6` family that contain only Fargate nodes by using version 1\.7\.7 or later of the Amazon VPC CNI plugin\. For more information, see [`IPv6` addresses for clusters, Pods, and services](cni-ipv6.md)
+ Security groups for Pods are supported by most [Nitro\-based](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances) Amazon EC2 instance families, though not by all generations of a family\. For example, the `m5`, `c5`, `r5`, `p3`, `m6g`, `c6g`, and `r6g` instance family and generations are supported\. No instance types in the `t` family are supported\. For a complete list of supported instance types, see the [limits\.go](https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/release-1.1.4/pkg/aws/vpc/limits.go) file on Github\. Your nodes must be one of the listed instance types that have `IsTrunkingCompatible: true` in that file\.
+ If you're also using Pod security policies to restrict access to Pod mutation, then the `eks:vpc-resource-controller` Kubernetes user must be specified in the Kubernetes `ClusterRoleBinding` for the `role` that your `psp` is assigned to\. If you're using the default Amazon EKS `psp`, `role`, and `ClusterRoleBinding`, this is the `eks:podsecuritypolicy:authenticated` `ClusterRoleBinding`\. For example, you add the user to the `subjects:` section, as shown in the following example:  
```
[...]
subjects:
- kind: Group
apiGroup: rbac.authorization.k8s.io
name: system:authenticated
- apiGroup: rbac.authorization.k8s.io
kind: User
name: eks:vpc-resource-controller
- kind: ServiceAccount
name: eks-vpc-resource-controller
```
+ If you're using custom networking and security groups for Pods together, the security group specified by security groups for Pods is used instead of the security group specified in the `ENIConfig`\.
+ If you're using version `1.10.2` or earlier of the Amazon VPC CNI plugin and you include the `terminationGracePeriodSeconds` setting in your Pod spec, the value for the setting can't be zero\.
+ If you're using version `1.10` or earlier of the Amazon VPC CNI plugin, or version `1.11` with `POD_SECURITY_GROUP_ENFORCING_MODE`=`strict`, which is the default setting, then Kubernetes services of type `NodePort` and `LoadBalancer` using instance targets with an `externalTrafficPolicy` set to `Local` aren't supported with Pods that you assign security groups to\. For more information about using a load balancer with instance targets, see [Network load balancing on Amazon EKS](network-load-balancing.md)
+ If you're using version `1.10` or earlier of the Amazon VPC CNI plugin or version `1.11` with `POD_SECURITY_GROUP_ENFORCING_MODE`=`strict`, which is the default setting, source NAT is disabled for outbound traffic from Pods with assigned security groups so that outbound security group rules are applied\. To access the internet, Pods with assigned security groups must be launched on nodes that are deployed in a private subnet configured with a NAT gateway or instance\. Pods with assigned security groups deployed to public subnets are not able to access the internet\.  
If you're using version `1.11` or later of the plugin with `POD_SECURITY_GROUP_ENFORCING_MODE`=`standard`, then Pod traffic destined for outside of the VPC is translated to the IP address of the instance's primary network interface\. For this traffic, the rules in the security groups for the primary network interface are used, rather than the rules in the Pod's security groups\.
+ To use Calico network policy with Pods that have associated security groups, you must use version `1.11.0` or later of the Amazon VPC CNI plugin and set `POD_SECURITY_GROUP_ENFORCING_MODE`=`standard`\. Otherwise, traffic flow to and from Pods with associated security groups are not subjected to Calico network policy enforcement and are limited to Amazon EC2 security group enforcement only\. To update your Amazon VPC CNI version, see [Working with the Amazon VPC CNI plugin for Kubernetes Amazon EKS add\-on](managing-vpc-cni.md)
+ Pods running on Amazon EC2 nodes that use security groups in clusters that use [Nodelocal DNSCache](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/) are only supported with version `1.11.0` or later of the Amazon VPC CNI plugin and with `POD_SECURITY_GROUP_ENFORCING_MODE`=`standard`\. To update your Amazon VPC CNI plugin version, see [Working with the Amazon VPC CNI plugin for Kubernetes Amazon EKS add\-on](managing-vpc-cni.md)
+ Security groups for Pods might lead to higher Pod startup latency for Pods with high churn\. This is due to rate limiting in the resource controller\.

+ If you have deprecated versions of the AWS Load Balancer Controller installed, learn how to [Migrate from Deprecated Controller](lbc-remove.md)\.
+ Deprecated versions cannot be upgraded\. They must be removed and a current version of the AWS Load Balancer Controller installed\.
+ <a name="lbc-deprecated-list"></a>Deprecated versions include:
+ AWS ALB Ingress Controller for Kubernetes \("Ingress Controller"\), a predecessor to the AWS Load Balancer Controller\.
+ Any `0.1.x` version of the AWS Load Balancer Controller

Answer
----------------------------------------
Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide the correct response to the question. The term "Ingress group order" is not a recognized concept or term, so it is unclear what specific information the user is seeking. Providing the requested information, if known, or clearly stating that the information is not known or understood would have been a more appropriate and helpful response.

Evaluation_score
----------------------------------------
1

Ground truth answer
You can add an order number of your ingress resource with 'alb.ingress.kubernetes.io/group.order' to determine the rule order between ingresses within the same ingress group.

====================================================================================================

Question
----------------------------------------
ALB IP target type

Documents
----------------------------------------
When you create a Kubernetes `ingress`, an AWS Application Load Balancer \(ALB\) is provisioned that load balances application traffic\. To learn more, see [What is an Application Load Balancer?](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html) in the *Application Load Balancers User Guide* and [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) in the Kubernetes documentation\. ALBs can be used with Pods that are deployed to nodes or to AWS Fargate\. You can deploy an ALB to public or private subnets\.  
Application traffic is balanced at `L7` of the OSI model\. To load balance network traffic at `L4`, you deploy a Kubernetes `service` of the `LoadBalancer` type\. This type provisions an AWS Network Load Balancer\. For more information, see [Network load balancing on Amazon EKS](network-load-balancing.md)\. To learn more about the differences between the two types of load balancing, see [Elastic Load Balancing features](https://aws.amazon.com/elasticloadbalancing/features/) on the AWS website\.  
**Prerequisites**  
Before you can load balance application traffic to an application, you must meet the following requirements\.
+ Have an existing cluster\. If you don't have an existing cluster, see [Getting started with Amazon EKS](getting-started.md)\. If you need to update the version of an existing cluster, see [Updating an Amazon EKS cluster Kubernetes version](update-cluster.md)\.
+ Have the AWS Load Balancer Controller deployed on your cluster\. For more information, see [What is the AWS Load Balancer Controller?](aws-load-balancer-controller.md)\. We recommend version `2.7.2` or later\.
+ At least two subnets in different Availability Zones\. The AWS Load Balancer Controller chooses one subnet from each Availability Zone\. When multiple tagged subnets are found in an Availability Zone, the controller chooses the subnet whose subnet ID comes first lexicographically\. Each subnet must have at least eight available IP addresses\.  
If you're using multiple security groups attached to worker node, exactly one security group must be tagged as follows\. Replace `my-cluster` with your cluster name\.
+ **Key** – `kubernetes.io/cluster/my-cluster`
+ **Value** – `shared` or `owned`
+ If you're using the AWS Load Balancer Controller version `2.1.1` or earlier, subnets must be tagged in the format that follows\. If you're using version `2.1.2` or later, tagging is optional\. However, we recommend that you tag a subnet if any of the following is the case\. You have multiple clusters that are running in the same VPC, or have multiple AWS services that share subnets in a VPC\. Or, you want more control over where load balancers are provisioned for each cluster\. Replace `my-cluster` with your cluster name\.
+ **Key** – `kubernetes.io/cluster/my-cluster`
+ **Value** – `shared` or `owned`
+ Your public and private subnets must meet the following requirements\. This is unless you explicitly specify subnet IDs as an annotation on a service or ingress object\. Assume that you provision load balancers by explicitly specifying subnet IDs as an annotation on a service or ingress object\. In this situation, Kubernetes and the AWS load balancer controller use those subnets directly to create the load balancer and the following tags aren't required\.
+ **Private subnets** – Must be tagged in the following format\. This is so that Kubernetes and the AWS load balancer controller know that the subnets can be used for internal load balancers\. If you use `eksctl` or an Amazon EKS AWS CloudFormation template to create your VPC after March 26, 2020, the subnets are tagged appropriately when created\. For more information about the Amazon EKS AWS CloudFormation VPC templates, see [Creating a VPC for your Amazon EKS cluster](creating-a-vpc.md)\.
+ **Key** – `kubernetes.io/role/internal-elb`
+ **Value** – `1`
+ **Public subnets** – Must be tagged in the following format\. This is so that Kubernetes knows to use only the subnets that were specified for external load balancers\. This way, Kubernetes doesn't choose a public subnet in each Availability Zone \(lexicographically based on their subnet ID\)\. If you use `eksctl` or an Amazon EKS AWS CloudFormation template to create your VPC after March 26, 2020, the subnets are tagged appropriately when created\. For more information about the Amazon EKS AWS CloudFormation VPC templates, see [Creating a VPC for your Amazon EKS cluster](creating-a-vpc.md)\.
+ **Key** – `kubernetes.io/role/elb`
+ **Value** – `1`  
If the subnet role tags aren't explicitly added, the Kubernetes service controller examines the route table of your cluster VPC subnets\. This is to determine if the subnet is private or public\. We recommend that you don't rely on this behavior\. Rather, explicitly add the private or public role tags\. The AWS Load Balancer Controller doesn't examine route tables\. It also requires the private and public tags to be present for successful auto discovery\.  
**Considerations**
+ The [AWS Load Balancer Controller](https://github.com/kubernetes-sigs/aws-load-balancer-controller) creates ALBs and the necessary supporting AWS resources whenever a Kubernetes ingress resource is created on the cluster with the `kubernetes.io/ingress.class: alb` annotation\. The ingress resource configures the ALB to route HTTP or HTTPS traffic to different Pods within the cluster\. To ensure that your ingress objects use the AWS Load Balancer Controller, add the following annotation to your Kubernetes ingress specification\. For more information, see [Ingress specification](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/guide/ingress/spec/) on GitHub\.  
```
annotations:
kubernetes.io/ingress.class: alb
```
**Note**
If you're load balancing to `IPv6` Pods, add the following annotation to your ingress spec\. You can only load balance over `IPv6` to IP targets, not instance targets\. Without this annotation, load balancing is over `IPv4`\.  
```
alb.ingress.kubernetes.io/ip-address-type: dualstack
```
+ The AWS Load Balancer Controller supports the following traffic modes:
+ **Instance** – Registers nodes within your cluster as targets for the ALB\. Traffic reaching the ALB is routed to `NodePort` for your service and then proxied to your Pods\. This is the default traffic mode\. You can also explicitly specify it with the `alb.ingress.kubernetes.io/target-type: instance` annotation\.
**Note**
Your Kubernetes service must specify the `NodePort` or "LoadBalancer" type to use this traffic mode\.
+ **IP** – Registers Pods as targets for the ALB\. Traffic reaching the ALB is directly routed to Pods for your service\. You must specify the `alb.ingress.kubernetes.io/target-type: ip` annotation to use this traffic mode\. The IP target type is required when target Pods are running on Fargate\.
+ To tag ALBs created by the controller, add the following annotation to the controller: `alb.ingress.kubernetes.io/tags`\. For a list of all available annotations supported by the AWS Load Balancer Controller, see [Ingress annotations](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/guide/ingress/annotations/) on GitHub\.
+ Upgrading or downgrading the ALB controller version can introduce breaking changes for features that rely on it\. For more information about the breaking changes that are introduced in each release, see the [ALB controller release notes](https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases) on GitHub\.
<a name="alb-ingress-groups"></a>
**To share an application load balancer across multiple service resources using `IngressGroups`**
To join an ingress to a group, add the following annotation to a Kubernetes ingress resource specification\.  
```
alb.ingress.kubernetes.io/group.name: my-group
```  
The group name must:
+ Be 63 or fewer characters in length\.
+ Consist of lower case letters, numbers, `-`, and `.`
+ Start and end with a letter or number\.  
The controller automatically merges ingress rules for all ingresses in the same ingress group\. It supports them with a single ALB\. Most annotations that are defined on an ingress only apply to the paths defined by that ingress\. By default, ingress resources don't belong to any ingress group\.  
**Warning**
**Potential security risk**: Specify an ingress group for an ingress only when all the Kubernetes users that have RBAC permission to create or modify ingress resources are within the same trust boundary\. If you add the annotation with a group name, other Kubernetes users might create or modify their ingresses to belong to the same ingress group\. Doing so can cause undesirable behavior, such as overwriting existing rules with higher priority rules\.  
You can add an order number of your ingress resource\.  
```
alb.ingress.kubernetes.io/group.order: '10'
```  
The number can be 1\-1000\. The lowest number for all ingresses in the same ingress group is evaluated first\. All ingresses without this annotation are evaluated with a value of zero\. Duplicate rules with a higher number can overwrite rules with a lower number\. By default, the rule order between ingresses within the same ingress group is determined lexicographically based namespace and name\.  
**Important**
Ensure that each ingress in the same ingress group has a unique priority number\. You can't have duplicate order numbers across ingresses\.

```
NAME                                                STATUS   ROLES    AGE     VERSION              INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
fargate-ip-192-0-2-0.region-code.compute.internal   Ready    <none>   8m3s    v1.2.3-eks-1234567   192.0.2.0     <none>        Amazon Linux 2   1.23.456-789.012.amzn2.x86_64   containerd://1.2.3
fargate-ip-192-0-2-1.region-code.compute.internal   Ready    <none>   7m30s   v1.2.3-eks-1234567   192-0-2-1     <none>        Amazon Linux 2   1.23.456-789.012.amzn2.x86_64   containerd://1.2.3
```  
------

Amazon EKS maintains relationships with a network of partners that offer support for compatible OIDC identity providers\. Refer to the following partners' documentation for details on how to integrate the identity provider with Amazon EKS\.  
|  Partner  |  Product  |  Documentation  |
| --- | --- | --- |
|  PingIdentity  |  [PingOne for Enterprise](https://docs.pingidentity.com/r/en-us/pingoneforenterprise/p14e_landing)  |  [Installation instructions](https://docs.pingidentity.com/r/en-us/solution-guides/htg_config_oidc_authn_aws_eks_custers)  |  
Amazon EKS aims to give you a wide selection of options to cover all use cases\. If you develop a commercially supported OIDC compatible identity provider that is not listed here, then contact our partner team at [aws\-container\-partners@amazon\.com](mailto:aws-container-partners@amazon.com) for more information\.

You can troubleshoot and investigate network connections that use network policies by reading the [Network policy logs](#network-policies-troubleshooting-flowlogs) and by running tools from the [eBPF SDK](#network-policies-ebpf-sdk)\.

Answer
----------------------------------------
The ALB IP target type is used to register Pods as targets for the Application Load Balancer (ALB) in Kubernetes. It is specified using the `alb.ingress.kubernetes.io/target-type: ip` annotation in the ingress resource. This traffic mode is required when target Pods are running on Fargate.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes, but incomplete.

The answer does provide the target type as IP for the ALB, but it does not explain what ALB IP target type is. Also, the annotation and traffic mode information given is not directly related to the question asked.

Evaluation_score
----------------------------------------
3

Ground truth answer
The IP target type is required when target Pods are running on Fargate and registers Pods as targets for the ALB, directly routing traffic to Pods for your service.

====================================================================================================

Question
----------------------------------------
FastAPI root_path?

Documents
----------------------------------------
Next we will see how to add dependencies to the whole `FastAPI` application, so that they apply to each *path operation*.

The first `"/static"` refers to the sub-path this "sub-application" will be "mounted" on. So, any path that starts with `"/static"` will be handled by it.  
The `directory="static"` refers to the name of the directory that contains your static files.  
The `name="static"` gives it a name that can be used internally by **FastAPI**.  
All these parameters can be different than "`static`", adjust them with the needs and specific details of your own application.

**FastAPI** gives you the following:

* Create an `app` directory and enter it.
* Create an empty file `__init__.py`.
* Create a `main.py` file with:  
```Python
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get("/")
def read_root():
return {"Hello": "World"}


@app.get("/items/{item_id}")
def read_item(item_id: int, q: Union[str, None] = None):
return {"item_id": item_id, "q": q}
```

Answer
----------------------------------------
The `root_path` is not explicitly defined in the provided code snippet. By default, FastAPI uses the base URL as the root path. In this case, the root path would be "/" and can be accessed using the route `"/"` defined in the `read_root()` function.

Reason: The context provided is relevant to learning about how to add dependencies to FastAPI and the purpose of the `"/static"` sub-path. However, it does not specifically mention or define the `root_path`.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not provide a clear and direct response to the question about the `root_path` in FastAPI. Instead, it explains the purpose of the provided code snippet and how the root path can be accessed, but it does not define what the `root_path` is.

Evaluation_score
----------------------------------------
2

Ground truth answer
The `root_path` is a mechanism from the ASGI specification used by FastAPI to communicate the mount path for a sub-application, allowing it to use the correct path prefix for the docs UI and handle all mounted sub-applications correctly.

====================================================================================================

Question
----------------------------------------
FastAPI sub-app path operations?

Documents
----------------------------------------
Next we will see how to add dependencies to the whole `FastAPI` application, so that they apply to each *path operation*.

Then, create your sub-application, and its *path operations*.  
This sub-application is just another standard FastAPI application, but this is the one that will be "mounted":  
```Python hl_lines="11  14-16"
{!../../../docs_src/sub_applications/tutorial001.py!}
```

When you mount a sub-application as described above, FastAPI will take care of communicating the mount path for the sub-application using a mechanism from the ASGI specification called a `root_path`.  
That way, the sub-application will know to use that path prefix for the docs UI.  
And the sub-application could also have its own mounted sub-applications and everything would work correctly, because FastAPI handles all these `root_path`s automatically.  
You will learn more about the `root_path` and how to use it explicitly in the section about [Behind a Proxy](behind-a-proxy.md){.internal-link target=_blank}.

When you create a **FastAPI** *path operation* you can normally return any data from it: a `dict`, a `list`, a Pydantic model, a database model, etc.  
By default, **FastAPI** would automatically convert that return value to JSON using the `jsonable_encoder` explained in [JSON Compatible Encoder](../tutorial/encoder.md){.internal-link target=_blank}.  
Then, behind the scenes, it would put that JSON-compatible data (e.g. a `dict`) inside of a `JSONResponse` that would be used to send the response to the client.  
But you can return a `JSONResponse` directly from your *path operations*.  
It might be useful, for example, to return custom headers or cookies.

Answer
----------------------------------------
To define FastAPI sub-app path operations, you can create a new FastAPI application and define the path operations within it, just like you would with a top-level FastAPI application. When you mount this sub-application using the `include_router` function, FastAPI will handle the communication of the mount path using the ASGI `root_path` mechanism.

Here's an example of how to define path operations in a sub-app:

```python
from fastapi import FastAPI

sub_app = FastAPI()

@sub_app.get("/sub-app-path-operation")
def sub_app_path_operation():
    return {"message": "This is a sub-app path operation"}
```

You can then mount this sub-app in your main application:

```python
from fastapi import FastAPI
from . import sub_app

main_app = FastAPI()

main_app.include_router(sub_app, prefix="/sub-app")
```

With this setup, when you access the URL `/sub-app/sub-app-path-operation` in your main application, it will route to the path operation you defined in the sub-app. FastAPI will automatically handle the mounting and routing based on the `root_path`.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The response accurately explains how to define path operations in a FastAPI sub-app and how to mount it in the main application using the `include_router` function. The explanation is clear and concise, with relevant examples provided to support the answer.

Evaluation_score
----------------------------------------
5

Ground truth answer
Each FastAPI sub-application, when mounted, handles all the path operations declared under its own path.

====================================================================================================

Question
----------------------------------------
FastAPI sub-app docs?

Documents
----------------------------------------
By this point, it's already clear that **FastAPI** with its ideas is being useful for many people.  
It is being chosen over previous alternatives for suiting many use cases better.  
Many developers and teams already depend on **FastAPI** for their projects (including me and my team).  
But still, there are many improvements and features to come.  
**FastAPI** has a great future ahead.  
And [your help](help-fastapi.md){.internal-link target=_blank} is greatly appreciated.

If you need to have two independent FastAPI applications, with their own independent OpenAPI and their own docs UIs, you can have a main app and "mount" one (or more) sub-application(s).

The first step is to disable the automatic docs, as by default, those use the default CDN.  
To disable them, set their URLs to `None` when creating your `FastAPI` app:  
```Python hl_lines="8"
{!../../../docs_src/custom_docs_ui/tutorial001.py!}
```

And the same way as with a custom CDN, now you can create the *path operations* for the custom docs.  
Again, you can re-use FastAPI's internal functions to create the HTML pages for the docs, and pass them the needed arguments:  
* `openapi_url`: the URL where the HTML page for the docs can get the OpenAPI schema for your API. You can use here the attribute `app.openapi_url`.
* `title`: the title of your API.
* `oauth2_redirect_url`: you can use `app.swagger_ui_oauth2_redirect_url` here to use the default.
* `swagger_js_url`: the URL where the HTML for your Swagger UI docs can get the **JavaScript** file. **This is the one that your own app is now serving**.
* `swagger_css_url`: the URL where the HTML for your Swagger UI docs can get the **CSS** file. **This is the one that your own app is now serving**.  
And similarly for ReDoc...  
```Python hl_lines="2-6  14-22  25-27  30-36"
{!../../../docs_src/custom_docs_ui/tutorial002.py!}
```  
!!! tip
The *path operation* for `swagger_ui_redirect` is a helper for when you use OAuth2.  
If you integrate your API with an OAuth2 provider, you will be able to authenticate and come back to the API docs with the acquired credentials. And interact with it using the real OAuth2 authentication.  
Swagger UI will handle it behind the scenes for you, but it needs this "redirect" helper.

Answer
----------------------------------------
Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide the requested information about FastAPI sub-app docs. Instead, it provides metadata about the response.

Evaluation_score
----------------------------------------
1

Ground truth answer
Each FastAPI sub-application has its own automatic API docs, accessible under a specific sub-path prefix.

====================================================================================================

Question
----------------------------------------
FastAPI mount path?

Documents
----------------------------------------
Next we will see how to add dependencies to the whole `FastAPI` application, so that they apply to each *path operation*.

This is available since FastAPI version `0.67.0`. 🔖

"Mounting" means adding a completely "independent" application in a specific path, that then takes care of handling everything under that path, with the _path operations_ declared in that sub-application.

* Create an `app` directory and enter it.
* Create an empty file `__init__.py`.
* Create a `main.py` file with:  
```Python
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get("/")
def read_root():
return {"Hello": "World"}


@app.get("/items/{item_id}")
def read_item(item_id: int, q: Union[str, None] = None):
return {"item_id": item_id, "q": q}
```

Answer
----------------------------------------
The mount path in FastAPI is added by creating a new FastAPI instance and then mounting it to the main application using the `include_router` or `mount` function from the `FastAPI` class. However, the provided context does not include any information about adding a mount path. The code snippet shows the creation of a new FastAPI application with two path operations, but there is no mention of a mount path.

Therefore, the answer is:

Reason: The context provided does not include any information about the mount path.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer provides a general explanation about how to add a mount path in FastAPI, but it does not answer the question which asks about the mount path in a specific context. The provided context includes a code snippet that does not mention a mount path, so the answer should have addressed this by stating that there is no mount path in the provided code.

Evaluation_score
----------------------------------------
2

Ground truth answer
The path where a sub-application is mounted is determined when you mount it into the main FastAPI application.

====================================================================================================

Question
----------------------------------------
Disable ALS sensor?

Documents
----------------------------------------
- Go to the Linux Mint Launcher or press the super key.
- Type out the word terminal, click to open it.
- Left click and drag to highlight and copy the code below in the gray box, right click/paste to copy it into the terminal window.
- **Then press the enter key, password, reboot.**  
``
sudo apt update && sudo apt upgrade -y && sudo apt-get install linux-oem-22.04c -y && echo "options snd-hda-intel model=dell-headset-multi" | sudo tee -a /etc/modprobe.d/alsa-base.conf && sudo sed -i 's/^GRUB_CMDLINE_LINUX_DEFAULT.*/GRUB_CMDLINE_LINUX_DEFAULT="quiet splash module_blacklist=hid_sensor_hub nvme.noacpi=1"/g' /etc/default/grub && sudo update-grub && echo "[connection]" | sudo tee /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf && echo "wifi.powersave = 2" | sudo tee -a /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf
``

- Browse to Activities in the upper left corner, click to open it.
- Type out the word terminal, click to open it.
- Left click and drag to highlight and copy the code below in the gray box, right click/paste to copy it into the terminal window.
- **Then press the enter key, password, reboot.**  
``
sudo apt update && sudo apt upgrade -y && sudo snap refresh && echo "options snd-hda-intel model=dell-headset-multi" | sudo tee -a /etc/modprobe.d/alsa-base.conf && gsettings set org.gnome.mutter experimental-features "['scale-monitor-framebuffer']" && sudo sed -i 's/^GRUB_CMDLINE_LINUX_DEFAULT.*/GRUB_CMDLINE_LINUX_DEFAULT="quiet splash module_blacklist=hid_sensor_hub nvme.noacpi=1"/g' /etc/default/grub && sudo update-grub && echo "[connection]" | sudo tee /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf && echo "wifi.powersave = 2" | sudo tee -a /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf
``

``echo "options snd-hda-intel model=dell-headset-multi" | sudo tee -a /etc/modprobe.d/alsa-base.conf``

``
sudo grubby --update-kernel=ALL --args="module_blacklist=hid_sensor_hub"
``

Answer
----------------------------------------
The command to disable the ALS (Automatic Light Sensor) sensor is:

sudo grubby --update-kernel=ALL --args="module_blacklist=hid_sensor_hub"

This command updates the GRUB configuration for all kernels to include the "module\_blacklist=hid\_sensor\_hub" argument, which prevents the hid\_sensor\_hub module (including the ALS sensor) from loading during boot.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes, but with additional and possibly unnecessary information. The core component of the answer is correct, but the response could be more concise and to the point.

Evaluation_score
----------------------------------------
3

Ground truth answer
The command 'sudo gedit /etc/default/grub' followed by appending 'GRUB_CMDLINE_LINUX_DEFAULT="quiet splash module_blacklist=hid_sensor_hub"' to the GRUB_CMDLINE_LINUX_DEFAULT="quiet splash section disables the ALS sensor.

====================================================================================================

Question
----------------------------------------
Install OEM kernel?

Documents
----------------------------------------
- Update your Ubuntu install's packages.
- Install the recommended OEM kernel and provide you with an alert should the OEM kernel needing updating.  
&nbsp; &nbsp; &nbsp; &nbsp;

``sudo apt install linux-oem-22.04c``

- [Install OEM D kernel](#step-1)
- [Allow both CPU and platform drivers to be simultaneously active](#step-3-allow-both-cpu-and-platform-drivers-to-be-simultaneously-active)
- [Prevent graphical artifacts from appearing](#addtionally-we-recommend-the-following-as-well-if-you-are-experiencing-graphical-artifacts-from-appearing)

```
ls /boot/vmlinuz-* | awk -F"-" '{split($0, a, "-"); version=a[3]; if (version>max) {max=version; kernel=a[2] "-" a[3] "-" a[4]}} END{print kernel}'
```  
Right now, this is **6.5.0.1013-oem** - but this may evolve in the future.

Answer
----------------------------------------
To install the OEM kernel, run the command:
```
sudo apt install linux-oem-22.04c
```
This will update your Ubuntu install's packages and install the recommended OEM kernel. The script provided will check the current version of the kernel and print it, which is currently 6.5.0.1013-oem but may change in the future.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The answer provided contains clear instructions on how to install the OEM kernel, making it a helpful and accurate response.

Evaluation_score
----------------------------------------
5

Ground truth answer
The command 'sudo apt install linux-oem-22.04c' installs the recommended OEM kernel.

====================================================================================================

Question
----------------------------------------
Prevent wifi drop offs?

Documents
----------------------------------------
- Browse to the upper left corner, click the horizontal line to open the menu.
- Type out the word terminal, click to open it.
- Click on the small icon shown in the image below to copy the code below in the gray box, right click/paste it into the terminal window.  
```
sudo apt install iw && interface=$(nmcli -t -f active,device d wifi list | grep '^yes' | cut -d':' -f2) && echo -e "\n\033[1;33mInterface:\033[0m $interface" && iw dev $interface get power_save
```
This will indicate if your Wi-Fi power save is on or off. If it's on and you're seeing drops, continue to the next line.  
```
sudo sed -i '/^wifi.powersave = 3$/s/3/2/' /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf
```  
This will change 3 into 2, which will disable powersave.  
> **TIP:** You can use the little clipboard icon to the right of the code to copy to your clipboard.  
<p style="text-align: left"><img src="https://raw.githubusercontent.com/FrameworkComputer/linux-docs/main/copied.png" alt="Copy The Code Below Like This" title="Copy The Code Above Like This"></p>  
**reboot.**  
&nbsp; &nbsp; &nbsp; &nbsp;

``sudo gnome-text-editor /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf``

- [MediaTek WiFi Dropout on WiFi 6E routers fix](#mediatek-wifi-dropout-on-wifi-6e-routers)
- [Buzzing sound from headphone jack](#buzzing-sound-from-headphone-jack)

``wifi.powersave = 2``

Answer
----------------------------------------
The terminal commands provided will change the Wi-Fi power save setting to 2, disabling power save mode, in an attempt to prevent Wi-Fi drop offs. After running these commands, a reboot is required for the changes to take effect.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Partial yes.

The answer does provide some information on how to potentially prevent wifi drop-offs by adjusting the power save setting. However, it is more focused on providing metadata and details about the response, rather than directly answering the question with a clear and concise solution.

Evaluation_score
----------------------------------------
3

Ground truth answer
The command 'sudo gedit /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf' followed by changing '3' into a '2' in 'wifi.powersave = 2' prevents wifi drop offs.

====================================================================================================

Question
----------------------------------------
Enable headset mic?

Documents
----------------------------------------
- Browse to Activities in the upper left corner, click to open it.
- Type out the word terminal, click to open it.
- Left click and drag to highlight and copy the code below in the gray box, right click/paste to copy it into the terminal window.
- **Then press the enter key, password, reboot.**  
``
sudo apt update && sudo apt upgrade -y && sudo snap refresh && echo "options snd-hda-intel model=dell-headset-multi" | sudo tee -a /etc/modprobe.d/alsa-base.conf && gsettings set org.gnome.mutter experimental-features "['scale-monitor-framebuffer']" && sudo sed -i 's/^GRUB_CMDLINE_LINUX_DEFAULT.*/GRUB_CMDLINE_LINUX_DEFAULT="quiet splash module_blacklist=hid_sensor_hub nvme.noacpi=1"/g' /etc/default/grub && sudo update-grub && echo "[connection]" | sudo tee /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf && echo "wifi.powersave = 2" | sudo tee -a /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf
``

In addition to "hold to speak", we are experimenting with a lighter alternative UI for this functionality. Activate Quick Voice by pressing `kbstyle(Cmd+K)` or `kbstyle(Ctrl+K)` and then holding `I`. While holding `I`, a more lightweight recording UI is shown. Upon release, the request is sent.  
![Quick Voice showing only a microphone indicator and the text of the recorded voice message.](images/1_86/inline-chat-quick-speak.png)

We are introducing a new extension to bring voice support to VS Code! The new [VS Code Speech](https://marketplace.visualstudio.com/items?itemName=ms-vscode.vscode-speech) extension integrates into [GitHub Copilot Chat](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat) to enable voice-to-text transcription services for Chat input.  
Once installed, a microphone icon appears and when selected, begins filling Chat input with text based on the transcribed results of your voice. The transcription is computed locally on your machine and does not require a connection to the internet.  
![VS Code Speech activated by the microphone icon](images/1_84/vscode-voice.gif)

You can dictate into the editor via these commands: **Voice: Start Dictation in Editor** (`kb(workbench.action.editorDictation.start)`) and **Voice: Stop Dictation in Editor** (`kb(workbench.action.editorDictation.stop)`). Once started, a little microphone icon appears where the cursor is, awaiting your voice input:  
![Editor Dictation Mode](images/accessibility/editor-dictate.png)  
You can press and hold the keybinding for the voice start command (`kb(workbench.action.editorDictation.start)`) to enable **walky-talky mode**. Voice recognition is active until you release the keys, after which the request is submitted automatically.  
**Note:** dictation even works in other places where a rich editor is used, such as the SCM commit input box and the comments input field when reviewing pull requests.

Answer
----------------------------------------
The context provided gives instructions on how to enable certain features and extensions to allow for voice-to-text functionality in a code editor and chat interface. However, it does not directly mention enabling a headset mic. Based on the context, the user should ensure that their headset is properly connected and configured on their system.

To use the voice-to-text features with a headset mic, the user might need to change their default audio input device to the headset mic. This can usually be done in the system settings or the sound settings of the application being used.

If the headset mic is not recognized or does not work as expected, the user might need to check the headset's compatibility with their system and software or consult the headset's documentation for troubleshooting steps.

Since the context provided does not have specific instructions for enabling a headset mic, I cannot confidently answer this question without additional information.

Response: I do not know the answer, as the context provided does not contain specific instructions for enabling a headset mic.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The response does not directly answer the question of enabling a headset mic. It provides general suggestions and information about possible steps to enable a headset mic, but it does not provide a clear and concise answer based on the given context.

Evaluation_score
----------------------------------------
2

Ground truth answer
The command 'echo "options snd-hda-intel model=dell-headset-multi" | sudo tee -a /etc/modprobe.d/alsa-base.conf' enables headset mic input.

====================================================================================================

Question
----------------------------------------
inf1 create cluster

Documents
----------------------------------------
**Important**
You cannot create new clusters with these versions\.  
This topic gives important changes to be aware of for versions `1.22 `and `1.21`\. When upgrading, carefully review the changes that have occurred between the old and new versions for your cluster\.

**To create a cluster with Inf1 Amazon EC2 instance nodes**  
1. Create a cluster with Inf1 Amazon EC2 instance nodes\. You can replace `inf1.2xlarge` with any [Inf1 instance type](https://aws.amazon.com/ec2/instance-types/inf1/)\. The `eksctl` utility detects that you are launching a node group with an `Inf1` instance type and will start your nodes using one of the Amazon EKS optimized accelerated Amazon Linux AMIs\.
**Note**
You can't use [IAM roles for service accounts](iam-roles-for-service-accounts.md) with TensorFlow Serving\.  
```
eksctl create cluster \
--name inferentia \
--region region-code \
--nodegroup-name ng-inf1 \
--node-type inf1.2xlarge \
--nodes 2 \
--nodes-min 1 \
--nodes-max 4 \
--ssh-access \
--ssh-public-key your-key \
--with-oidc
```
**Note**
Note the value of the following line of the output\. It's used in a later \(optional\) step\.  
```
[9]  adding identity "arn:aws:iam::111122223333:role/eksctl-inferentia-nodegroup-ng-in-NodeInstanceRole-FI7HIYS3BS09" to auth ConfigMap
```  
When launching a node group with `Inf1` instances, `eksctl` automatically installs the AWS Neuron Kubernetes device plugin\. This plugin advertises Neuron devices as a system resource to the Kubernetes scheduler, which can be requested by a container\. In addition to the default Amazon EKS node IAM policies, the Amazon S3 read only access policy is added so that the sample application, covered in a later step, can load a trained model from Amazon S3\.  
1. Make sure that all Pods have started correctly\.  
```
kubectl get pods -n kube-system
```  
Abbreviated output:  
```
NAME                                   READY   STATUS    RESTARTS   AGE
[...]
neuron-device-plugin-daemonset-6djhp   1/1     Running   0          5m
neuron-device-plugin-daemonset-hwjsj   1/1     Running   0          5m
```

```
eksctl create cluster --name my-cluster --region region-code --fargate
```  
------

Flux is a tool that you can use to manage your cluster configuration using Git\. It uses an operator in the cluster to trigger deployments inside of Kubernetes\. For more information about operators, see [OperatorHub\.io](https://operatorhub.io/) on GitHub\.
+ [Project URL](https://fluxcd.io/)
+ [Project documentation](https://docs.fluxcd.io/)

Answer
----------------------------------------
Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide a response to the command "inf1 create cluster". It only includes metadata information.

Evaluation_score
----------------------------------------
1

Ground truth answer
To create a cluster with Inf1 Amazon EC2 instance nodes, use the provided `eksctl` command with the desired instance type and node details.

====================================================================================================

Question
----------------------------------------
kubectl install prerequisites

Documents
----------------------------------------
See the following common questions and answers about using Kubecost with Amazon EKS\.

```
kubectl exec -it eks-sample-linux-deployment-65b7669776-m6qxz -n eks-sample-app -- /bin/bash
```  
------

Run the following commands\.  
```
kubectl delete deployment -n kube-system vpc-admission-webhook
kubectl delete service -n kube-system vpc-admission-webhook
kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io vpc-admission-webhook-cfg
```  
------

Yes\. `Kubectl-cost` is an open source tool by Kubecost \(Apache 2\.0 License\) that provides CLI access to Kubernetes cost allocation metrics\. To install `kubectl-cost`, see [Installation](https://github.com/kubecost/kubectl-cost#installation) on GitHub\.

Answer
----------------------------------------
Reason: I cannot answer the question confidently
Reason: The context provided discusses some commands and a tool related to Kubernetes, but it does not mention anything about installing kubectl prerequisites.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not provide a solution to install kubectl prerequisites. It explains the reason for not answering the question, which doesn't add value to the user's query.

Evaluation_score
----------------------------------------
2

Ground truth answer
Have `kubectl` installed on your computer. For more information, see Installing or updating `kubectl`.

====================================================================================================

Question
----------------------------------------
Forward gRPC port to TensorFlow Serving service

Documents
----------------------------------------
1. To test locally, forward the gRPC port to the `eks-neuron-test` service\.  
```
kubectl port-forward service/eks-neuron-test 8500:8500 &
```  
1. Create a Python script called `tensorflow-model-server-infer.py` with the following content\. This script runs inference via gRPC, which is service framework\.  
```
import numpy as np
import grpc
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2_grpc
from tensorflow.keras.applications.resnet50 import decode_predictions

if __name__ == '__main__':
channel = grpc.insecure_channel('localhost:8500')
stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)
img_file = tf.keras.utils.get_file(
"./kitten_small.jpg",
"https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg")
img = image.load_img(img_file, target_size=(224, 224))
img_array = preprocess_input(image.img_to_array(img)[None, ...])
request = predict_pb2.PredictRequest()
request.model_spec.name = 'resnet50_inf1'
request.inputs['input'].CopyFrom(
tf.make_tensor_proto(img_array, shape=img_array.shape))
result = stub.Predict(request)
prediction = tf.make_ndarray(result.outputs['output'])
print(decode_predictions(prediction))
```  
1. Run the script to submit predictions to your service\.  
```
python3 tensorflow-model-server-infer.py
```  
An example output is as follows\.  
```
[[(u'n02123045', u'tabby', 0.68817204), (u'n02127052', u'lynx', 0.12701613), (u'n02123159', u'tiger_cat', 0.08736559), (u'n02124075', u'Egyptian_cat', 0.063844085), (u'n02128757', u'snow_leopard', 0.009240591)]]
```

Support for port forwarding is built into Visual Studio Code via [Microsoft dev tunnels](https://learn.microsoft.com/azure/developer/dev-tunnels/overview), no extension required. When running a local web service, you can use the **Ports** view to make the service accessible to others over the internet.

Again, if you are running your container behind a TLS Termination Proxy (load balancer) like Nginx or Traefik, add the option `--proxy-headers` to the command:  
```Dockerfile
CMD ["fastapi", "run", "app/main.py", "--proxy-headers", "--port", "80"]
```

Uses Chalice and AWS CLI commands to deploy routing and data-handling layers
to AWS Lambda, set up API Gateway to handle HTTP requests, and
establish AWS Identity and Access Management (IAM) roles and profiles to manage
permissions.  
Deploy the REST API by running the following command at a command
prompt.  
```
python library_demo.py deploy_rest
```  
The REST API is now deployed and can received HTTP requests. Try it yourself
using your favorite HTTP client or run the next step to see a demonstration
of how to use the Requests package to call the web service.  
---

Answer
----------------------------------------
To forward the gRPC port to the TensorFlow Serving service, you can use the following command:

```
kubectl port-forward service/eks-neuron-test 8500:8500 &
```

This command forwards the gRPC port (8500) of the `eks-neuron-test` service to your local machine. After running this command, you can use the `localhost:8500` address in your Python script to connect to the service.

The provided context contains information about how to create a Python script to run inference via gRPC and submit predictions to the service. However, it does not contain any information about forwarding the port to the service. Therefore, the context is not directly relevant to the question, but it is still necessary to understand how to use the service.

So, to answer the original question, you can forward the gRPC port to the TensorFlow Serving service using the provided `kubectl` command.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The response directly addresses the question by providing the command to forward the gRPC port to the TensorFlow Serving service. It clearly explains how to use the command and its relevance to the question.

Evaluation_score
----------------------------------------
5

Ground truth answer
To test locally, forward the gRPC port to the `eks-neuron-test` service using the `kubectl port-forward service/eks-neuron-test 8500:8500 &` command.

====================================================================================================

Question
----------------------------------------
Create TensorFlow Serving Kubernetes service

Documents
----------------------------------------
Right-click on your AKS cluster and select **Create GitHub Workflow** to easily open and create a workflow starter template. This helps in quick generation of the workflow templates with pre-populated resource group and cluster name for:  
* [Starter Workflow](https://github.com/actions/starter-workflows/blob/main/deployments/azure-kubernetes-service.yml)
* [Helm Workflow](https://github.com/actions/starter-workflows/blob/main/deployments/azure-kubernetes-service-helm.yml)
* [Kompose Workflow](https://github.com/actions/starter-workflows/blob/main/deployments/azure-kubernetes-service-kompose.yml)
* [Kustomize Workflow](https://github.com/actions/starter-workflows/blob/main/deployments/azure-kubernetes-service-kustomize.yml)

1. If you have an existing Kubernetes service account that you want to assume an IAM role, then you can skip this step\.  
Create a Kubernetes service account\. Copy the following contents to your device\. Replace *my\-service\-account* with your desired name and *default* with a different namespace, if necessary\. If you change *default*, the namespace must already exist\.  
```
cat >my-service-account.yaml <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
name: my-service-account
namespace: default
EOF
kubectl apply -f my-service-account.yaml
```  
1. Set your AWS account ID to an environment variable with the following command\.  
```
account_id=$(aws sts get-caller-identity --query "Account" --output text)
```  
1. Set your cluster's OIDC identity provider to an environment variable with the following command\. Replace `my-cluster` with the name of your cluster\.  
```
oidc_provider=$(aws eks describe-cluster --name my-cluster --region $AWS_REGION --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///")
```  
1. Set variables for the namespace and name of the service account\. Replace `my-service-account` with the Kubernetes service account that you want to assume the role\. Replace *default* with the namespace of the service account\.  
```
export namespace=default
export service_account=my-service-account
```  
1. Run the following command to create a trust policy file for the IAM role\. If you want to allow all service accounts within a namespace to use the role, then copy the following contents to your device\. Replace *StringEquals* with **StringLike** and replace *$service\_account* with **\***\. You can add multiple entries in the `StringEquals` or `StringLike` conditions to allow multiple service accounts or namespaces to assume the role\. To allow roles from a different AWS account than the account that your cluster is in to assume the role, see [Cross\-account IAM permissions](cross-account-access.md) for more information\.  
```
cat >trust-relationship.json <<EOF
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"Federated": "arn:aws:iam::$account_id:oidc-provider/$oidc_provider"
},
"Action": "sts:AssumeRoleWithWebIdentity",
"Condition": {
"StringEquals": {
"$oidc_provider:aud": "sts.amazonaws.com",
"$oidc_provider:sub": "system:serviceaccount:$namespace:$service_account"
}
}
}
]
}
EOF
```  
1. Create the role\. Replace `my-role` with a name for your IAM role, and `my-role-description` with a description for your role\.  
```
aws iam create-role --role-name my-role --assume-role-policy-document file://trust-relationship.json --description "my-role-description"
```  
1. Attach an IAM policy to your role\. Replace `my-role` with the name of your IAM role and `my-policy` with the name of an existing policy that you created\.  
```
aws iam attach-role-policy --role-name my-role --policy-arn=arn:aws:iam::$account_id:policy/my-policy
```  
1. Annotate your service account with the Amazon Resource Name \(ARN\) of the IAM role that you want the service account to assume\. Replace `my-role` with the name of your existing IAM role\. Suppose that you allowed a role from a different AWS account than the account that your cluster is in to assume the role in a previous step\. Then, make sure to specify the AWS account and role from the other account\. For more information, see [Cross\-account IAM permissions](cross-account-access.md)\.  
```
kubectl annotate serviceaccount -n $namespace $service_account eks.amazonaws.com/role-arn=arn:aws:iam::$account_id:role/my-role
```  
------  
1. Confirm that the role and service account are configured correctly\.  
1. Confirm that the IAM role's trust policy is configured correctly\.  
```
aws iam get-role --role-name my-role --query Role.AssumeRolePolicyDocument
```  
An example output is as follows\.  
```
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"Federated": "arn:aws:iam::111122223333:oidc-provider/oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE"
},
"Action": "sts:AssumeRoleWithWebIdentity",
"Condition": {
"StringEquals": {
"oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE:sub": "system:serviceaccount:default:my-service-account",
"oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE:aud": "sts.amazonaws.com"
}
}
}
]
}
```  
1. Confirm that the policy that you attached to your role in a previous step is attached to the role\.  
```
aws iam list-attached-role-policies --role-name my-role --query AttachedPolicies[].PolicyArn --output text
```  
An example output is as follows\.  
```
arn:aws:iam::111122223333:policy/my-policy
```  
1. Set a variable to store the Amazon Resource Name \(ARN\) of the policy that you want to use\. Replace *my\-policy* with the name of the policy that you want to confirm permissions for\.  
```
export policy_arn=arn:aws:iam::111122223333:policy/my-policy
```  
1. View the default version of the policy\.  
```
aws iam get-policy --policy-arn $policy_arn
```  
An example output is as follows\.  
```
{
"Policy": {
"PolicyName": "my-policy",
"PolicyId": "EXAMPLEBIOWGLDEXAMPLE",
"Arn": "arn:aws:iam::111122223333:policy/my-policy",
"Path": "/",
"DefaultVersionId": "v1",
[...]
}
}
```  
1. View the policy contents to make sure that the policy includes all the permissions that your Pod needs\. If necessary, replace *1* in the following command with the version that's returned in the previous output\.  
```
aws iam get-policy-version --policy-arn $policy_arn --version-id v1
```  
An example output is as follows\.  
```
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": "s3:GetObject",
"Resource": "arn:aws:s3:::my-pod-secrets-bucket"
}
]
}
```  
If you created the example policy in a previous step, then your output is the same\. If you created a different policy, then the *example* content is different\.  
1. Confirm that the Kubernetes service account is annotated with the role\.  
```
kubectl describe serviceaccount my-service-account -n default
```  
An example output is as follows\.  
```
Name:                my-service-account
Namespace:           default
Annotations:         eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/my-role
Image pull secrets:  <none>
Mountable secrets:   my-service-account-token-qqjfl
Tokens:              my-service-account-token-qqjfl
[...]
```  
1. \(Optional\) [Configure the AWS Security Token Service endpoint for a service account](configure-sts-endpoint.md)\. AWS recommends using a regional AWS STS endpoint instead of the global endpoint\. This reduces latency, provides built\-in redundancy, and increases session token validity\.  
**Next step**
[Configure Pods to use a Kubernetes service account](pod-configuration.md)

You can use IP targets with Pods deployed to Amazon EC2 nodes or Fargate\. Your Kubernetes service must be created as type `LoadBalancer`\. For more information, see [Type LoadBalancer](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer) in the Kubernetes documentation\.  
To create a load balancer that uses IP targets, add the following annotations to a service manifest and deploy your service\. The `external` value for `aws-load-balancer-type` is what causes the AWS Load Balancer Controller, rather than the AWS cloud provider load balancer controller, to create the Network Load Balancer\. You can view a [sample service manifest](#network-load-balancing-service-sample-manifest) with the annotations\.  
```
service.beta.kubernetes.io/aws-load-balancer-type: "external"
service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
```  
**Note**
If you're load balancing to `IPv6` Pods, add the following annotation\. You can only load balance over `IPv6` to IP targets, not instance targets\. Without this annotation, load balancing is over `IPv4`\.  
```
service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack
```  
Network Load Balancers are created with the `internal` `aws-load-balancer-scheme`, by default\. You can launch Network Load Balancers in any subnet in your cluster's VPC, including subnets that weren't specified when you created your cluster\.  
Kubernetes examines the route table for your subnets to identify whether they are public or private\. Public subnets have a route directly to the internet using an internet gateway, but private subnets do not\.  
If you want to create a Network Load Balancer in a public subnet to load balance to Amazon EC2 nodes \(Fargate can only be private\), specify `internet-facing` with the following annotation:  
```
service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
```  
**Note**
The `service.beta.kubernetes.io/aws-load-balancer-type: "nlb-ip"` annotation is still supported for backwards compatibility\. However, we recommend using the previous annotations for new load balancers instead of `service.beta.kubernetes.io/aws-load-balancer-type: "nlb-ip"`\.  
**Important**
Do not edit the annotations after creating your service\. If you need to modify it, delete the service object and create it again with the desired value for this annotation\.  
------

If you don't already have the Amazon EKS Pod execution role for your cluster, you can use the AWS Management Console or the AWS CLI to create it\.  
------

Answer
----------------------------------------
Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

Evaluation_score
----------------------------------------
1

Ground truth answer
Create a Kubernetes service for your TensorFlow model Serving application using the `rn50_service.yaml` file.

====================================================================================================

Question
----------------------------------------
What is WebSocketException in FastAPI?

Documents
----------------------------------------
These are the exceptions that you can raise to show errors to the client.  
When you raise an exception, as would happen with normal Python, the rest of the execution is aborted. This way you can raise these exceptions from anywhere in the code to abort a request and show the error to the client.  
You can use:  
* `HTTPException`
* `WebSocketException`  
These exceptions can be imported directly from `fastapi`:  
```python
from fastapi import HTTPException, WebSocketException
```  
::: fastapi.HTTPException  
::: fastapi.WebSocketException

Here's the reference information for the `APIRouter` class, with all its parameters, attributes and methods.  
You can import the `APIRouter` class directly from `fastapi`:  
```python
from fastapi import APIRouter
```  
::: fastapi.APIRouter
options:
members:
- websocket
- include_router
- get
- put
- post
- delete
- options
- head
- patch
- trace
- on_event

**FastAPI** has some default exception handlers.  
These handlers are in charge of returning the default JSON responses when you `raise` an `HTTPException` and when the request has invalid data.  
You can override these exception handlers with your own.

This is available since FastAPI version `0.67.0`. 🔖

Answer
----------------------------------------
WebSocketException in FastAPI is a type of exception that can be raised to show errors to the client when using WebSocket connections. It can be imported directly from `fastapi` using `from fastapi import WebSocketException`. FastAPI has default exception handlers for handling these exceptions and returning default JSON responses.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes, but incomplete.

The answer correctly identifies that WebSocketException in FastAPI is a type of exception for WebSocket connections. However, it lacks a clear and concise explanation of what the exception is used for or what it represents. Providing a brief description or example of its usage would improve the answer.

Evaluation_score
----------------------------------------
3

Ground truth answer
WebSocketException is a type of exception in FastAPI that can be raised to show errors to the client during WebSocket connections.

====================================================================================================

Question
----------------------------------------
How to import WebSocketException in FastAPI?

Documents
----------------------------------------
These are the exceptions that you can raise to show errors to the client.  
When you raise an exception, as would happen with normal Python, the rest of the execution is aborted. This way you can raise these exceptions from anywhere in the code to abort a request and show the error to the client.  
You can use:  
* `HTTPException`
* `WebSocketException`  
These exceptions can be imported directly from `fastapi`:  
```python
from fastapi import HTTPException, WebSocketException
```  
::: fastapi.HTTPException  
::: fastapi.WebSocketException

Here's the reference information for the `APIRouter` class, with all its parameters, attributes and methods.  
You can import the `APIRouter` class directly from `fastapi`:  
```python
from fastapi import APIRouter
```  
::: fastapi.APIRouter
options:
members:
- websocket
- include_router
- get
- put
- post
- delete
- options
- head
- patch
- trace
- on_event

This is available since FastAPI version `0.67.0`. 🔖

Now you can import and use the client code, it could look like this, notice that you get autocompletion for the methods:  
<img src="/img/tutorial/generate-clients/image02.png">  
You will also get autocompletion for the payload to send:  
<img src="/img/tutorial/generate-clients/image03.png">  
!!! tip
Notice the autocompletion for `name` and `price`, that was defined in the FastAPI application, in the `Item` model.  
You will have inline errors for the data that you send:  
<img src="/img/tutorial/generate-clients/image04.png">  
The response object will also have autocompletion:  
<img src="/img/tutorial/generate-clients/image05.png">

Answer
----------------------------------------
You can import WebSocketException in FastAPI using the following line of code:

```python
from fastapi import HTTPException, WebSocketException
```
Reason: The statement is copied directly from the context and is relevant to the question.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes, but could be more specific.

The answer does provide a valid way to import WebSocketException in FastAPI, but it doesn't explicitly mention the need to import HTTPException as well. To specifically answer the question, it would be ideal to only mention the import statement for WebSocketException.

Evaluation_score
----------------------------------------
3

Ground truth answer
WebSocketException can be imported directly from FastAPI using `from fastapi import WebSocketException`.

====================================================================================================

Question
----------------------------------------
How to import HTTPException in FastAPI?

Documents
----------------------------------------
**FastAPI** has some default exception handlers.  
These handlers are in charge of returning the default JSON responses when you `raise` an `HTTPException` and when the request has invalid data.  
You can override these exception handlers with your own.

You can use the `TestClient` class to test FastAPI applications without creating an actual HTTP and socket connection, just communicating directly with the FastAPI code.  
Read more about it in the [FastAPI docs for Testing](https://fastapi.tiangolo.com/tutorial/testing/).  
You can import it directly from `fastapi.testclient`:  
```python
from fastapi.testclient import TestClient
```  
::: fastapi.testclient.TestClient

If you catch an exception using `except` in a dependency with `yield` and you don't raise it again (or raise a new exception), FastAPI won't be able to notice there was an exception, the same way that would happen with regular Python:  
=== "Python 3.9+"  
```Python hl_lines="15-16"
{!> ../../../docs_src/dependencies/tutorial008c_an_py39.py!}
```  
=== "Python 3.8+"  
```Python hl_lines="14-15"
{!> ../../../docs_src/dependencies/tutorial008c_an.py!}
```  
=== "Python 3.8+ non-Annotated"  
!!! tip
Prefer to use the `Annotated` version if possible.  
```Python hl_lines="13-14"
{!> ../../../docs_src/dependencies/tutorial008c.py!}
```  
In this case, the client will see an *HTTP 500 Internal Server Error* response as it should, given that we are not raising an `HTTPException` or similar, but the server will **not have any logs** or any other indication of what was the error. 😱

You can define *path operation function* parameters to be of the type `UploadFile` to receive files from the request.  
You can import it directly from `fastapi`:  
```python
from fastapi import UploadFile
```  
::: fastapi.UploadFile
options:
members:
- file
- filename
- size
- headers
- content_type
- read
- write
- seek
- close

Answer
----------------------------------------
You can import HTTPException in FastAPI by using the following line of code:

```python
from fastapi import HTTPException
```
Context is relevant to the question.
Reason: N/A

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The answer provides the correct code for importing HTTPException in FastAPI.

Evaluation_score
----------------------------------------
5

Ground truth answer
HTTPException can be imported directly from FastAPI using `from fastapi import HTTPException`.

====================================================================================================

Question
----------------------------------------
What is HTTPException in FastAPI?

Documents
----------------------------------------
**FastAPI** has some default exception handlers.  
These handlers are in charge of returning the default JSON responses when you `raise` an `HTTPException` and when the request has invalid data.  
You can override these exception handlers with your own.

These are the exceptions that you can raise to show errors to the client.  
When you raise an exception, as would happen with normal Python, the rest of the execution is aborted. This way you can raise these exceptions from anywhere in the code to abort a request and show the error to the client.  
You can use:  
* `HTTPException`
* `WebSocketException`  
These exceptions can be imported directly from `fastapi`:  
```python
from fastapi import HTTPException, WebSocketException
```  
::: fastapi.HTTPException  
::: fastapi.WebSocketException

To return HTTP responses with errors to the client you use `HTTPException`.

Here's the reference information for the `FastAPI` class, with all its parameters, attributes and methods.  
You can import the `FastAPI` class directly from `fastapi`:  
```python
from fastapi import FastAPI
```  
::: fastapi.FastAPI
options:
members:
- openapi_version
- webhooks
- state
- dependency_overrides
- openapi
- websocket
- include_router
- get
- put
- post
- delete
- options
- head
- patch
- trace
- on_event
- middleware
- exception_handler

Answer
----------------------------------------
HTTPException in FastAPI is a type of exception that can be raised to return HTTP responses with errors to the client. It can be imported directly from `fastapi`. The rest of the execution is aborted when an exception is raised.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The answer provides a clear and concise definition of HTTPException in FastAPI, including its purpose and how it can be imported. The explanation is correct and detailed enough to be helpful.

Evaluation_score
----------------------------------------
5

Ground truth answer
HTTPException is a type of exception in FastAPI that can be raised to show errors to the client during HTTP requests.

====================================================================================================

Question
----------------------------------------
Debug FastAPI in Pycharm?

Documents
----------------------------------------
Let's create the application!  
1. Create a new Python file by using **File** > **New File…** and then select **Python File**.  
2. Save it as `main.py` (`kb(workbench.action.files.saveAs)`) in the `groceries-plugin` folder.  
3. Add the following code to `main.py` and save the file:  
```python
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def root():
return {"message": "Hello World"}
```  
4. Run the code by starting up the debugger (`kb(workbench.action.debug.start)`).  
5. From the dropdown menu, select the **FastAPI** configuration option from the list:  
![Dropdown with debugger configuration options, with FastAPI being highlighted](images/fastapi-tutorial/fastapi_debug_config_option.png)  
This invokes uvicorn to start the application server through the debugger and allow you to step through the source code to inspect its behavior. You should see something like the following in the terminal:  
![Uvicorn server running message displayed in the terminal, with an URL to access the app](images/fastapi-tutorial/fastapi_debug_terminal.png)  
6. `kbstyle(Ctrl+Click)` the `http://127.0.0.1:8000/` URL in the terminal to open your default browser to that address:  
![Hello World message displayed in the browser](images/fastapi-tutorial/helloworld_browser.png)  
Congratulations! Your FastAPI app is up and running!  
7. Stop the debugger by using the **Stop** button in the debug toolbar, or through `kb(workbench.action.debug.stop)`.

Pydantic includes the tests for **FastAPI** with its own tests, so new versions of Pydantic (above `1.0.0`) are always compatible with FastAPI.  
You can pin Pydantic to any version above `1.0.0` that works for you and below `2.0.0`.  
For example:  
```txt
pydantic>=1.2.0,<2.0.0
```

Before FastAPI 0.110.0, if you used a dependency with `yield`, and then you captured an exception with `except` in that dependency, and you didn't raise the exception again, the exception would be automatically raised/forwarded to any exception handlers or the internal server error handler.  
This was changed in version 0.110.0 to fix unhandled memory consumption from forwarded exceptions without a handler (internal server errors), and to make it consistent with the behavior of regular Python code.

You can connect the debugger in your editor, for example with Visual Studio Code or PyCharm.

Answer
----------------------------------------
To debug a FastAPI application in PyCharm, follow these steps:

1. Create a new Python file named `main.py` in the `groceries-plugin` folder and add the FastAPI code as shown in the context.
2. Save the file and start the debugger (`kb(workbench.action.debug.start)`).
3. Select the **FastAPI** configuration option from the dropdown menu. This will start the application server through the debugger.
4. The terminal will display the URL for the application. Click on it to open the application in your default browser.
5. To stop the debugger, use the **Stop** button in the debug toolbar or through `kb(workbench.action.debug.stop)`.

Regarding the version of Pydantic, you can pin it to any version above `1.0.0` that works for you and below `2.0.0`, for example:
```
pydantic>=1.2.0,<2.0.0
```
Debugging can also be done in PyCharm by connecting the debugger in the editor.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The response directly addresses the question of how to debug a FastAPI application in PyCharm and provides a step-by-step guide for the process. The response also addresses an additional question regarding pinning the version of Pydantic. Overall, the response is comprehensive, clear, and accurate.

Evaluation_score
----------------------------------------
5

Ground truth answer
The document provides steps on how to debug a FastAPI application in Pycharm by opening the "Run" menu, selecting the option "Debug...", and then selecting the file to debug.

====================================================================================================

Question
----------------------------------------
Run FastAPI with debugger?

Documents
----------------------------------------
Let's create the application!  
1. Create a new Python file by using **File** > **New File…** and then select **Python File**.  
2. Save it as `main.py` (`kb(workbench.action.files.saveAs)`) in the `groceries-plugin` folder.  
3. Add the following code to `main.py` and save the file:  
```python
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def root():
return {"message": "Hello World"}
```  
4. Run the code by starting up the debugger (`kb(workbench.action.debug.start)`).  
5. From the dropdown menu, select the **FastAPI** configuration option from the list:  
![Dropdown with debugger configuration options, with FastAPI being highlighted](images/fastapi-tutorial/fastapi_debug_config_option.png)  
This invokes uvicorn to start the application server through the debugger and allow you to step through the source code to inspect its behavior. You should see something like the following in the terminal:  
![Uvicorn server running message displayed in the terminal, with an URL to access the app](images/fastapi-tutorial/fastapi_debug_terminal.png)  
6. `kbstyle(Ctrl+Click)` the `http://127.0.0.1:8000/` URL in the terminal to open your default browser to that address:  
![Hello World message displayed in the browser](images/fastapi-tutorial/helloworld_browser.png)  
Congratulations! Your FastAPI app is up and running!  
7. Stop the debugger by using the **Stop** button in the debug toolbar, or through `kb(workbench.action.debug.stop)`.

Now let's use the utilities provided by **FastAPI** to handle this.

**FastAPI** is already being used in production in many applications and systems. And the test coverage is kept at 100%. But its development is still moving quickly.  
New features are added frequently, bugs are fixed regularly, and the code is still continuously improving.  
That's why the current versions are still `0.x.x`, this reflects that each version could potentially have breaking changes. This follows the <a href="https://semver.org/" class="external-link" target="_blank">Semantic Versioning</a> conventions.  
You can create production applications with **FastAPI** right now (and you have probably been doing it for some time), you just have to make sure that you use a version that works correctly with the rest of your code.

You can connect the debugger in your editor, for example with Visual Studio Code or PyCharm.

Answer
----------------------------------------
To run FastAPI with the debugger, follow the steps provided in the context:
1. Create a new Python file named `main.py` in the `groceries-plugin` folder.
2. Add the FastAPI code to `main.py`.
3. Start the debugger and select the **FastAPI** configuration option.
4. The debugger will start the FastAPI application server, and you can access it in your browser using the provided URL.

Therefore, the answer to the question is:
1. Create a new Python file as `main.py`.
2. Add the FastAPI code to `main.py`.
3. Run the code by starting up the debugger and selecting the **FastAPI** configuration option.
4. Access the FastAPI application in your browser.

Reason: The context provided is relevant to the question.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes.

(The answer does provide steps to run FastAPI with the debugger, but it could have been more direct.)

Evaluation_score
----------------------------------------
3

Ground truth answer
You can run your FastAPI application with the debugger by calling your Python program directly from the debugger.

====================================================================================================

Question
----------------------------------------
uvicorn run code example?

Documents
----------------------------------------
Uvicorn also has an option to start and run several **worker processes**.  
Nevertheless, as of now, Uvicorn's capabilities for handling worker processes are more limited than Gunicorn's. So, if you want to have a process manager at this level (at the Python level), then it might be better to try with Gunicorn as the process manager.  
In any case, you would run it like this:  
<div class="termy">  
```console
$ uvicorn main:app --host 0.0.0.0 --port 8080 --workers 4
<font color="#A6E22E">INFO</font>:     Uvicorn running on <b>http://0.0.0.0:8080</b> (Press CTRL+C to quit)
<font color="#A6E22E">INFO</font>:     Started parent process [<font color="#A1EFE4"><b>27365</b></font>]
<font color="#A6E22E">INFO</font>:     Started server process [<font color="#A1EFE4">27368</font>]
<font color="#A6E22E">INFO</font>:     Waiting for application startup.
<font color="#A6E22E">INFO</font>:     Application startup complete.
<font color="#A6E22E">INFO</font>:     Started server process [<font color="#A1EFE4">27369</font>]
<font color="#A6E22E">INFO</font>:     Waiting for application startup.
<font color="#A6E22E">INFO</font>:     Application startup complete.
<font color="#A6E22E">INFO</font>:     Started server process [<font color="#A1EFE4">27370</font>]
<font color="#A6E22E">INFO</font>:     Waiting for application startup.
<font color="#A6E22E">INFO</font>:     Application startup complete.
<font color="#A6E22E">INFO</font>:     Started server process [<font color="#A1EFE4">27367</font>]
<font color="#A6E22E">INFO</font>:     Waiting for application startup.
<font color="#A6E22E">INFO</font>:     Application startup complete.
```  
</div>  
The only new option here is `--workers` telling Uvicorn to start 4 worker processes.  
You can also see that it shows the **PID** of each process, `27365` for the parent process (this is the **process manager**) and one for each worker process: `27368`, `27369`, `27370`, and `27367`.

Because you are running the Uvicorn server directly from your code, you can call your Python program (your FastAPI application) directly from the debugger.  
---  
For example, in Visual Studio Code, you can:  
* Go to the "Debug" panel.
* "Add configuration...".
* Select "Python"
* Run the debugger with the option "`Python: Current File (Integrated Terminal)`".  
It will then start the server with your **FastAPI** code, stop at your breakpoints, etc.  
Here's how it might look:  
<img src="/img/tutorial/debugging/image01.png">  
---  
If you use Pycharm, you can:  
* Open the "Run" menu.
* Select the option "Debug...".
* Then a context menu shows up.
* Select the file to debug (in this case, `main.py`).  
It will then start the server with your **FastAPI** code, stop at your breakpoints, etc.  
Here's how it might look:  
<img src="/img/tutorial/debugging/image02.png">

Now that you are more familiar with Python in VS Code, let’s learn how to run, debug, and test your code.

To understand, compare, and see the difference between Uvicorn, Starlette and FastAPI, check the section about [Benchmarks](benchmarks.md){.internal-link target=_blank}.

Answer
----------------------------------------
To run the Uvicorn server with 4 worker processes, use the command:

```bash
uvicorn main:app --host 0.0.0.0 --port 8080 --workers 4
```

This command starts the Uvicorn server on `http://0.0.0.0:8080` with 4 worker processes. The **PID** (process ID) of each process will be displayed during the server startup.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not directly provide the command to run Uvicorn, but instead explains how to interpret the command. It does not answer the question as asked.

Evaluation_score
----------------------------------------
2

Ground truth answer
The document provides an example of how to import and run `uvicorn` directly in your FastAPI application.

====================================================================================================

Question
----------------------------------------
Purpose __name__ == "__main__" ?

Documents
----------------------------------------
The main purpose of the `__name__ == "__main__"` is to have some code that is executed when your file is called with:  
<div class="termy">  
```console
$ python myapp.py
```  
</div>  
but is not called when another file imports it, like in:  
```Python
from myapp import app
```

- `commandCenter.foreground`: Foreground color of the Command Center.
- `commandCenter.activeForeground`: Active foreground color of the Command Center.
- `commandCenter.background`: Background color of the Command Center.
- `commandCenter.activeBackground`: Active background color of the Command Center.
- `commandCenter.border`: Border color of the Command Center.
- `commandCenter.inactiveForeground`: Foreground color of the Command Center when the window is inactive.
- `commandCenter.inactiveBorder`: Border color of the Command Center when the window is inactive.
- `commandCenter.activeBorder`: Active border color of the Command Center.
- `commandCenter.debuggingBackground`: Command Center background color when a program is being debugged.

You can use the same dependency *functions* you use normally.

Two new context keys have been added for resources.  
* `resourceDirname` for the folder path of the resource, equivalent to `dirname(uri.fsPath)`
* `resourcePath` for the full path of the resource, equivalent to `uri.fsPath`

Answer
----------------------------------------
Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not address the purpose of the __name__ == "__main__" condition in Python. It simply provides unrelated information about the context and model used for generating the response.

Evaluation_score
----------------------------------------
1

Ground truth answer
The main purpose of `__name__ == "__main__"` is to have some code that is executed when the file is called directly, but not when another file imports it.

====================================================================================================

Question
----------------------------------------
How to disable Wi-Fi power save?

Documents
----------------------------------------
- Browse to the upper left corner, click the horizontal line to open the menu.
- Type out the word terminal, click to open it.
- Click on the small icon shown in the image below to copy the code below in the gray box, right click/paste it into the terminal window.  
```
sudo apt install iw && interface=$(nmcli -t -f active,device d wifi list | grep '^yes' | cut -d':' -f2) && echo -e "\n\033[1;33mInterface:\033[0m $interface" && iw dev $interface get power_save
```
This will indicate if your Wi-Fi power save is on or off. If it's on and you're seeing drops, continue to the next line.  
```
sudo sed -i '/^wifi.powersave = 3$/s/3/2/' /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf
```  
This will change 3 into 2, which will disable powersave.  
> **TIP:** You can use the little clipboard icon to the right of the code to copy to your clipboard.  
<p style="text-align: left"><img src="https://raw.githubusercontent.com/FrameworkComputer/linux-docs/main/copied.png" alt="Copy The Code Below Like This" title="Copy The Code Above Like This"></p>  
**reboot.**  
&nbsp; &nbsp; &nbsp; &nbsp;

``sudo gnome-text-editor /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf``

``sudo gedit /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf``

``wifi.powersave = 2``

Answer
----------------------------------------
To disable Wi-Fi power save, follow these steps:

1. Open the terminal by clicking on the horizontal line in the upper left corner and typing "terminal".
2. Copy the command `sudo apt install iw && interface=$(nmcli -t -f active,device d wifi list | grep '^yes' | cut -d':' -f2) && echo -e "\n\033[1;33mInterface:\033[0m $interface" && iw dev $interface get power_save` and paste it into the terminal window. This will indicate if your Wi-Fi power save is on or off.
3. If it's on and you're seeing drops, copy the command `sudo sed -i '/^wifi.powersave = 3$/s/3/2/' /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf` and paste it into the terminal window. This will change 3 into 2, which will disable powersave.
4. Reboot your system.

Alternatively, you can directly edit the configuration file by running the command `sudo gnome-text-editor /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf` or `sudo gedit /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf` and set `wifi.powersave = 2`.

Make sure to run these commands with administrative privileges (using `sudo`).

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.
The answer provides a clear and detailed process to disable Wi-Fi power save on a system. It might be more concise, but the required information is complete and accurate. The answer could have been improved if it had been formatted to make it more readable, but it doesn't affect the overall understanding.

Evaluation_score
----------------------------------------
5

Ground truth answer
Use the command "sudo sed -i '/^wifi.powersave = 3$/s/3/2/' /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf" in the terminal.

====================================================================================================

Question
----------------------------------------
How to disable ALS sensor for brightness keys?

Documents
----------------------------------------
This will address your brightness keys.  
``
GRUB_CMDLINE_LINUX_DEFAULT="quiet splash module_blacklist=hid_sensor_hub"
``

- Browse to Activities in the upper left corner, click to open it.
- Type out the word terminal, click to open it.
- Left click and drag to highlight and copy the code below in the gray box, right click/paste to copy it into the terminal window.
- **Then press the enter key, password, reboot.**  
``
sudo apt update && sudo apt upgrade -y && sudo snap refresh && echo "options snd-hda-intel model=dell-headset-multi" | sudo tee -a /etc/modprobe.d/alsa-base.conf && gsettings set org.gnome.mutter experimental-features "['scale-monitor-framebuffer']" && sudo sed -i 's/^GRUB_CMDLINE_LINUX_DEFAULT.*/GRUB_CMDLINE_LINUX_DEFAULT="quiet splash module_blacklist=hid_sensor_hub nvme.noacpi=1"/g' /etc/default/grub && sudo update-grub && echo "[connection]" | sudo tee /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf && echo "wifi.powersave = 2" | sudo tee -a /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf
``

- Disables the ALS sensor so that your brightness keys work.
- Ensures GRUB is using the latest OEM D kernel at every boot.
- Creates a desktop file as an autostart to check for OEM kernel status.
- If an update comes about for the OEM kernel, is installed, but GRUB still has the older version - an alert box will provide you with a link to get this corrected.  
&nbsp; &nbsp; &nbsp; &nbsp;

- Update your Ubuntu install's packages.
- Provide a workaround for dropped Intel AX210 wifi.
- Disable the ALS sensor so that your brightness keys work.
- We are NOT recommending an OEM kernel at this time, this may change in the future. Default kernel is where you need to be.  
&nbsp; &nbsp; &nbsp; &nbsp;

Answer
----------------------------------------
To disable the ALS (automatic brightness sensor) for the brightness keys, follow these steps:

1. Copy the following code: `sudo sed -i 's/^GRUB_CMDLINE_LINUX_DEFAULT.*/GRUB_CMDLINE_LINUX_DEFAULT="quiet splash module_blacklist=hid_sensor_hub"/g' /etc/default/grub && sudo update-grub`
2. Open the terminal.
3. Paste the code into the terminal and press enter.
4. Enter your password and press enter.
5. Restart your system.

This will disable the ALS sensor and allow your brightness keys to work.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The answer provides a clear and detailed set of instructions for disabling the ALS sensor for the brightness keys. The instructions are easy to follow and include the necessary code to be copied and pasted into the terminal. The use of the `sed` command to modify the Grub configuration file is appropriate and effective. The response is also formatted in a way that is easy to read and understand.

Evaluation_score
----------------------------------------
5

Ground truth answer
The document does not provide specific instructions for this task.

====================================================================================================

Question
----------------------------------------
What is the workaround for dropped Intel AX210 wifi?

Documents
----------------------------------------
- Update your Ubuntu install's packages.
- Provide a workaround for dropped Intel AX210 wifi.
- We are NOT recommending an OEM kernel at this time, this may change in the future. Default kernel is where you need to be.  
&nbsp; &nbsp; &nbsp; &nbsp;

- Browse to the upper left corner, click the horizontal line to open the menu.
- Type out the word terminal, click to open it.
- Click on the small icon shown in the image below to copy the code below in the gray box, right click/paste it into the terminal window.  
```
sudo apt install iw && interface=$(nmcli -t -f active,device d wifi list | grep '^yes' | cut -d':' -f2) && echo -e "\n\033[1;33mInterface:\033[0m $interface" && iw dev $interface get power_save
```
This will indicate if your Wi-Fi power save is on or off. If it's on and you're seeing drops, continue to the next line.  
```
sudo sed -i '/^wifi.powersave = 3$/s/3/2/' /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf
```  
This will change 3 into 2, which will disable powersave.  
> **TIP:** You can use the little clipboard icon to the right of the code to copy to your clipboard.  
<p style="text-align: left"><img src="https://raw.githubusercontent.com/FrameworkComputer/linux-docs/main/copied.png" alt="Copy The Code Below Like This" title="Copy The Code Above Like This"></p>  
**reboot.**  
&nbsp; &nbsp; &nbsp; &nbsp;

- [MediaTek WiFi Dropout on WiFi 6E routers fix](#mediatek-wifi-dropout-on-wifi-6e-routers)
- [Suspend wakes up unexpectedly or fails to remain in a suspend state](#suspend-keeps-waking-up-or-fails-to-suspend)

- Go to the Linux Mint Launcher or press the super key.
- Type out the word terminal, click to open it.
- Left click and drag to highlight and copy the code below in the gray box, right click/paste to copy it into the terminal window.
- **Then press the enter key, password, reboot.**  
``
sudo apt update && sudo apt upgrade -y && sudo apt-get install linux-oem-22.04c -y && echo "options snd-hda-intel model=dell-headset-multi" | sudo tee -a /etc/modprobe.d/alsa-base.conf && sudo sed -i 's/^GRUB_CMDLINE_LINUX_DEFAULT.*/GRUB_CMDLINE_LINUX_DEFAULT="quiet splash nvme.noacpi=1"/g' /etc/default/grub && sudo update-grub && echo "[connection]" | sudo tee /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf && echo "wifi.powersave = 2" | sudo tee -a /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf
``

Answer
----------------------------------------
The workaround for dropped Intel AX210 wifi is to change the Wi-Fi power save setting from 3 to 2 in the /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf file using the provided command, and then reboot. This will disable power save mode and may help prevent dropped connections.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes.

Although the response does provide a potential workaround for the dropped Intel AX210 wifi issue, it is not explicitly answering the question as it starts with "The workaround...". A more direct answer would be "Change the Wi-Fi power save setting from 3 to 2 in the /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf file using the provided command, and then reboot. This will disable power save mode and may help prevent dropped connections."

Evaluation_score
----------------------------------------
3

Ground truth answer
Disable power save for Intel Wi-Fi by executing specific commands in the terminal.

====================================================================================================

Question
----------------------------------------
Which kernel is recommended for the 12th Gen Intel Core Framework Laptop 13?

Documents
----------------------------------------
- Getting  your laptop fully updated.
- Allow both CPU and platform drivers to be simultaneously active.
- Enable improved fractional scaling support Fedora's GNOME environment using Wayland.
- Enabling tap to click on the touchpad.  
&nbsp;
&nbsp;
&nbsp;

This guide has been moved and improved at https://knowledgebase.frame.work/en_us/updating-fingerprint-reader-firmware-on-linux-for-13th-gen-and-amd-ryzen-7040-series-laptops-HJrvxv_za

- Please follow the steps outlined in this guide:
https://knowledgebase.frame.work/allocate-additional-ram-to-igpu-framework-laptop-13-amd-ryzen-7040-series-BkpPUPQa  
&nbsp;
&nbsp;
&nbsp;  
---------

- Update your Ubuntu install's packages.
- Provide a workaround for dropped Intel AX210 wifi.
- We are NOT recommending an OEM kernel at this time, this may change in the future. Default kernel is where you need to be.  
&nbsp; &nbsp; &nbsp; &nbsp;

Answer
----------------------------------------
The recommended kernel for the 12th Gen Intel Core Framework Laptop 1

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer doesn't provide any information about the recommended kernel for the 12th Gen Intel Core Framework Laptop 13.

Evaluation_score
----------------------------------------
1

Ground truth answer
The default kernel is recommended.

====================================================================================================

Question
----------------------------------------
linux docs related files?

Documents
----------------------------------------
The documentation uses <a href="https://www.mkdocs.org/" class="external-link" target="_blank">MkDocs</a>.  
And there are extra tools/scripts in place to handle translations in `./scripts/docs.py`.  
!!! tip
You don't need to see the code in `./scripts/docs.py`, you just use it in the command line.  
All the documentation is in Markdown format in the directory `./docs/en/`.  
Many of the tutorials have blocks of code.  
In most of the cases, these blocks of code are actual complete applications that can be run as is.  
In fact, those blocks of code are not written inside the Markdown, they are Python files in the `./docs_src/` directory.  
And those Python files are included/injected in the documentation when generating the site.

Here are several pointers to other places in the docs, for general or frequent questions.

With the [Outline view](/docs/getstarted/userinterface.md#outline-view), you can conveniently navigate the members within the current file. [Projects view](/docs/java/java-project.md#projects-view) also provide a great overview of your project. As a Java editor, it also supports Call Hierarchy, Type Hierarchy, Definition Navigation, Search Types in Workspace, etc.

Later, when reading about how to structure bigger applications ([Bigger Applications - Multiple Files](../../tutorial/bigger-applications.md){.internal-link target=_blank}), possibly with multiple files, you will learn how to declare a single `dependencies` parameter for a group of *path operations*.

Answer
----------------------------------------
Reason: The context provided is about the documentation structure and tools used for a specific project, but it does not contain information about any linux docs related files.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not provide any information about the specific Linux docs related files.

Evaluation_score
----------------------------------------
2

Ground truth answer
This repo not only serves as a place to host the formatted code blocks and instructions for the Linux guides, but also related files for the Linux documents.

====================================================================================================

Question
----------------------------------------
markdown guides repo purpose?

Documents
----------------------------------------
This repo serves as a place to host the formatted code blocks and instructions for the Linux guides.

The topics in this repository use Markdown.  Here is a good overview of [Markdown basics](https://docs.github.com/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax).

When a `TreeItem` has a trusted Markdown tooltip, that tooltip can contain commands of the format `[this is a link](command:workbench.action.quickOpenView)`. This follows the same format as all other places where command links are supported in Markdown.

The VS Code Markdown preview supports the [CommonMark specification](https://spec.commonmark.org). Extensions can add support for additional Markdown syntax by contributing a [markdown-it plugin.](https://github.com/markdown-it/markdown-it#syntax-extensions)  
To contribute a markdown-it plugin, first add a `"markdown.markdownItPlugins"` contribution in your extension's `package.json`:  
```json
"contributes": {
"markdown.markdownItPlugins": true
}
```  
Then, in the extension's main `activation` function, return an object with a function named `extendMarkdownIt`. This function takes the current markdown-it instance and must return a new markdown-it instance:  
```ts
import * as vscode from 'vscode';

export function activate(context: vscode.ExtensionContext) {
return {
extendMarkdownIt(md: any) {
return md.use(require('markdown-it-emoji'));
}
};
}
```  
To contribute multiple markdown-it plugins, return multiple `use` statements chained together:  
```ts
return md.use(require('markdown-it-emoji')).use(require('markdown-it-hashtag'));
```  
Extensions that contribute markdown-it plugins are activated lazily, when a Markdown preview is shown for the first time.  
The [markdown-emoji](https://marketplace.visualstudio.com/items?itemName=bierner.markdown-emoji) extension demonstrates using a markdown-it plugin to add emoji support to the markdown preview. You can review the Emoji extension's source code on [GitHub](https://github.com/mjbvz/vscode-markdown-emoji).  
You may also want to review:  
- [Guidelines](https://github.com/markdown-it/markdown-it/blob/master/docs/development.md) for markdown-it plugin developers
- [Existing markdown-it plugins](https://www.npmjs.com/browse/keyword/markdown-it-plugin)

Answer
----------------------------------------
The purpose of the "markdown guides" repo is to host formatted code blocks and instructions for Linux guides, using Markdown syntax. The repository contains information on how to contribute to the Markdown rendering in Visual Studio Code by adding markdown-it plugins.

Evaluation
----------------------------------------
Score (1-5): 4
Answered: Yes.

The answer provides a clear and accurate description of the "markdown guides" repo's purpose. However, it could be more specific and refer to the exact location of the repository (e.g., GitHub) and the fact that it's used for Visual Studio Code documentation.

Evaluation_score
----------------------------------------
4

Ground truth answer
The purpose of this repo is to host the formatted code blocks and instructions for the Linux guides, which are in markdown format.

====================================================================================================

Question
----------------------------------------
markdown guides format?

Documents
----------------------------------------
The topics in this repository use Markdown.  Here is a good overview of [Markdown basics](https://docs.github.com/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax).

---
Order: 7
Area: languages
TOCTitle: Markdown
ContentId: 47A8BA5A-A103-4B61-B5FB-185C15E54C52
PageTitle: Markdown editing with Visual Studio Code
DateApproved: 05/02/2024
MetaDescription: Get the best out of Visual Studio Code for Markdown
---

The CSS Languages Features extension also provides a formatter. The formatter works with CSS, LESS and SCSS. It is implemented by the [JS Beautify library](https://github.com/beautify-web/js-beautify) and comes with the following settings:  
* `css.format.enable` - Enable/disable default CSS formatter.
* `css.format.newlineBetweenRules` - Separate rulesets by a blank line.
* `css.format.newlineBetweenSelectors` - Separate selectors with a new line.
* `css.format.spaceAroundSelectorSeparator` - Ensure a space character around selector separators '>', '+', '~' (for example, `a > b`).  
The same settings also exist for `less` and `scss`.

When a `TreeItem` has a trusted Markdown tooltip, that tooltip can contain commands of the format `[this is a link](command:workbench.action.quickOpenView)`. This follows the same format as all other places where command links are supported in Markdown.

Answer
----------------------------------------
The formatting of Markdown guides is done using the CSS Languages Features extension in Visual Studio Code, which is implemented by the JS Beautify library. It has settings such as `css.format.enable`, `css.format.newlineBetweenRules`, `css.format.newlineBetweenSelectors`, and `css.format.spaceAroundSelectorSeparator` for customizing the formatting style. Markdown tooltips can contain commands of the format `[this is a link](command:workbench.action.quickOpenView)`. This format is consistent with other places where command links are supported in Markdown.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes, but incomplete.

The answer does mention the formatting of Markdown guides and some of the settings used for customizing the formatting style, but it does not provide a clear and complete answer to the question. The answer could be improved by providing a more concise and clear explanation of the Markdown guides format.

Evaluation_score
----------------------------------------
3

Ground truth answer
This repo serves as a place to host the formatted code blocks and instructions for the Linux guides, which are in markdown format.

====================================================================================================

Question
----------------------------------------
linux docs where hosted?

Documents
----------------------------------------
This repo serves as a place to host the formatted code blocks and instructions for the Linux guides.

Open the interactive docs: <a href="http://127.0.0.1:8000/docs" class="external-link" target="_blank">http://127.0.0.1:8000/docs</a>.

As long as your Linux VM supports SSH, it can be hosted anywhere; on your local host, on premise, in Azure, or in any other cloud. You can also be on any client - Windows, macOS, or Linux.  
**Note**: macOS and Windows SSH hosts are not yet supported.  
In this blog post, we'll use a Windows client and target a Linux VM on Azure.

As already mentioned above, you can use the `./scripts/docs.py` with the `live` command to preview the results (or `mkdocs serve`).  
Once you are done, you can also test it all as it would look online, including all the other languages.  
To do that, first build all the docs:  
<div class="termy">  
```console
// Use the command "build-all", this will take a bit
$ python ./scripts/docs.py build-all

Building docs for: en
Building docs for: es
Successfully built docs for: es
```  
</div>  
This builds all those independent MkDocs sites for each language, combines them, and generates the final output at `./site/`.  
Then you can serve that with the command `serve`:  
<div class="termy">  
```console
// Use the command "serve" after running "build-all"
$ python ./scripts/docs.py serve

Warning: this is a very simple server. For development, use mkdocs serve instead.
This is here only to preview a site with translations already built.
Make sure you run the build-all command first.
Serving at: http://127.0.0.1:8008
```  
</div>

Answer
----------------------------------------
The Linux docs are built and hosted in a Linux VM, which can be located on-premise or in a cloud service like Azure. The final output of the built docs is served from `./site/`. The hosting of the docs is not restricted to a specific location, as long as SSH is supported on the Linux VM.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes, but incomplete.

The answer does provide some information about where the Linux docs are hosted, but it doesn't give a clear and specific answer to the question.

Evaluation_score
----------------------------------------
3

Ground truth answer
This repo serves as a place to host the formatted code blocks and instructions for the Linux guides.

====================================================================================================

Question
----------------------------------------
Start debugging

Documents
----------------------------------------
Before you start stepping through the code, let's take a moment to notice several changes in the user interface:  
- The Integrated Terminal appears at the bottom of the source code editor. In the **Debug Console** tab, you see output that indicates the debugger is up and running.
- The editor highlights the line where you set a breakpoint before starting the debugger:  
![Initial breakpoint](images/playbutton/breakpoint-debug.png)  
- The **Run and Debug** view in the Activity Bar shows debugging information.  
- At the top of the code editor, a debugging control panel appears. You can move this around the screen by grabbing the dots on the left side.  
![Debugging controls](images/cpp/debug-controls.png)

Full path to debug server to launch.

The process you're debugging is not built in Debug, make sure to build as debugging before debugging the process.

For the `C++ (GDB/LLDB)` debugging environment, you can execute GDB, LLDB and LLDB-MI commands directly through the debug console with the `-exec` command, but be careful, executing commands directly in the debug console is untested and might crash VS Code in some cases.

Answer
----------------------------------------
Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide any guidance or solution to start debugging. It simply states that the context is irrelevant to the question.

Evaluation_score
----------------------------------------
1

Ground truth answer
The command for starting to debug is `kb(workbench.action.debug.start)`.

====================================================================================================

Question
----------------------------------------
Close editor?

Documents
----------------------------------------
There are situations where you can end up with many dirty editors open (for example, you ran a search and replace operation without saving). Previously, when you wanted to close them all, you were asked to Save or Revert each editor one by one. Now, when you invoke the **Close All Editors** action (`kb(workbench.action.closeAllEditors)`), you will see a combined prompt for all dirty files:  
![Close All Prompt](images/1_9/closeall.png)

We always provided an action to reopen an editor after it was closed, `workbench.action.reopenClosedEditor` (`kb(workbench.action.reopenClosedEditor)`). With this release, editors will now reopen at the same index, preserving the order that existed before they were closed.

If you'd like an editor tab to always be visible, you can pin it to the editor tab bar. You can pin an editor tab from either the context menu or using the command **View: Pin Editor** (`kb(workbench.action.pinEditor)`).  
![Pinned editor tab with pin button highlighted](images/custom-layout/pinned-editor-tab.png)  
Pinned tabs help access files that are important to you as:  
* Pinned tabs always appear first before non-pinned tabs.
* They do not scroll out of view if you have many tabs opened.
* They do not close when using editor tab commands such as **Close Others** or **Close All**.
* They do not close even when you exceed a set limit of opened editors.  
Unpin an editor by clicking on the pin icon, using the **Unpin** editor tab context menu item, or the **View: Unpin Editor** command.  
You can choose how you'd like to display pinned editors with the **Workbench > Editor: Pinned Tab Sizing** (`workbench.editor.pinnedTabSizing`) setting. The options are:  
* `normal`: A pinned tab inherits the look of other tabs (default)
* `shrink`: A pinned tab shrinks to a fixed size showing parts of the editor label.
* `compact`: A pinned tab will only show as icon or first letter of the editor label.  
You can also show pinned editor tabs on a separate row above the regular editor tab bar by setting **Workbench > Editor: Pinned Tabs On Separate Row**. You can pin and unpin editors by dragging and dropping their tabs between the two rows.

To make it easier to start and end selections with the keyboard, there are four commands: **Set Selection Anchor** (`kb(editor.action.setSelectionAnchor)`), **Select From Anchor to Cursor** (`kb(editor.action.selectFromAnchorToCursor)`), **Cancel Selection Anchor** (`kb(editor.action.cancelSelectionAnchor)`) and **Go to Selection Anchor**.

Answer
----------------------------------------
Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide a response to the question, "Close editor?" It instead provides metadata and information about the model's token usage and ID, which is irrelevant to the question.

Evaluation_score
----------------------------------------
1

Ground truth answer
To close the current editor in Visual Studio Code, you can use the command `kb(workbench.action.closeActiveEditor)`, `kbstyle(Ctrl+W)`.

====================================================================================================

Question
----------------------------------------
Create new terminal

Documents
----------------------------------------
You can now set the environment of terminal instances launched via the extension API:  
```ts
window.createTerminal({
name: 'My session',
shell: 'zsh',
env: {
JAVA_HOME: '<path>'
}
});
```

* [Integrated Terminal user guide](/docs/terminal/basics.md) - Learn more about general terminal use and configuration.

Run **Terminal** > **New Terminal** (`kb(workbench.action.terminal.toggleTerminal)`) to open a new terminal instance.  
![new terminal in WSL](images/wsl-tutorial/new-terminal-in-wsl.png)  
You'll start a new instance of the bash shell in WSL, again from VS Code running on Windows.  
**Tip**: In the lower left corner of the Status Bar, you can see that you're connected to your **WSL: Ubuntu** instance.  
![WSL extension Status bar](images/wsl-tutorial/wsl-status-bar.png)

Profiles and setting shortcuts have moved out of the dropdown and into a new **+** button with a dropdown.  
![When selected, the dropdown button displays a menu](images/1_56/tabs-dropdown-button.png)  
This also supports creating a new split terminal based on a non-default profile.

Answer
----------------------------------------
To create a new terminal, you can use the shortcut `kb(workbench.action.terminal.toggleTerminal)` or run **Terminal** > **New Terminal** from the menu. The new terminal instance will use the default shell, which is bash in WSL in this context. If you want to set a specific environment for the terminal instances, you can use the `window.createTerminal` API as shown in the provided code snippet.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes, but could be more concise.

Evaluation_score
----------------------------------------
3

Ground truth answer
The command for creating a new terminal is `kb(workbench.action.terminal.new)`.

====================================================================================================

Question
----------------------------------------
Go to Symbol?

Documents
----------------------------------------
You can navigate symbols inside a file with `kb(workbench.action.gotoSymbol)`. By typing `kbstyle(:)` the symbols will be grouped by category. Press `kbstyle(Up)` or `kbstyle(Down)` and navigate to the place you want.  
![Go to Symbol](images/editingevolved/gotosymbol.png)

You can take a quick look at how a symbol was defined by using the **Peek Definition** feature. This feature displays a few lines of code near the definition inside a peek window, so you can take a look without navigating away from your current location.  
To peek at a symbol's definition, place your cursor on the symbol anywhere it's used in your source code and then press `kb(editor.action.peekDefinition)`. Alternatively, you can choose **Peek Definition** from the context menu (right-click, then choose **Peek > Peek Definition**).  
![Peek Definition example](images/navigate-edit/peek-definition.gif)

Symbols are located in the `*.pdb` file.

The theme colors for symbol icons that appears in the Outline view, breadcrumb navigation, and suggest widget:  
- `symbolIcon.arrayForeground`: The foreground color for array symbols.
- `symbolIcon.booleanForeground`: The foreground color for boolean symbols.
- `symbolIcon.classForeground`: The foreground color for class symbols.
- `symbolIcon.colorForeground`: The foreground color for color symbols.
- `symbolIcon.constantForeground`: The foreground color for constant symbols.
- `symbolIcon.constructorForeground`: The foreground color for constructor symbols.
- `symbolIcon.enumeratorForeground`: The foreground color for enumerator symbols.
- `symbolIcon.enumeratorMemberForeground`: The foreground color for enumerator member symbols.
- `symbolIcon.eventForeground`: The foreground color for event symbols.
- `symbolIcon.fieldForeground`: The foreground color for field symbols.
- `symbolIcon.fileForeground`: The foreground color for file symbols.
- `symbolIcon.folderForeground`: The foreground color for folder symbols.
- `symbolIcon.functionForeground`: The foreground color for function symbols.
- `symbolIcon.interfaceForeground`: The foreground color for interface symbols.
- `symbolIcon.keyForeground`: The foreground color for key symbols.
- `symbolIcon.keywordForeground`: The foreground color for keyword symbols.
- `symbolIcon.methodForeground`: The foreground color for method symbols.
- `symbolIcon.moduleForeground`: The foreground color for module symbols.
- `symbolIcon.namespaceForeground`: The foreground color for namespace symbols.
- `symbolIcon.nullForeground`: The foreground color for null symbols.
- `symbolIcon.numberForeground`: The foreground color for number symbols.
- `symbolIcon.objectForeground`: The foreground color for object symbols.
- `symbolIcon.operatorForeground`: The foreground color for operator symbols.
- `symbolIcon.packageForeground`: The foreground color for package symbols.
- `symbolIcon.propertyForeground`: The foreground color for property symbols.
- `symbolIcon.referenceForeground`: The foreground color for reference symbols.
- `symbolIcon.snippetForeground`: The foreground color for snippet symbols.
- `symbolIcon.stringForeground`: The foreground color for string symbols.
- `symbolIcon.structForeground`: The foreground color for struct symbols.
- `symbolIcon.textForeground`: The foreground color for text symbols.
- `symbolIcon.typeParameterForeground`: The foreground color for type parameter symbols.
- `symbolIcon.unitForeground`: The foreground color for unit symbols.
- `symbolIcon.variableForeground`: The foreground color for variable symbols.

Answer
----------------------------------------
Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

Evaluation_score
----------------------------------------
1

Ground truth answer
You can use the command `kb(workbench.action.gotoSymbol)` to go to a symbol in Visual Studio Code.

====================================================================================================

Question
----------------------------------------
EKS patching?

Documents
----------------------------------------
Amazon EKS doesn't automatically update Amazon EBS CSI for your cluster when new versions are released or after you [update your cluster](update-cluster.md) to a new Kubernetes minor version\. To update Amazon EBS CSI on an existing cluster, you must initiate the update and then Amazon EKS updates the add\-on for you\.  
------

The Amazon EKS managed worker node upgrade strategy has four different phases described in the following sections\.

**How many Kubernetes versions are available in standard support?**
In line with the Kubernetes community support for Kubernetes versions, Amazon EKS is committed to offering standard support for at least four production\-ready versions of Kubernetes at any given time\. We will announce the end of standard support date of a given Kubernetes minor version at least 60 days in advance\. Because of the Amazon EKS qualification and release process for new Kubernetes versions, the end of standard support date of a Kubernetes version on Amazon EKS will be on or after the date that the Kubernetes project stops supporting the version upstream\.  
**How long does a Kubernetes receive standard support by Amazon EKS?**
A Kubernetes version received standard support for 14 months after first being available on Amazon EKS\. This is true even if upstream Kubernetes no longer support a version that's available on Amazon EKS\. We backport security patches that are applicable to the Kubernetes versions that are supported on Amazon EKS\.  
**Am I notified when standard support is ending for a Kubernetes version on Amazon EKS?**
Yes\. If any clusters in your account are running the version nearing the end of support, Amazon EKS sends out a notice through the AWS Health Dashboard approximately 12 months after the Kubernetes version was released on Amazon EKS\. The notice includes the end of support date\. This is at least 60 days from the date of the notice\.  
**Which Kubernetes features are supported by Amazon EKS?**
Amazon EKS supports all generally available \(GA\) features of the Kubernetes API\. Starting with Kubernetes version `1.24`, new beta APIs aren't enabled in clusters by default\. However, previously existing beta APIs and new versions of existing beta APIs continue to be enabled by default\. Alpha features aren't supported\.  
**Are Amazon EKS managed node groups automatically updated along with the cluster control plane version?**
No\. A managed node group creates Amazon EC2 instances in your account\. These instances aren't automatically upgraded when you or Amazon EKS update your control plane\. For more information, see [Updating a managed node group](update-managed-node-group.md)\. We recommend maintaining the same Kubernetes version on your control plane and nodes\.  
**Are self\-managed node groups automatically updated along with the cluster control plane version?**
No\. A self\-managed node group includes Amazon EC2 instances in your account\. These instances aren't automatically upgraded when you or Amazon EKS update the control plane version on your behalf\. A self\-managed node group doesn't have any indication in the console that it needs updating\. You can view the `kubelet` version installed on a node by selecting the node in the **Nodes** list on the **Overview** tab of your cluster to determine which nodes need updating\. You must manually update the nodes\. For more information, see [Self\-managed node updates](update-workers.md)\.
The Kubernetes project tests compatibility between the control plane and nodes for up to three minor versions\. For example, `1.26` nodes continue to operate when orchestrated by a `1.29` control plane\. However, running a cluster with nodes that are persistently three minor versions behind the control plane isn't recommended\. For more information, see [Kubernetes version and version skew support policy](https://kubernetes.io/docs/setup/version-skew-policy/) in the Kubernetes documentation\. We recommend maintaining the same Kubernetes version on your control plane and nodes\.  
**Are Pods running on Fargate automatically upgraded with an automatic cluster control plane version upgrade?**
No\. We strongly recommend running Fargate Pods as part of a replication controller, such as a Kubernetes deployment\. Then do a rolling restart of all Fargate Pods\. The new version of the Fargate Pod is deployed with a `kubelet` version that's the same version as your updated cluster control plane version\. For more information, see [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment) in the Kubernetes documentation\.
If you update the control plane, you must still update the Fargate nodes yourself\. To update Fargate nodes, delete the Fargate Pod represented by the node and redeploy the Pod\. The new Pod is deployed with a `kubelet` version that's the same version as your cluster\.

Amazon EKS security best practices are maintained on Github: [https://aws\.github\.io/aws\-eks\-best\-practices/security/docs/](https://aws.github.io/aws-eks-best-practices/security/docs/)

Answer
----------------------------------------
A Kubernetes version receives standard support by Amazon EKS for 14 months after being available on Amazon EKS.
Confidence: 85%

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes, but lacks specificity.

The answer does mention EKS patching, but it describes the support duration for a Kubernetes version on EKS rather than providing information on how to patch or the process of patching.

Evaluation_score
----------------------------------------
3

Ground truth answer
Amazon EKS automatically patches the control plane to ensure security and stability.

====================================================================================================

Question
----------------------------------------
EKS auto scaling?

Documents
----------------------------------------
Related auto scaling projects for Amazon EKS and Kubernetes clusters\.

This topic describes how you can enable Auto Scaling group metrics collection using [AWS Lambda](https://aws.amazon.com/lambda) and [AWS CloudTrail](https://aws.amazon.com/cloudtrail)\. Amazon EKS doesn't automatically enable group metrics collection for Auto Scaling groups created for managed nodes\.  
You can use [Auto Scaling group metrics](https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-cloudwatch-monitoring.html) to track changes in an Auto Scaling group and to set alarms on threshold values\. Auto Scaling group metrics are available in the Auto Scaling console or the [Amazon CloudWatch](https://aws.amazon.com/cloudwatch) console\. Once enabled, the Auto Scaling group sends sampled data to Amazon CloudWatch every minute\. There is no charge for enabling these metrics\.  
By enabling Auto Scaling group metrics collection, you'll be able to monitor the scaling of managed node groups\. Auto Scaling group metrics report the minimum, maximum, and desired size of an Auto Scaling group\. You can create an alarm if the number of nodes in a node group falls below the minimum size, which would indicate an unhealthy node group\. Tracking node group size is also useful in adjusting the maximum count so that your data plane doesn't run out of capacity\.  
When you create a managed node group, AWS CloudTrail sends a `CreateNodegroup` event to [Amazon EventBridge](https://aws.amazon.com/eventbridge)\. By creating an Amazon EventBridge rule that matches the `CreateNodegroup` event, you trigger a Lambda function to enable group metrics collection for the Auto Scaling group associated with the managed node group\.  
![\[Diagram showing the managed node group, CloudTrail, and EventBridge component\]](http://docs.aws.amazon.com/eks/latest/userguide/images/enable-asg-metrics.png)  
**To enable Auto Scaling group metrics collection**  
1. Create an IAM role for Lambda\.  
```
LAMBDA_ROLE=$(aws iam create-role \
--role-name lambda-asg-enable-metrics \
--assume-role-policy-document '{"Version": "2012-10-17","Statement": [{ "Effect": "Allow", "Principal": {"Service": "lambda.amazonaws.com"}, "Action": "sts:AssumeRole"}]}' \
--output text \
--query 'Role.Arn')
echo $LAMBDA_ROLE
```  
1. Create a policy that allows describing Amazon EKS node groups and enabling Auto Scaling group metrics collection\.  
```
cat > /tmp/lambda-policy.json <<EOF
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"eks:DescribeNodegroup",
"autoscaling:EnableMetricsCollection"
],
"Resource": [
"*"
]
}
]
}
EOF
LAMBDA_POLICY_ARN=$(aws iam create-policy \
--policy-name lambda-asg-enable-metrics-policy \
--policy-document file:///tmp/lambda-policy.json \
--output text \
--query 'Policy.Arn')
echo $LAMBDA_POLICY_ARN
```  
1. Attach the policy to the IAM role for Lambda\.  
```
aws iam attach-role-policy \
--policy-arn $LAMBDA_POLICY_ARN \
--role-name lambda-asg-enable-metrics
```  
1. Add the `AWSLambdaBasicExecutionRole` managed policy, which has the permissions that the function needs to write logs to CloudWatch Logs\.  
```
aws iam attach-role-policy \
--role-name lambda-asg-enable-metrics \
--policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
```  
1. Create the Lambda code\.  
```
cat > /tmp/lambda-handler.py <<EOF
import json
import boto3
import time
import logging

eks = boto3.client('eks')
autoscaling = boto3.client('autoscaling')

logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
ASG_METRICS_COLLLECTION_TAG_NAME = "ASG_METRICS_COLLLECTION_ENABLED"
initial_retry_delay = 10
attempts = 0

#print(event)

if not event["detail"]["eventName"] == "CreateNodegroup":
print("invalid event.")
return -1

clusterName = event["detail"]["requestParameters"]["name"]
nodegroupName = event["detail"]["requestParameters"]["nodegroupName"]
try:
metricsCollectionEnabled = event["detail"]["requestParameters"]["tags"][ASG_METRICS_COLLLECTION_TAG_NAME]
except KeyError:
print(ASG_METRICS_COLLLECTION_TAG_NAME, "tag not found.")
return

# Check if metrics collection is enabled in tags
if metricsCollectionEnabled.lower() != "true":
print("Metrics collection is not enabled in nodegroup tags.")
return

# Get the name of the associated autoscaling group
print("Getting the autoscaling group name for nodegroup=", nodegroupName, ", cluster=", clusterName )
for i in range(0,10):
try:
autoScalingGroup = eks.describe_nodegroup(clusterName=clusterName,nodegroupName=nodegroupName)["nodegroup"]["resources"]["autoScalingGroups"][0]["name"]
except:
attempts += 1
print("Failed to obtain the associated autoscaling group for nodegroup", nodegroupName, "Retrying in", initial_retry_delay*attempts, "seconds.")
time.sleep(initial_retry_delay*attempts)
else:
break

print("Enabling metrics collection on autoscaling group ", autoScalingGroup)

# Enable metrics collection in the autoscaling group
try:
enableMetricsCollection = autoscaling.enable_metrics_collection(AutoScalingGroupName=autoScalingGroup,Granularity="1Minute")
except:
print("Unable to enable metrics collection on nodegroup=",nodegroup)
print("Enabled metrics collection on nodegroup", nodegroupName)
EOF
```  
1. Create a deployment package\.  
```
cd /tmp
zip function.zip lambda-handler.py
```  
1. Create a Lambda function\.  
```
LAMBDA_ARN=$(aws lambda create-function --function-name asg-enable-metrics-collection \
--zip-file fileb://function.zip --handler lambda-handler.lambda_handler \
--runtime python3.9 \
--timeout 600 \
--role $LAMBDA_ROLE \
--output text \
--query 'FunctionArn')
echo $LAMBDA_ARN
```  
1. Create an EventBridge rule\.  
```
RULE_ARN=$(aws events put-rule --name CreateNodegroupRuleToLambda \
--event-pattern "{\"source\":[\"aws.eks\"],\"detail-type\":[\"AWS API Call via CloudTrail\"],\"detail\":{\"eventName\":[\"CreateNodegroup\"],\"eventSource\":[\"eks.amazonaws.com\"]}}" \
--output text \
--query 'RuleArn')
echo $RULE_ARN
```  
1. Add the Lambda function as a target\.  
```
aws events put-targets --rule CreateNodegroupRuleToLambda \
--targets "Id"="1","Arn"="$LAMBDA_ARN"
```  
1. Add a policy that allows EventBridge to invoke the Lambda function\.  
```
aws lambda add-permission \
--function-name asg-enable-metrics-collection \
--statement-id CreateNodegroupRuleToLambda \
--action 'lambda:InvokeFunction' \
--principal events.amazonaws.com \
--source-arn $RULE_ARN
```  
The Lambda function enables Auto Scaling group metrics collection for any managed node groups that you tag with `ASG_METRICS_COLLLECTION_ENABLED` set to `TRUE`\. To confirm that **Auto Scaling group metrics collection** is enabled, navigate to the associated Auto Scaling group in the Amazon EC2 console\. In the **Monitoring** tab, you should see that the **Enable** check box is activated\.

The Kubernetes Metrics Server is an aggregator of resource usage data in your cluster, and it isn't deployed by default in Amazon EKS clusters\. For more information, see [Kubernetes Metrics Server](https://github.com/kubernetes-sigs/metrics-server) on GitHub\. The Metrics Server is commonly used by other Kubernetes add ons, such as the [Horizontal Pod Autoscaler](horizontal-pod-autoscaler.md) or the [Kubernetes Dashboard](eks-managing.md)\. For more information, see [Resource metrics pipeline](https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/) in the Kubernetes documentation\. This topic explains how to deploy the Kubernetes Metrics Server on your Amazon EKS cluster\.  
**Important**
The metrics are meant for point\-in\-time analysis and aren't an accurate source for historical analysis\. They can't be used as a monitoring solution or for other non\-auto scaling purposes\. For information about monitoring tools, see [Observability in Amazon EKS](eks-observe.md)\.  
**Deploy the Metrics Server**  
1. Deploy the Metrics Server with the following command:  
```
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```  
If you are using Fargate, you will need to change this file\. In the default configuration, the metrics server uses port 10250\. This port is reserved on Fargate\. Replace references to port 10250 in components\.yaml with another port, such as 10251\.  
1. Verify that the `metrics-server` deployment is running the desired number of Pods with the following command\.  
```
kubectl get deployment metrics-server -n kube-system
```  
An example output is as follows\.  
```
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
metrics-server   1/1     1            1           6m
```

The scale down phase decrements the Auto Scaling group maximum size and desired size by one to return to values before the update started\.  
If the Upgrade workflow determines that the Cluster Autoscaler is scaling up the node group during the scale down phase of the workflow, it exits immediately without bringing the node group back to its original size\.

Answer
----------------------------------------
To enable Auto Scaling group metrics collection for Amazon EKS and Kubernetes clusters, you can follow these steps:

1. Create an IAM role for Lambda with necessary permissions
2. Create a policy that allows describing Amazon EKS node groups and enabling Auto Scaling group metrics collection
3. Attach the policy to the IAM role for Lambda
4. Add the `AWSLambdaBasicExecutionRole` managed policy to the IAM role for Lambda
5. Create a Lambda function with code to enable Auto Scaling group metrics collection
6. Create a deployment package for the Lambda function
7. Create a Lambda function with the deployment package
8. Create an EventBridge rule to trigger the Lambda function when a `CreateNodegroup` event is sent to CloudTrail
9. Add the Lambda function as a target for the EventBridge rule
10. Add a policy that allows EventBridge to invoke the Lambda function

This will enable Auto Scaling group metrics collection, which can be confirmed in the associated Auto Scaling group in the Amazon EC2 console. The metrics can be used for point-in-time analysis but are not meant for historical analysis or as a monitoring solution.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes, but could be more concise.

The answer provides a detailed, step-by-step guide on how to enable EKS auto scaling, which is correct. However, the response is quite lengthy and could be more concise. A more direct and shorter answer would improve its effectiveness.

Evaluation_score
----------------------------------------
3

Ground truth answer
Amazon EKS actively monitors the load on control plane instances and automatically scales them to ensure high performance. It also automatically detects and replaces unhealthy control plane instances, restarting them across the Availability Zones within the AWS Region as needed.

====================================================================================================

Question
----------------------------------------
AWS regions and EKS?

Documents
----------------------------------------
The following table compares the differences between the two options\.  
| Feature | Extended cluster | Local cluster |
| --- | --- | --- |
|  Kubernetes control plane location  | AWS Region | Outpost |
|  Kubernetes control plane account  | AWS account | Your account |
| Regional availability | See [Service endpoints](https://docs.aws.amazon.com/general/latest/gr/eks.html#eks_region) | US East \(Ohio\), US East \(N\. Virginia\), US West \(N\. California\), US West \(Oregon\), Asia Pacific \(Seoul\), Asia Pacific \(Singapore\), Asia Pacific \(Sydney\), Asia Pacific \(Tokyo\), Canada \(Central\), Europe \(Frankfurt\), Europe \(Ireland\), Europe \(London\), Middle East \(Bahrain\), and South America \(São Paulo\) |
| Kubernetes minor versions |  [Supported Amazon EKS versions](kubernetes-versions.md)\. | [Supported Amazon EKS versions](kubernetes-versions.md)\. |
| Platform versions | See [Amazon EKS platform versions](platform-versions.md) | See [Amazon EKS local cluster platform versions](eks-outposts-platform-versions.md) |
| Outpost form factors | Outpost racks | Outpost racks |
| User interfaces | AWS Management Console, AWS CLI, Amazon EKS API, `eksctl`, AWS CloudFormation, and Terraform | AWS Management Console, AWS CLI, Amazon EKS API, eksctl, AWS CloudFormation, and Terraform |
| Managed policies | [AmazonEKSClusterPolicy](security-iam-awsmanpol.md#security-iam-awsmanpol-AmazonEKSClusterPolicy) and [AmazonEKSServiceRolePolicy](security-iam-awsmanpol.md#security-iam-awsmanpol-AmazonEKSServiceRolePolicy) | [AmazonEKSLocalOutpostClusterPolicy](security-iam-awsmanpol.md#security-iam-awsmanpol-AmazonEKSLocalOutpostClusterPolicy) and [AmazonEKSLocalOutpostServiceRolePolicy](security-iam-awsmanpol.md#security-iam-awsmanpol-AmazonEKSLocalOutpostServiceRolePolicy) |
| Cluster VPC and subnets | See [Amazon EKS VPC and subnet requirements and considerations](network_reqs.md) | See [Amazon EKS local cluster VPC and subnet requirements and considerations](eks-outposts-vpc-subnet-requirements.md) |
| Cluster endpoint access | Public or private or both | Private only |
| Kubernetes API server authentication | AWS Identity and Access Management \(IAM\) and OIDC | IAM and `x.509` certificates |
| Node types | Self\-managed only | Self\-managed only |
| Node compute types | Amazon EC2 on\-demand | Amazon EC2 on\-demand |
| Node storage types | Amazon EBS `gp2` and local NVMe SSD | Amazon EBS `gp2` and local NVMe SSD |
| Amazon EKS optimized AMIs | Amazon Linux, Windows, and Bottlerocket | Amazon Linux only |
| IP versions | `IPv4` only | `IPv4` only |
| Add\-ons | Amazon EKS add\-ons or self\-managed add\-ons | Self\-managed add\-ons only |
| Default Container Network Interface | Amazon VPC CNI plugin for Kubernetes | Amazon VPC CNI plugin for Kubernetes |
| Kubernetes control plane logs | Amazon CloudWatch Logs |  Amazon CloudWatch Logs  |
| Load balancing | Use the [AWS Load Balancer Controller](aws-load-balancer-controller.md) to provision Application Load Balancers only \(no Network Load Balancers\) | Use the [AWS Load Balancer Controller](aws-load-balancer-controller.md) to provision Application Load Balancers only \(no Network Load Balancers\) |
| Secrets envelope encryption | See [Enabling secret encryption on an existing cluster](enable-kms.md) | Not supported |
| IAM roles for service accounts | See [IAM roles for service accounts](iam-roles-for-service-accounts.md) | Not supported |
| Troubleshooting | See [Amazon EKS troubleshooting](troubleshooting.md) | See [Troubleshooting local clusters for Amazon EKS on AWS Outposts](eks-outposts-troubleshooting.md) |  
**Topics**

An AWS Local Zone is an extension of an AWS Region in geographic proximity to your users\. Local Zones have their own connections to the internet and support AWS Direct Connect\. Resources created in a Local Zone can serve local users with low\-latency communications\. For more information, see [Local Zones](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-local-zones)\.  
Amazon EKS supports certain resources in Local Zones\. This includes [self\-managed Amazon EC2 nodes](worker.md), Amazon EBS volumes, and Application Load Balancers \(ALBs\)\. We recommend that you consider the following when using Local Zones as part of your Amazon EKS cluster\.  
**Nodes**
You can't create managed node groups or Fargate nodes in Local Zones with Amazon EKS\. However, you can create self\-managed Amazon EC2 nodes in Local Zones using the Amazon EC2 API, AWS CloudFormation, or `eksctl`\. For more information, see [Self\-managed nodes](worker.md)\.  
**Network architecture**
+ The Amazon EKS managed Kubernetes control plane always runs in the AWS Region\. The Amazon EKS managed Kubernetes control plane can't run in the Local Zone\. Because Local Zones appear as a subnet within your VPC, Kubernetes sees your Local Zone resources as part of that subnet\.
+ The Amazon EKS Kubernetes cluster communicates with the Amazon EC2 instances you run in the AWS Region or Local Zone using Amazon EKS managed [elastic network interfaces](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html)\. To learn more about Amazon EKS networking architecture, see [Amazon EKS networking](eks-networking.md)\.
+ Unlike regional subnets, Amazon EKS can't place network interfaces into your Local Zone subnets\. This means that you must not specify Local Zone subnets when you create your cluster\.

Amazon EKS is integrated with AWS CloudFormation, a service that helps you model and set up your AWS resources so that you can spend less time creating and managing your resources and infrastructure\. You create a template that describes all the AWS resources that you want, for example an Amazon EKS cluster, and AWS CloudFormation takes care of provisioning and configuring those resources for you\.  
When you use AWS CloudFormation, you can reuse your template to set up your Amazon EKS resources consistently and repeatedly\. Just describe your resources once, and then provision the same resources over and over in multiple AWS accounts and Regions\.

No\. This version is only compatible with Amazon EKS clusters\.

Answer
----------------------------------------
The regional availability of Amazon EKS is in several AWS regions, as listed in the table. It is not available in AWS Local Zones, which are extensions of AWS Regions but do not support all EKS resources. The Kubernetes control plane of EKS is located in an AWS region and not in a Local Zone. AWS CloudFormation can be used to provision and configure EKS resources. The version provided is only compatible with Amazon EKS clusters.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Partially.

The answer does provide some information about AWS regions and EKS, but it does not directly answer the question. It explains the regional availability of EKS and the compatibility of the Kubernetes control plane with AWS regions, but it does not specify the number or names of the regions where EKS is available.

Evaluation_score
----------------------------------------
3

Ground truth answer
Amazon EKS leverages the architecture of AWS Regions to maintain high availability and is able to offer an SLA for API server endpoint availability.

====================================================================================================

Question
----------------------------------------
EKS etcd instances?

Documents
----------------------------------------
An Amazon EKS cluster consists of two primary components:
+ The Amazon EKS control plane
+ Amazon EKS nodes that are registered with the control plane  
The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as `etcd` and the Kubernetes API server\. The control plane runs in an account managed by AWS, and the Kubernetes API is exposed via the Amazon EKS endpoint associated with your cluster\. Each Amazon EKS cluster control plane is single\-tenant and unique, and runs on its own set of Amazon EC2 instances\.  
All of the data stored by the `etcd` nodes and associated Amazon EBS volumes is encrypted using AWS KMS\. The cluster control plane is provisioned across multiple Availability Zones and fronted by an Elastic Load Balancing Network Load Balancer\. Amazon EKS also provisions elastic network interfaces in your VPC subnets to provide connectivity from the control plane instances to the nodes \(for example, to support  `kubectl exec` `logs` `proxy` data flows\)\.  
**Important**
In the Amazon EKS environment, `etcd` storage is limited to 8 GiB as per [upstream](https://etcd.io/docs/v3.5/dev-guide/limit/#storage-size-limit) guidance\. You can monitor a metric for the current database size by running the following command\. If your cluster has a Kubernetes version below `1.28`, replace `apiserver_storage_size_bytes` with the following:
Kubernetes version `1.27` and `1.26` – **`apiserver_storage_db_total_size_in_bytes`**
Kubernetes version `1.25` and below – **`etcd_db_total_size_in_bytes`**  
```
kubectl get --raw=/metrics | grep "apiserver_storage_size_bytes"
```  
Amazon EKS nodes run in your AWS account and connect to your cluster's control plane via the API server endpoint and a certificate file that is created for your cluster\.  
**Note**
You can find out how the different components of Amazon EKS work in [Amazon EKS networking](eks-networking.md)\.
For connected clusters, see [Amazon EKS Connector](eks-connector.md)\.  
**Topics**
+ [Creating an Amazon EKS cluster](create-cluster.md)
+ [Cluster insights](cluster-insights.md)
+ [Updating an Amazon EKS cluster Kubernetes version](update-cluster.md)
+ [Deleting an Amazon EKS cluster](delete-cluster.md)
+ [Amazon EKS cluster endpoint access control](cluster-endpoint.md)
+ [Enabling secret encryption on an existing cluster](enable-kms.md)
+ [Enabling Windows support for your Amazon EKS cluster](windows-support.md)
+ [Private cluster requirements](private-clusters.md)
+ [Amazon EKS Kubernetes versions](kubernetes-versions.md)
+ [Amazon EKS platform versions](platform-versions.md)
+ [Autoscaling](autoscaling.md)

This topic covers some common errors that you may see while using Amazon EKS with IAM and how to work around them\.

Several networking add\-ons are available for your Amazon EKS cluster\.

For more information about using Amazon EKS optimized Amazon Linux AMIs, see the following sections:
+ To use Amazon Linux with managed node groups, see [Managed node groups](managed-node-groups.md)\.
+ To launch self\-managed Amazon Linux nodes, see [Retrieving Amazon EKS optimized Amazon Linux AMI IDs](retrieve-ami-id.md)\.
+ For version information, see [Amazon EKS optimized Amazon Linux AMI versions](eks-linux-ami-versions.md)\.
+ To retrieve the latest IDs of the Amazon EKS optimized Amazon Linux AMIs, see [Retrieving Amazon EKS optimized Amazon Linux AMI IDs](retrieve-ami-id.md)\.
+ For open\-source scripts that are used to build the Amazon EKS optimized AMI, see [Amazon EKS optimized Amazon Linux AMI build script](eks-ami-build-scripts.md)\.

Answer
----------------------------------------
The `etcd` instances are part of the Amazon EKS control plane, which consists of control plane nodes that run Kubernetes software such as `etcd` and the Kubernetes API server. These instances are managed by AWS and run on their own set of Amazon EC2 instances across multiple Availability Zones.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not directly provide the number or specific details of the EKS etcd instances. Instead, it describes the general role and management of etcd instances within an Amazon EKS control plane.

Evaluation_score
----------------------------------------
2

Ground truth answer
The control plane of Amazon EKS consists of at least two API server instances and three `etcd` instances that run across three Availability Zones within an AWS Region.

====================================================================================================

Question
----------------------------------------
Attach managed policy to IAM role?

Documents
----------------------------------------
You can't attach this policy to your IAM entities\. When you create a cluster using an IAM principal that has the `iam:CreateServiceLinkedRole` permission, Amazon EKS automatically creates the [`AWSServiceRoleforAmazonEKSLocalOutpost`](using-service-linked-roles-eks-outpost.md) service\-linked role for you and attaches this policy to it\. This policy allows the service\-linked role to call AWS services on your behalf for local clusters\.  
The `AmazonEKSLocalOutpostServiceRolePolicy` includes the following permissions:
+ **`ec2`** – Allows Amazon EKS to work with security, network, and other resources to successfully launch and manage control plane instances in your account\.
+ **`ssm`** – Allows Amazon EC2 Systems Manager connection to the control plane instances, which is used by Amazon EKS to communicate and manage the local cluster in your account\.
+ **`iam`** – Allows Amazon EKS to manage the instance profile associated with the control plane instances\.
+ **`secretsmanager`** – Allows Amazon EKS to put bootstrap data for the control plane instances into AWS Secrets Manager so it can be securely referenced during instance bootstrapping\.
+ **`outposts`** – Allows Amazon EKS to get Outpost information from your account to successfully launch a local cluster in an Outpost\.  
To view the latest version of the JSON policy document, see [AmazonEKSLocalOutpostServiceRolePolicy](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEKSLocalOutpostServiceRolePolicy.html#AmazonEKSLocalOutpostServiceRolePolicy-json) in the AWS Managed Policy Reference Guide\.

This example shows how you might create a policy that allows IAM users to view the inline and managed policies that are attached to their user identity\. This policy includes permissions to complete this action on the console or programmatically using the AWS CLI or AWS API\.  
```
{
"Version": "2012-10-17",
"Statement": [
{
"Sid": "ViewOwnUserInfo",
"Effect": "Allow",
"Action": [
"iam:GetUserPolicy",
"iam:ListGroupsForUser",
"iam:ListAttachedUserPolicies",
"iam:ListUserPolicies",
"iam:GetUser"
],
"Resource": ["arn:aws:iam::*:user/${aws:username}"]
},
{
"Sid": "NavigateInConsole",
"Effect": "Allow",
"Action": [
"iam:GetGroupPolicy",
"iam:GetPolicyVersion",
"iam:GetPolicy",
"iam:ListAttachedGroupPolicies",
"iam:ListGroupPolicies",
"iam:ListPolicyVersions",
"iam:ListPolicies",
"iam:ListUsers"
],
"Resource": "*"
}
]
}
```

Identity\-based policies are JSON permissions policy documents that you can attach to an identity, such as an IAM user, group of users, or role\. These policies control what actions users and roles can perform, on which resources, and under what conditions\. To learn how to create an identity\-based policy, see [Creating IAM policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html) in the *IAM User Guide*\.  
Identity\-based policies can be further categorized as *inline policies* or *managed policies*\. Inline policies are embedded directly into a single user, group, or role\. Managed policies are standalone policies that you can attach to multiple users, groups, and roles in your AWS account\. Managed policies include AWS managed policies and customer managed policies\. To learn how to choose between a managed policy or an inline policy, see [Choosing between managed policies and inline policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#choosing-managed-or-inline) in the *IAM User Guide*\.

1. Run the following command to create the `node-role-trust-relationship.json` file\.  
```
cat >node-role-trust-relationship.json <<EOF
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"Service": "ec2.amazonaws.com"
},
"Action": "sts:AssumeRole"
}
]
}
EOF
```  
1. Create the IAM role\.  
```
aws iam create-role \
--role-name AmazonEKSNodeRole \
--assume-role-policy-document file://"node-role-trust-relationship.json"
```  
1. Attach two required IAM managed policies to the IAM role\.  
```
aws iam attach-role-policy \
--policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy \
--role-name AmazonEKSNodeRole
aws iam attach-role-policy \
--policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \
--role-name AmazonEKSNodeRole
```  
1. Attach one of the following IAM policies to the IAM role depending on which IP family you created your cluster with\. The policy must be attached to this role or to a role associated to the Kubernetes `aws-node` service account that's used for the Amazon VPC CNI plugin for Kubernetes\. We recommend assigning the policy to the role associated to the Kubernetes service account\. To assign the policy to the role associated to the Kubernetes service account, see [Configuring the Amazon VPC CNI plugin for Kubernetes to use IAM roles for service accounts \(IRSA\)](cni-iam-role.md)\.
+ IPv4  
```
aws iam attach-role-policy \
--policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy \
--role-name AmazonEKSNodeRole
```
+ IPv6  
1. Copy the following text and save it to a file named `vpc-cni-ipv6-policy.json`\.  
```
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"ec2:AssignIpv6Addresses",
"ec2:DescribeInstances",
"ec2:DescribeTags",
"ec2:DescribeNetworkInterfaces",
"ec2:DescribeInstanceTypes"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": [
"ec2:CreateTags"
],
"Resource": [
"arn:aws:ec2:*:*:network-interface/*"
]
}
]
}
```  
1. Create the IAM policy\.  
```
aws iam create-policy --policy-name AmazonEKS_CNI_IPv6_Policy --policy-document file://vpc-cni-ipv6-policy.json
```  
1. Attach the IAM policy to the IAM role\. Replace `111122223333` with your account ID\.  
```
aws iam attach-role-policy \
--policy-arn arn:aws:iam::111122223333:policy/AmazonEKS_CNI_IPv6_Policy \
--role-name AmazonEKSNodeRole
```  
------

Answer
----------------------------------------
IAM roles cannot have managed policies attached to them directly. Instead, you need to create a policy using the JSON document provided or another policy of your choice, and then attach that policy to the role. Here's how you can attach a policy to the IAM role named "AmazonEKSNodeRole":

```
aws iam create-policy \
--policy-name MyEKSNodePolicy \
--policy-document file://mypolicy.json

aws iam attach-role-policy \
--role-name AmazonEKSNodeRole \
--policy-arn arn:aws:iam::111122223333:policy/MyEKSNodePolicy
```

Replace "MyEKSNodePolicy" with the desired policy name, and "mypolicy.json" with the path to your policy document. Also replace "111122223333" with your AWS account ID.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

Although the response includes an example of how to attach a managed policy to an IAM role, which is correct, the question asks for a way to do it, not an example. Therefore, the response is still valid and useful, so I give it the maximum score.

Evaluation_score
----------------------------------------
5

Ground truth answer
Attach the `AmazonEFSCSIDriverPolicy` policy to the `AmazonEKS_EFS_CSI_DriverRole` role using the AWS CLI and the `attach-role-policy` command.

====================================================================================================

Question
----------------------------------------
Prerequisites for creating IAM role?

Documents
----------------------------------------
**To create the IAM role**  
1. Determine the IP family of your cluster\.  
```
aws eks describe-cluster --name my-cluster | grep ipFamily
```  
An example output is as follows\.  
```
"ipFamily": "ipv4"
```  
The output may return `ipv6` instead\.  
1. Create the IAM role\. You can use `eksctl` or `kubectl` and the AWS CLI to create your IAM role\.  
------

An [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) is an entity within your AWS account that has specific permissions\.

Before you use IAM to manage access to Amazon EKS, you should understand what IAM features are available to use with Amazon EKS\. To get a high\-level view of how Amazon EKS and other AWS services work with IAM, see [AWS services that work with IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html) in the *IAM User Guide*\.  
**Topics**
+ [Amazon EKS identity\-based policies](#security_iam_service-with-iam-id-based-policies)
+ [Amazon EKS resource\-based policies](#security_iam_service-with-iam-resource-based-policies)
+ [Authorization based on Amazon EKS tags](#security_iam_service-with-iam-tags)
+ [Amazon EKS IAM roles](#security_iam_service-with-iam-roles)

+ **Publisher** – Calyptia
+ **Name** – `calyptia_fluent-bit`
+ **Namespace** – `calytia-fluentbit`
+ **Service account name** – `clyptia-fluentbit`
+ **AWS managed IAM policy** – [AWSMarketplaceMeteringRegisterUsage](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSMarketplaceMeteringRegisterUsage.html)\.
+ **Command to create required IAM role** – The following command requires that you have an IAM OpenID Connect \(OIDC\) provider for your cluster\. To determine whether you have one, or to create one, see [Create an IAM OIDC provider for your cluster](enable-iam-roles-for-service-accounts.md)\. Replace `my-cluster` with the name of your cluster and `my-calyptia-role` with the name for your role\. This command requires that you have [https://eksctl.io](https://eksctl.io) installed on your device\. If you need to use a different tool to create the role and annotate the Kubernetes service account, see [Configure a Kubernetes service account to assume an IAM role](associate-service-account-role.md)\.  
```
eksctl create iamserviceaccount --name service-account-name  --namespace calyptia-fluentbit --cluster my-cluster --role-name my-calyptia-role \
--role-only --attach-policy-arn arn:aws:iam::aws:policy/AWSMarketplaceMeteringRegisterUsage --approve
```
+ **Setup and usage instructions** – See [Calyptia for Fluent Bit](https://docs.calyptia.com/calyptia-for-fluent-bit/installation/eks-add-on) in the Calyptia documentation\.

Answer
----------------------------------------
The prerequisites for creating an IAM role in the context provided are:

1. Determine the IP family of your cluster using the `aws eks describe-cluster` command.
2. Have an IAM OpenID Connect (OIDC) provider for your cluster. You can check this by following the instructions in "Create an IAM OIDC provider for your cluster."
3. Have `eksctl` installed on your device or be able to use a different tool to create the role and annotate the Kubernetes service account.

Reason: The context provides steps to create an IAM role using `eksctl`, but it assumes that the user already has an IAM OIDC provider and `eksctl` installed. These are not mentioned as explicit prerequisites in the context.

Evaluation
----------------------------------------
Score (1-5): 4
Answered: Yes.

The answer provides detailed prerequisites for creating an IAM role, even though it refers to the context in the response. It directly answers the question, albeit with some irrelevant information in the reasoning part.

Evaluation_score
----------------------------------------
4

Ground truth answer
Before creating the IAM role, you need to view your cluster's OIDC provider URL using the AWS CLI and the `describe-cluster` command.

====================================================================================================

Question
----------------------------------------
creating efs csi driver IAM role?

Documents
----------------------------------------
**To create your Amazon EFS CSI driver IAM role with the AWS Management Console**  
1. Open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.  
1. In the left navigation pane, choose **Roles**\.  
1. On the **Roles** page, choose **Create role**\.  
1. On the **Select trusted entity** page, do the following:  
1. In the **Trusted entity type** section, choose **Web identity**\.  
1. For **Identity provider**, choose the **OpenID Connect provider URL** for your cluster \(as shown under **Overview** in Amazon EKS\)\.  
1. For **Audience**, choose `sts.amazonaws.com`\.  
1. Choose **Next**\.  
1. On the **Add permissions** page, do the following:  
1. In the **Filter policies** box, enter `AmazonEFSCSIDriverPolicy`\.  
1. Select the check box to the left of the `AmazonEFSCSIDriverPolicy` returned in the search\.  
1. Choose **Next**\.  
1. On the **Name, review, and create** page, do the following:  
1. For **Role name**, enter a unique name for your role, such as ***AmazonEKS\_EFS\_CSI\_DriverRole***\.  
1. Under **Add tags \(Optional\)**, add metadata to the role by attaching tags as key\-value pairs\. For more information about using tags in IAM, see [Tagging IAM resources](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_tags.html) in the *IAM User Guide*\.  
1. Choose **Create role**\.  
1. After the role is created, choose the role in the console to open it for editing\.  
1. Choose the **Trust relationships** tab, and then choose **Edit trust policy**\.  
1. Find the line that looks similar to the following line:  
```
"oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE:aud": "sts.amazonaws.com"
```  
Add the following line above the previous line\. Replace `region-code` with the AWS Region that your cluster is in\. Replace `EXAMPLED539D4633E53DE1B71EXAMPLE` with your cluster's OIDC provider ID\.  
```
"oidc.eks.region-code.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE:sub": "system:serviceaccount:kube-system:efs-csi-*",
```  
1. Modify the `Condition` operator from `"StringEquals"` to `"StringLike"`\.  
1. Choose **Update policy** to finish\.  
------

You can deploy a variety of sample apps and modify them as needed\. For more information, see [Examples](https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/docs/README.md#examples) on GitHub\.

Yes, as long as the [Amazon EBS CSI driver](ebs-csi.md) is installed\.

Yes, as long as the [Amazon EBS CSI driver](ebs-csi.md) is installed\. For newly created version `1.23` and later clusters, we recommend installing the Amazon EBS CSI driver as part of your cluster creation process\. We also recommend only using `StorageClasses` based on the `ebs.csi.aws.com` provisioner\.  
If you've updated your cluster control plane to version `1.23` and haven't yet updated your nodes to `1.23`, then the `CSIMigration` and `CSIMigrationAWS` `kubelet` flags aren't enabled\. In this case, the in\-tree driver is used to mount `kubernetes.io/aws-ebs` based volumes\. The Amazon EBS CSI driver must still be installed however, to ensure that Pods using `kubernetes.io/aws-ebs` based volumes can be scheduled\. The driver is also required for other volume operations to succeed\.

Answer
----------------------------------------
To create your Amazon EFS CSI driver IAM role with the AWS Management Console, follow the steps provided in the context. Replace `region-code` and `EXAMPLED539D463

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not directly provide the steps to create an EFS CSI driver IAM role, but instead mentions using the AWS Management Console and provides some response metadata, which is not relevant to the question.

Evaluation_score
----------------------------------------
2

Ground truth answer
To create an IAM role for the Amazon EFS CSI driver, you can use `eksctl`, the AWS Management Console, or the AWS CLI. The specific steps to follow depend on your chosen method and are detailed in the given document.

====================================================================================================

Question
----------------------------------------
Install Amazon EFS CSI driver?

Documents
----------------------------------------
The Amazon EFS CSI driver requires IAM permissions to interact with your file system\. Create an IAM role and attach the required AWS managed policy to it\. You can use `eksctl`, the AWS Management Console, or the AWS CLI\.  
**Note**
The specific steps in this procedure are written for using the driver as an Amazon EKS add\-on\. For details on self\-managed installations, see [Set up driver permission](https://github.com/kubernetes-sigs/aws-efs-csi-driver#set-up-driver-permission) on GitHub\.  
------

Yes, as long as the [Amazon EBS CSI driver](ebs-csi.md) is installed\.

**To update the Amazon EBS CSI add\-on using the AWS Management Console**  
1. Open the Amazon EKS console at [https://console\.aws\.amazon\.com/eks/home\#/clusters](https://console.aws.amazon.com/eks/home#/clusters)\.  
1. In the left navigation pane, choose **Clusters**\.  
1. Choose the name of the cluster that you want to update the Amazon EBS CSI add\-on for\.  
1. Choose the **Add\-ons** tab\.  
1. Choose **Amazon EBS CSI Driver**\.  
1. Choose **Edit**\.  
1. On the **Configure Amazon EBS CSI Driver** page, do the following:  
1. Select the **Version** you'd like to use\.  
1. For **Select IAM role**, select the name of an IAM role that you attached the Amazon EBS CSI driver IAM policy to\.  
1. \(Optional\) You can expand the **Optional configuration settings** and modify as needed\.  
1. Choose **Save changes**\.  
------

Yes\. If you're installing the Amazon EBS CSI driver using the open source Helm chart, set `node.enableWindows` to `true`\. This is set by default if installing the Amazon EBS CSI driver as an Amazon EKS add\-on\. When creating `StorageClasses`, set the `fsType` to a Windows file system, such as `ntfs`\. Volume operations for Windows workloads are then migrated to the Amazon EBS CSI driver the same as they are for Linux workloads\.

Answer
----------------------------------------
Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide any information on how to install the Amazon EFS CSI driver.

Evaluation_score
----------------------------------------
1

Ground truth answer
We recommend installing the Amazon EFS CSI driver through the Amazon EKS add-on. If you're unable to use the Amazon EKS add-on, you can install it using the instructions on the GitHub repository.

====================================================================================================

Question
----------------------------------------
GitHub Copilot Chat use

Documents
----------------------------------------
- Get started with the [GitHub Copilot Chat tutorial](/docs/copilot/getting-started-chat.md)
- [Use the Chat Participant API to build a chat extension](/api/extension-guides/chat.md)

Previously, the Copilot Chat view was initially hidden and then later shown. The Copilot Chat view now remains active between window reloads so you don't have to manually reopen it. Additionally, the Chat view now guides you through the process of signing into GitHub and activating your free trial of GitHub Copilot.

This article answers frequently asked questions about using GitHub Copilot in Visual Studio Code.

We emphasize responsible usage of AI, especially when it comes to source code. The `inlineChat.acceptedOrDiscardBeforeSave` setting, which is enabled by default, asks you for confirmation before saving code that was generated by Copilot.  
When the setting is enabled, a file save operation waits for you to accept or discard any pending inline chat session. This also applies when Auto Save is enabled, which will be temporarily disabled until inline chat has ended.  
![Inline chat save consent message.](images/copilot-chat/inline-chat-save-consent.png)  
To enable more workspace search features for private repositories, we require additional permissions. If we detect that we don't have these permissions already, we will ask for them at startup. Once granted, we'll securely store the session for the future.  
![Modal window asking for additional authentication for a private repository.](images/copilot-chat/authentication.png)  
Learn more about security, privacy, and transparency in the [GitHub Copilot Trust Center](https://resources.github.com/copilot-trust-center/).

Answer
----------------------------------------
To enable more workspace search features for private repositories, additional permissions are required. If not already granted, GitHub Copilot will ask for these permissions at startup, and will securely store the session for future use. This is to ensure responsible usage of AI in source code generation.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not directly provide the information about GitHub Copilot Chat use. It talks about additional permissions required for workspace search features for private repositories, but that is not the same as using GitHub Copilot Chat.

Evaluation_score
----------------------------------------
2

Ground truth answer
The document provides information on how to use AI chat conversations with GitHub Copilot Chat.

====================================================================================================

Question
----------------------------------------
Copilot with programming languages

Documents
----------------------------------------
Copilot is not a compiler or language service  
Tools, which you may already be using, can help.

Copilot works best when it has sufficient context to know what you're doing and what you want help with. Just as you would provide a colleague with the context when asking for help with a specific programming task, you can do the same with Copilot.

[GitHub Copilot](https://copilot.github.com/) is an AI-powered code completion tool that helps you write code faster and smarter. You can use the [GitHub Copilot extension](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot) in VS Code to generate code, or to learn from the code it generates.  
[![GitHub Copilot extension in the VS Code Marketplace](images/python/copilot-extension.png)](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot)  
GitHub Copilot provides suggestions for languages beyond Python and a wide variety of frameworks, including JavaScript, TypeScript, Ruby, Go, C# and C++.  
You can learn more about how to get started with Copilot in the [Copilot documentation](/docs/editor/github-copilot.md).

- Get started with the [GitHub Copilot Chat tutorial](/docs/copilot/getting-started-chat.md)
- [Use the Chat Participant API to build a chat extension](/api/extension-guides/chat.md)

Answer
----------------------------------------
The context provided explains that GitHub Copilot is an AI-powered code completion tool that suggests code in various programming languages and frameworks, including but not limited to Python. It is not a compiler or language service. The tool works best when it has sufficient context about what the user is doing and what help is needed. The Copilot documentation and a tutorial on getting started with Copilot Chat are available for learning how to use the tool.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Yes, but lacks specific answer.

The response does acknowledge that GitHub Copilot is an AI-powered code completion tool that suggests code in various programming languages, including Python. However, it does not provide a specific answer to what programming languages Copilot works with, it only gives an example of one language (Python) and states that it works with "various programming languages and frameworks."

Evaluation_score
----------------------------------------
3

Ground truth answer
The VS Code Copilot Series on YouTube offers programming-specific videos for using Copilot with Python, C#, Java, PowerShell, and more.

====================================================================================================

Question
----------------------------------------
Copilot in Visual Studio Code

Documents
----------------------------------------
Copilot is not a compiler or language service  
Tools, which you may already be using, can help.

This tutorial walks you through setting up your GitHub Copilot subscription and installing the [GitHub Copilot](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot) extension in Visual Studio Code. After you complete these steps, you can get started with GitHub Copilot in VS Code.  
For an overview of what you can do with GitHub Copilot in VS Code, see the [GitHub Copilot Overview](/docs/copilot/overview.md).

While the Chat view is great for keeping a conversation going with Copilot, having access to chat directly from the editor might be more efficient for particular scenarios.  
Let's look at how to use chat for code refactoring.  
1. Open the `app.ts` file in the editor and select the line `const port = 3000`  
By selecting a line of code, you can provide more context to Copilot about what we want to change.  
![Screenshot of VS Code editor, highlighting a selected line in the editor.](./images/getting-started-chat/copilot-inline-chat-selection.png)  
1. Then press `kb(inlinechat.start)` on your keyboard to bring up Copilot inline chat.  
Copilot inline chat enables you to ask questions to Copilot directly from the editor.  
![Screenshot of VS Code editor, highlighting the inline chat popup control.](./images/getting-started-chat/copilot-inline-chat-popup.png)  
Let's now ask Copilot to make the port number configurable.  
1. Enter *make configurable* in the chat input field and press `kbstyle(Enter)`.  
Notice how Copilot updates the selected code and suggests reading the port number from an environment variable.  
![Screenshot of VS Code editor with the suggested code change.](./images/getting-started-chat/copilot-inline-chat-configurable-port.png)  
You can view the applied changes in a diff view by selecting the **Show Changes** control.  
![Screenshot of VS Code inline chat, highlighting the 'Show changes' button and diff editor.](./images/getting-started-chat/copilot-inline-chat-show-changes.png)  
1. Select **Accept** or **Discard** to apply or ignore the changes.  
If you're not happy with the suggested code changes, you can select the **Regenerate** button to get another suggestion.  
Congratulations on using Copilot inline chat in the editor to help you with code refactoring!

To give you relevant inline suggestions, Copilot looks at the current and open files in your editor to analyze the context and create appropriate suggestions. Having related files open in VS Code while using Copilot helps set this context and lets the Copilot see a bigger picture of your project.

Answer
----------------------------------------
Copilot is a tool in Visual Studio Code that provides context-aware suggestions for code refactoring and other tasks. It can be set up with a GitHub Copilot subscription and the corresponding VS Code extension. Copilot looks at the current and open files in the editor to analyze the context and create appropriate suggestions, and it can be accessed directly from the editor using inline chat. To use chat for code refactoring, you can select a line of code, bring up Copilot inline chat, and enter a request like "make configurable" to get relevant suggestions for code changes. Copilot can be helpful for code refactoring and other tasks, especially when related files are open in the editor to provide context.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The response directly answers the question about Copilot in Visual Studio Code, explaining what it is, how it works, and its uses for code refactoring and other tasks. The explanation is detailed and informative, making it a high-quality response.

Evaluation_score
----------------------------------------
5

Ground truth answer
The document provides information on how to use GitHub Copilot in Visual Studio Code.

====================================================================================================

Question
----------------------------------------
copilot install

Documents
----------------------------------------
Copilot is not a compiler or language service  
Tools, which you may already be using, can help.

Yes, you can switch to the pre-release (nightly) version of a Copilot extension to try the latest features and fixes. From the Extensions view, right-click or select the gear icon to bring up the context menu, and then select **Switch to Pre-Release Version**:  
![Extensions view context menu with Switch to Pre-Release Version option](images/faq/switch-to-pre-release.png)  
You can tell if you're running a pre-release version by the "Pre-release" badge in the extension details:  
![Pre-release version of the GitHub Copilot extension](images/faq/copilot-ext-pre-release.png)

---
Order: 2
Area: copilot
TOCTitle: Setup
ContentId: 37fd3bd2-4209-49f6-bec5-c544d6b1b289
PageTitle: Set up GitHub Copilot
DateApproved: 02/28/2024
MetaDescription: Access your GitHub Copilot subscription and set up GitHub Copilot in Visual Studio.
MetaSocialImage: images/shared/github-copilot-social.png
---

A preview of the terminal Inline Chat is available in this release, which gives convenient access to Copilot's capabilities directly in the terminal.  
You can enable terminal Inline Chat with the <code codesetting="terminal.integrated.experimentalInlineChat:true">terminal.integrated.experimentalInlineChat</code> setting. To invoke the inline chat in a terminal, use the `kb(inlinechat.start)` keybinding.  
![Opening terminal inline chat will open and focus an input box similar to inline chat in the editor](images/1_88/copilot-terminal-inline.png)  
The terminal Inline Chat uses the `@terminal` chat participant, which has context about the integrated terminal's shell and its contents.  
![You can ask complex questions like "list the top 5 largest files in the src directory"](images/1_88/copilot-terminal-inline-query.png)  
Once a command is suggested, use `kb(workbench.action.terminal.chat.runCommand)` to run the command in the terminal or `kb(workbench.action.terminal.chat.insertCommand)` to insert the command into the terminal. The command can also be edited directly in Copilot's response before running it (currently `kbstyle(Ctrl+down)`, `kbstyle(Tab)`, `kbstyle(Tab)` on Windows & Linux, `kbstyle(Cmd+down)`, `kbstyle(Tab)`, `kbstyle(Tab)` on macOS).  
![Complex queries are OS and shell-specific](images/1_88/copilot-terminal-inline-answer.png)

Answer
----------------------------------------
The context discusses how to set up and use GitHub Copilot, including enabling a preview of the terminal Inline Chat feature. However, it does not mention installing Copilot.

Answer: Context is irrelevant to the question.
Reason: The context provided does not discuss installing Copilot.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not provide information on how to install Copilot, nor does it confirm if the process is mentioned in the given context.

Evaluation_score
----------------------------------------
2

Ground truth answer
To use GitHub Copilot in Visual Studio Code, you must have the GitHub Copilot extension, which is installed from the Visual Studio Code marketplace.

====================================================================================================

Question
----------------------------------------
Graphical artifacts fix

Documents
----------------------------------------
Apple recently released the final version of macOS Sierra and with the Electron update, we were able to fix some issues we had seen (fonts and icons did not look sharp on a Retina display). However, [some users](https://github.com/microsoft/vscode/issues/12473) are seeing bad background artifacts in the editor the underlying issue is Chrome related and it seems to happen when you are using a custom color profile. There is a workaround, you can run VS Code with forced GPU rasterization to mitigate this issue:  
```bash
code --force-gpu-rasterization
```

- [Install OEM D kernel](#step-1)
- [Allow both CPU and platform drivers to be simultaneously active](#step-3-allow-both-cpu-and-platform-drivers-to-be-simultaneously-active)
- [Prevent graphical artifacts from appearing](#addtionally-we-recommend-the-following-as-well-if-you-are-experiencing-graphical-artifacts-from-appearing)

Dashed and dotted underlines in the terminal are now rendered in a pixel perfect pattern:  
![Dotted and dashed underlines are now pixel perfect](images/1_85/terminal-underline.png)  
![The underline improvements also work when zoomed in](images/1_85/terminal-underline-zoom.png)

Today, finding a "bad" extension can be easy or hard.  Open the Extensions view (`kb(workbench.view.extensions)`), [disable an extension](/docs/editor/extension-marketplace.md#disable-an-extension), reload the window (**Developer: Reload Window**), and check to see if the problem still exists. If the problem is gone, that extension is "bad" and you are done. Otherwise, re-enable the extension and repeat the process with the next extension.  
![Disabling an extension step by step](./disable_manually.png)  
If you are lucky, the first extension is the "bad" one; if you are unlucky, it's the last extension. Using computer science language, this means with `N` extensions, you have a worst-case of repeating the process `O(N)` (order N), and an average-case of `O(N/2)`. Because this algorithm is operated by a human (you), even small values of `N` are laborious. This is where the **extension bisect** utility comes in handy. It is much better in the worst and average cases because it disables extensions by halves.

Answer
----------------------------------------
The context provides information on how to fix graphical artifacts in VS Code, which is a Chrome-related issue and can be mitigated by running VS Code with forced GPU rasterization. The context also describes how to find a "bad" extension by disabling them one by one and checking if the problem still exists. However, the extension bisect utility can be used to make this process faster by disabling extensions by halves. The context is relevant to the question, and I can answer the question confidently.

Answer: To address the graphical artifacts issue, you can run VS Code with forced GPU rasterization using the command `code --force-gpu-rasterization`. To find a "bad" extension, you can use the extension bisect utility, which disables extensions by halves, making the process faster and more efficient.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The answer provides specific steps on how to fix the graphical artifacts issue in VS Code, including running VS Code with forced GPU rasterization and using the extension bisect utility to find a "bad" extension. The answer is clear, concise, and directly addresses the question.

Evaluation_score
----------------------------------------
5

Ground truth answer
The document suggests following a guide to allocate additional RAM to the iGPU for the Framework Laptop 13 AMD Ryzen 7040 Series to prevent graphical artifacts from appearing.

====================================================================================================

Question
----------------------------------------
Kernel alert look

Documents
----------------------------------------
&nbsp; &nbsp;
> **Note:** This will appear if the code below is pasted into the terminal, enter key pressed and system rebooted.
When a new version of the OEM kernel is ready, this will alert you at bootup - if you're *on the current OEM D kernel* AND you have *followed my above directions*, then and only then **you will not be alerted**.  
![What does the OEM Kernel alert looks like](https://raw.githubusercontent.com/FrameworkComputer/linux-docs/d8becead412d3858a1f561fb2f827f803ab17c47/oem-d-alert.png)  
&nbsp; &nbsp; &nbsp; &nbsp;  
---------

Screen reader and braille users now have more configurable alerts, such as when debug breakpoints are hit, if there are errors on the current line, and more. These can be configured via settings starting with `accessibility.alert`, or explored and configured with the **Help: List Alerts** command.

Status bar items that represent warnings can use the newly added colors `statusBarItem.warningBackground` and `statusBarItem.warningForeground`.

Optional flags to determine what types of messages should be logged to the Debug Console.  
- **exceptions**: Optional flag to determine whether exception messages should be logged to the Debug Console. Defaults to true.
- **moduleLoad**: Optional flag to determine whether module load events should be logged to the Debug Console. Defaults to true.
- **programOutput**: Optional flag to determine whether program output should be logged to the Debug Console. Defaults to true.
- **engineLogging**: Optional flag to determine whether diagnostic engine logs should be logged to the Debug Console. Defaults to false.
- **trace**: Optional flag to determine whether diagnostic adapter command tracing should be logged to the Debug Console. Defaults to false.
- **traceResponse**: Optional flag to determine whether diagnostic adapter command and response tracing should be logged to the Debug Console. Defaults to false.

Answer
----------------------------------------
Context is irrelevant to the question.

Explanation: The context provided is about system notifications, screen reader and braille user alerts, and debug console message flags. There is no question related to these topics. The question is about the appearance of a Kernel alert.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The response does not provide an answer to the question. It explains why the context provided is not relevant, but it fails to address the actual question about the appearance of a Kernel alert.

Evaluation_score
----------------------------------------
2

Ground truth answer
The document shows what the OEM Kernel alert looks like on the Framework Laptop 13 AMD Ryzen 7040 Series when a new version of the OEM kernel is ready and the user is on the current OEM D kernel.

====================================================================================================

Question
----------------------------------------
Suspend with lid workaround

Documents
----------------------------------------
- [Install OEM D kernel](#step-1)
- [Allow both CPU and platform drivers to be simultaneously active](#step-4)
- [Suspend with lid while attached to power workaround](#step-6)
- [Prevent graphical artifacts from appearing](#step-5)

There is an active bug that occurs for some users, creating a bogus key press when you suspend. This provides a solid workaround.  
```
sudo sh -c '[ ! -f /etc/udev/rules.d/20-suspend-fixes.rules ] && echo "ACTION==\"add\", SUBSYSTEM==\"serio\", DRIVERS==\"atkbd\", ATTR{power/wakeup}=\"disabled\"" > /etc/udev/rules.d/20-suspend-fixes.rules'
```
This checks for an existing /etc/udev/rules.d/20-suspend-fixes.rules file, if none is found, creates it and appends ACTION=="add", SUBSYSTEM=="serio", DRIVERS=="atkbd", ATTR{power/wakeup}="disabled" to the file.  
&nbsp; &nbsp; &nbsp; &nbsp;
---------

The `disconnect` request is used to end a debug session and to either continue execution of the debuggee or to terminate it. There is a new option to keep the debuggee in the suspended state after the debugger disconnects. This makes it possible to pick up debugging with a new session.  
A debug adapter implementing this feature must announce its support with the `supportSuspendDebuggee` capability. A client can use the feature by passing a new optional argument `suspendDebuggee` to the `disconnect` request.

Folding range providers can signal to the editor that folding ranges need to be updated using the `onDidChangeFoldingRanges` event.  
For more details and to provide feedback, please use [issue #108929](https://github.com/microsoft/vscode/issues/108929).

Answer
----------------------------------------
The context is not relevant to the question.
Reason: Context is irrelevant to the question.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide any information about a "suspend with lid workaround". It simply states that the context is not relevant to the question.

Evaluation_score
----------------------------------------
1

Ground truth answer
The document offers a workaround for an active bug that causes a bogus key press when suspending the Framework Laptop 13 AMD Ryzen 7040 Series while attached to power.

====================================================================================================

Question
----------------------------------------
change GRUB default

Documents
----------------------------------------
``sudo gedit /etc/default/grub``

``sudo gnome-text-editor /etc/default/grub``

```
GRUB_DEFAULT="0"
```  
into  
```
GRUB_DEFAULT="Advanced options for Ubuntu>Ubuntu, with Linux 6.5.0.1009-oem"
```
> **TIP:** You can use the little clipboard icon to the right of the code to copy to your clipboard.

If encrypted, make sure you place this correctly or use the copy/paste command above instead.  
``sudo gedit /etc/default/grub``

Answer
----------------------------------------
To change the GRUB default entry, you need to modify the `GRUB_DEFAULT` line in the `/etc/default/grub` file. In your case, you should change it from:

```
GRUB_DEFAULT="0"
```

to:

```
GRUB_DEFAULT="Advanced options for Ubuntu>Ubuntu, with Linux 6.5.0.1009-oem"`
```

After making this change, you should update the GRUB configuration by running:

```
sudo update-grub
```

Please note that the actual entry names in the `GRUB_DEFAULT` list may vary depending on your system's configuration. Make sure to replace the entry name with the correct one for your system.

Evaluation
----------------------------------------
Score (1-5): 5
Answered: Yes.

The answer provides clear and detailed instructions on how to change the GRUB default. It explains the exact steps, including modifying the GRUB\_DEFAULT line in the /etc/default/grub file and updating the GRUB configuration using the sudo update-grub command. The answer also mentions the potential variation in entry names and advises the user to replace the entry name with the correct one for their system.

Evaluation_score
----------------------------------------
5

Ground truth answer
The command to change the GRUB default is "sudo sed -i 's/^GRUB_DEFAULT=\"0\"/GRUB_DEFAULT=\"Advanced options for Ubuntu>Ubuntu, with Linux 6.5.0.1013-oem\"/' /etc/default/grub".

====================================================================================================

Question
----------------------------------------
What is CORSMiddleware?

Documents
----------------------------------------
For more info about <abbr title="Cross-Origin Resource Sharing">CORS</abbr>, check the <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS" class="external-link" target="_blank">Mozilla CORS documentation</a>.  
!!! note "Technical Details"
You could also use `from starlette.middleware.cors import CORSMiddleware`.  
**FastAPI** provides several middlewares in `fastapi.middleware` just as a convenience for you, the developer. But most of the available middlewares come directly from Starlette.

It's the most popular Python framework and is widely trusted. It is used to build systems like Instagram.  
It's relatively tightly coupled with relational databases (like MySQL or PostgreSQL), so, having a NoSQL database (like Couchbase, MongoDB, Cassandra, etc) as the main store engine is not very easy.  
It was created to generate the HTML in the backend, not to create APIs used by a modern frontend (like React, Vue.js and Angular) or by other systems (like <abbr title="Internet of Things">IoT</abbr> devices) communicating with it.

There are several Flask REST frameworks, but after investing the time and work into investigating them, I found that many are discontinued or abandoned, with several standing issues that made them unfit.

[Data Wrangler](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.datawrangler) is a code-centric data viewing and cleaning tool that is integrated into VS Code and VS Code Jupyter Notebooks. It provides a rich user interface to view and analyze your data, show insightful column statistics and visualizations, and automatically generate Pandas code as you clean and transform the data.  
The following is an example of opening Data Wrangler from the notebook to analyze and clean the data with the built-in operations. Then the automatically generated code is exported back into the notebook.  
![a gif of opening Data Wrangler from a notebook, looking through the data, switching from Viewing to Editing mode, applying data transformations, and exporting the generated Python code back into the notebook](images/data-wrangler-quick-start/full-dw-loop.gif)  
The goal of this page is to help you quickly get up and running with Data Wrangler.

Answer
----------------------------------------
Context is irrelevant to the question.

OR

I do not know the answer. Reason: The provided context does not contain information about CORSMiddleware.

Evaluation
----------------------------------------
Score (1-5): 2
Answered: No.

The answer does not provide any information about what CORSMiddleware is.

Evaluation_score
----------------------------------------
2

Ground truth answer
You can configure CORS in your FastAPI application using the CORSMiddleware.

====================================================================================================

Question
----------------------------------------
What are allowed origins?

Documents
----------------------------------------
An origin is the combination of protocol (`http`, `https`), domain (`myapp.com`, `localhost`, `localhost.tiangolo.com`), and port (`80`, `443`, `8080`).  
So, all these are different origins:  
* `http://localhost`
* `https://localhost`
* `http://localhost:8080`  
Even if they are all in `localhost`, they use different protocols or ports, so, they are different "origins".

It's also possible to declare the list as `"*"` (a "wildcard") to say that all are allowed.  
But that will only allow certain types of communication, excluding everything that involves credentials: Cookies, Authorization headers like those used with Bearer Tokens, etc.  
So, for everything to work correctly, it's better to specify explicitly the allowed origins.

There are certain restrictions we impose upon extensions. Here are the restrictions and their purposes.

Uses `var` to declare local variables.

Answer
----------------------------------------
Context is irrelevant to the question.

(The context provided discusses the concept of an "origin" in web development and the restrictions imposed on extensions, but it does not provide any information about what the "allowed origins" might be.)

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide any information about what "allowed origins" are, instead it questions the relevance of the context to the question.

Evaluation_score
----------------------------------------
1

Ground truth answer
The backend must have a list of "allowed origins" to allow the JavaScript in the frontend send its request to the backend.

====================================================================================================

Question
----------------------------------------
When to use wildcards?

Documents
----------------------------------------
VS Code supports the following glob syntax:  
* `/` to separate path segments
* `*` to match zero or more characters in a path segment
* `?` to match on one character in a path segment
* `**` to match any number of path segments, including none
* `{}` to group conditions (for example `{**/*.html,**/*.txt}` matches all HTML and text files)
* `[]` to **declare** a range of characters to match (`example.[0-9]` to match on `example.0`, `example.1`, …)
* `[!...]` to negate a range of characters to match (`example.[!0-9]` to match on `example.a`, `example.b`, but not `example.0`)  
**Note:** Paths are separated by `/` and not `\` even on Windows. But when applied, glob patterns will match paths with both slash and backslashes.

- **loadAll**: If true, symbols for all libs will be loaded, otherwise no solib symbols will be loaded. Modified by ExceptionList. Default value is true.
- **exceptionList**: List of filenames (wildcards allowed) separated by semicolons `;`. Modifies behavior of LoadAll. If LoadAll is true then don't load symbols for libs that match any name in the list. Otherwise only load symbols for libs that match. Example: ```"foo.so;bar.so"```

Let's say you have a directory named `my_super_project` that contains a sub-directory called `sql_app` with a structure like this:  
```
.
└── sql_app
├── __init__.py
├── crud.py
├── database.py
├── main.py
└── schemas.py
```  
This is almost the same structure as we had for the SQLAlchemy tutorial.  
Now let's see what each file/module does.

We have improved the filtering and scoring of suggestions. In addition to prefix and camel-case matching continuous substring matching is supported.  
![dnd](images/1_11/fuzzy-score.png)

Answer
----------------------------------------
You should use wildcards in the `exceptionList` option to exclude specific libraries from having their symbols loaded. For example, if you want to exclude any libraries with "sql" in their name, you could set `exceptionList` to `"*.sql*"`. This will prevent any files that match the pattern (like the `sql_app` directory in your example) from having their symbols loaded.

Evaluation
----------------------------------------
Score (1-5): 3
Answered: Partially

Even though the answer provides an example of using wildcards in the `exceptionList` option, it does not explicitly explain when to use wildcards in general. It would be more appropriate to mention that wildcards are useful when you want to apply a pattern to multiple files or names and perform a specific action on them, such as excluding or including them based on that pattern.

Evaluation_score
----------------------------------------
3

Ground truth answer
It's also possible to declare the list as `"*"` (a "wildcard") to say that all are allowed, but that will only allow certain types of communication, excluding everything that involves credentials.

====================================================================================================

Question
----------------------------------------
What is origin?

Documents
----------------------------------------
An origin is the combination of protocol (`http`, `https`), domain (`myapp.com`, `localhost`, `localhost.tiangolo.com`), and port (`80`, `443`, `8080`).  
So, all these are different origins:  
* `http://localhost`
* `https://localhost`
* `http://localhost:8080`  
Even if they are all in `localhost`, they use different protocols or ports, so, they are different "origins".

In VS Code's core codebase, we use a type called `ILocalizedString` to render both a localized and an English string next to each other. You might have seen this if you use a language pack and you open the Command Palette.  
Previously, we had to write these expressions like:  
```ts
const str = { value: localize('id', "Hello World"), original: "Hello World" };
```  
This can now be expressed as follows:  
```ts
const str = localize2('id', "Hello World");
```  
Our localization pipeline will then pick up these `localize2` occurrences.

What inspired **FastAPI**, how it compares to alternatives and what it learned from them.

The documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.  
The sample code within this documentation is made available under a modified MIT license. See the LICENSE-SAMPLECODE file.

Answer
----------------------------------------
Context is irrelevant to the question.
Reason: The context provided does not give any information about the origin in the context of what inspired FastAPI or its comparison to alternatives.

Evaluation
----------------------------------------
Score (1-5): 1
Answered: No.

The answer does not provide any information about the origin of FastAPI or its inspiration. It simply states that the context is irrelevant, which does not contribute to answering the question.

Evaluation_score
----------------------------------------
1

Ground truth answer
An origin is the combination of protocol (`http`, `https`), domain (`myapp.com`, `localhost`, `localhost.tiangolo.com`), and port (`80`, `443`, `8080`).

====================================================================================================

