{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\testbed\\ragas_cookbook\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating model exists...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness, faithfulness\n",
    "from ragas.testset.evolutions import multi_context, reasoning, simple\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "\n",
    "sys.path.append(str(Path.cwd().parents[1]))  # nopep8\n",
    "from utilities.ollama import verify_ollama_model_present\n",
    "from utilities.project_paths import sample_document_dirpath\n",
    "\n",
    "MODEL_NAME = \"openchat\"\n",
    "# https://ollama.com/library/openchat\n",
    "\n",
    "generator_llm = Ollama(model=MODEL_NAME)\n",
    "embeddings = OllamaEmbeddings(model=MODEL_NAME)\n",
    "\n",
    "# make sure that the ollama is available to use locally (might need to ollama pull)\n",
    "verify_ollama_model_present(\"openchat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 14.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader, TextLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=sample_document_dirpath,\n",
    "    loader_cls=UnstructuredMarkdownLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "raw_documents = loader.load()\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "\n",
    "# split it into chunks\n",
    "docs = []\n",
    "for i in raw_documents:\n",
    "    docs.extend(markdown_splitter.split_text(i.page_content))\n",
    "\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=generator_llm,\n",
    "    embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# generate testset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43msimple\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_context\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m testset\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynthetic_testset.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\projects\\testbed\\ragas_cookbook\\venv\\lib\\site-packages\\ragas\\testset\\generator.py:175\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[1;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[0;32m    173\u001b[0m distributions \u001b[38;5;241m=\u001b[39m distributions \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_langchain_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    180\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[0;32m    181\u001b[0m     distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    186\u001b[0m )\n",
      "File \u001b[1;32md:\\projects\\testbed\\ragas_cookbook\\venv\\lib\\site-packages\\ragas\\testset\\docstore.py:215\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_documents\u001b[1;34m(self, docs, show_progress)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[0;32m    211\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    212\u001b[0m     Node\u001b[38;5;241m.\u001b[39mfrom_langchain_document(d)\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39mtransform_documents(docs)\n\u001b[0;32m    214\u001b[0m ]\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\testbed\\ragas_cookbook\\venv\\lib\\site-packages\\ragas\\testset\\docstore.py:252\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_nodes\u001b[1;34m(self, nodes, show_progress)\u001b[0m\n\u001b[0;32m    245\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mextract,\n\u001b[0;32m    247\u001b[0m             n,\n\u001b[0;32m    248\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyphrase-extraction[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    249\u001b[0m         )\n\u001b[0;32m    250\u001b[0m         result_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 252\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[1;32md:\\projects\\testbed\\ragas_cookbook\\venv\\lib\\site-packages\\ragas\\executor.py:132\u001b[0m, in \u001b[0;36mExecutor.results\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m executor_job\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m     \u001b[43mexecutor_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents,\n",
    "                                                 test_size=len(documents), distributions={\n",
    "                                                     simple: 0.5, reasoning: 0.3, multi_context: 0.2})\n",
    "\n",
    "df = testset.to_pandas()\n",
    "df.to_excel('synthetic_testset.xlsx')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AWS Chalice and AWS Lambda REST API example\\n\\nPurpose\\n\\nShows how to use AWS Chalice with the AWS SDK for Python (Boto3) to \\ncreate a serverless REST API that uses Amazon API Gateway, AWS Lambda, and \\nAmazon DynamoDB. The REST API simulates a system that tracks daily cases\\nof COVID-19 in the United States, using fictional data. Learn how to:\\n\\nUse AWS Chalice to define routes in AWS Lambda functions that\\n are called to handle REST requests that come through Amazon API Gateway.\\n\\nUse AWS Lambda functions to retrieve and store data in an Amazon DynamoDB \\ntable to serve REST requests.\\n\\nDefine table structure and security role resources in an AWS CloudFormation template.\\n\\nUse AWS Chalice and AWS CloudFormation to package and deploy all necessary resources.\\n\\nUse AWS CloudFormation to clean up all created resources.\\n\\nThis example brings together some of the same information you can find in the\\ntutorials in the \\nAWS Chalice GitHub repository.\\n\\nPrerequisites\\n\\nYou must have an AWS account, and have your default credentials and AWS Region\\n  configured as described in the AWS Tools and SDKs Shared Configuration and\\n  Credentials Reference Guide.\\n\\nPython 3.7 or later\\n\\nAn Amazon S3 bucket\\n\\nCautions\\n\\nAs an AWS best practice, grant this code least privilege, or only the \\n  permissions required to perform a task. For more information, see \\n  Grant Least Privilege \\n  in the AWS Identity and Access Management \\n  User Guide.\\n\\nThis code has not been tested in all AWS Regions. Some AWS services are \\n  available only in specific Regions. For more information, see the \\n  AWS Region Table\\n  on the AWS website.\\n\\nRunning this code might result in charges to your AWS account.\\n\\nRunning the code\\n\\nInstall prerequisites by running the following at a command prompt.\\n\\npython -m pip install -r requirements.txt\\n\\nRun the following to create a deployment package in a subfolder named out.\\nchalice package --merge-template resources.json out\\n\\nRun the following to prepare the package for deployment. Replace the\\nYOUR-BUCKET-NAME placeholder with the name of an Amazon S3 bucket that you control.\\naws cloudformation package --template-file out/sam.json \\\\ \\n   --s3-bucket YOUR-BUCKET-NAME --output-template-file out/template.yml\\n\\nRun the following to create the resources and deploy your REST API to AWS.\\naws cloudformation deploy --template-file out\\\\template.yml \\\\ \\n--stack-name ChaliceRestDemo --capabilities CAPABILITY_IAM\\nAt this point, the REST API is available and can be called from any client\\nthat can issue HTTP requests.\\n\\nRun the following to find the URL of the REST API in the AWS CloudFormation stack.\\n\\naws cloudformation describe-stacks --stack-name ChaliceRestDemo \\\\\\n   --query \"Stacks[0].Outputs[?OutputKey==\\'EndpointURL\\'].OutputValue\" --output text\\n\\nAppend \"states\" to the base URL returned by the previous step and use curl to\\nget the list of states from the API. For example, if your endpoint ID is 1234567890, \\nrun the following command.\\ncurl https://1234567890.execute-api.us-west-2.amazonaws.com/api/states\\n\\nStart the client demonstration by running the following command. The client\\ndemo uses the Requests package to send requests to the REST API.\\npython client_demo.py\\n\\nAfter the demonstration completes, clean up all resources by running the following\\ncommand.\\naws cloudformation delete-stack --stack-name ChaliceRestDemo\\n\\nExample structure\\n\\nThe example is divided into the following files.\\n\\napp.py\\n\\nDefines the routes for the REST API. Uses Chalice to decorate route functions and\\nhandle deserializing requests and serializing responses. Calls a custom Storage\\nclass to handle moving data in and out of an Amazon DynamoDB table.\\n\\nclient_demo.py\\n\\nShows how to use the Requests package to send a variety of requests to the REST API.\\nIf the api_url option is not specified, the script finds the base REST URL in the \\nAWS CloudFormation stack.\\n\\nrequirements.txt\\n\\nDefines the minimum version of Boto3 to deploy to AWS Lambda.\\n\\nresources.json\\n\\nAn additional AWS CloudFormation template that is merged into the main template when\\nthe deployment package is created. This template defines additional resources used\\nby this demo, such as an Amazon DynamoDB table and IAM role.\\n\\nchalicelib/covid_data.py\\n\\nThe Storage class that handles moving data in and out of Amazon DynamoDB in response\\nto REST requests. When a chalicelib module is present, Chalice automatically\\ndeploys its contents to AWS Lambda so that it is available to the main app.py module.\\n\\n.chalice/config.json\\n\\nConfiguration for the application. The TABLE_NAME environment variable is deployed\\nto AWS Lambda and is used by the Storage class to access the Amazon DynamoDB table.\\n\\nRunning the tests\\n\\nThe unit tests in this module use the botocore Stubber. This captures requests before \\nthey are sent to AWS, and returns a mocked response. To run all of the tests, \\nrun the following in your \\n[GitHub root]/python/example_code/lambda/chalice_examples/lambda_rest folder.\\n\\npython -m pytest\\n\\nAdditional information\\n\\nAWS Chalice Quickstart\\n\\nAWS Command Line Interface User Guide\\n\\nRequests documentation\\n\\nBoto3 Quickstart\\n\\nBoto3 Amazon API Gateway service reference\\n\\nBoto3 AWS Lambda service reference\\n\\nActual COVID-19 data can be found in Amazon\\'s public COVID-19 data lake\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0', metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\apigateway_covid-19_tracker.md'}),\n",
       " Document(page_content='Amazon API Gateway websocket chat example\\n\\nPurpose\\n\\nShows how to use the AWS SDK for Python (Boto3) with Amazon API Gateway V2 to\\ncreate a websocket API that integrates with AWS Lambda and Amazon DynamoDB.\\n\\nCreate a websocket API served by API Gateway.\\n\\nDefine a Lambda handler that stores connections in DynamoDB and posts messages to\\nother chat participants.\\n\\nConnect to the websocket chat application and send messages with the Websockets\\npackage.\\n\\nYou can create a similar API Gateway websocket chat application by using \\nAWS Chalice.\\nFor a tutorial, see \\nChat Server Example.\\n\\nPrerequisites\\n\\nYou must have an AWS account, and have your default credentials and AWS Region\\n  configured as described in the AWS Tools and SDKs Shared Configuration and\\n  Credentials Reference Guide.\\n\\nPython 3.8.5 or later\\n\\nCautions\\n\\nAs an AWS best practice, grant this code least privilege, or only the \\n  permissions required to perform a task. For more information, see \\n  Grant Least Privilege \\n  in the AWS Identity and Access Management \\n  User Guide.\\n\\nThis code has not been tested in all AWS Regions. Some AWS services are \\n  available only in specific Regions. For more information, see the \\n  AWS Region Table\\n  on the AWS website.\\n\\nRunning this code might result in charges to your AWS account.\\n\\nRunning the code\\n\\nInstall prerequisites by running the following at a command prompt.\\n\\npython -m pip install -r requirements.txt\\n\\nThis example requires AWS resources that can be deployed by the \\nAWS CloudFormation stack that is defined in the accompanying setup.yaml file.\\nThis stack manages the following resources:\\n\\nA DynamoDB table with a specific key schema.\\n\\nA Lambda function that handles API Gateway websocket request events.\\n\\nAn AWS Identity and Access Management (IAM) role that grants permission to let \\nLambda run the function and perform actions on the table.\\n\\nDeploy resources\\n\\nDeploy prerequisite resources by running the example script with the deploy flag at \\na command prompt.\\n\\npython websocket_chat.py deploy\\n\\nRun the usage demonstration\\n\\nCreate and deploy the API Gateway websocket API by running with the demo flag at \\na command prompt.\\n\\npython websocket_chat.py demo\\n\\nRun the chat demonstration\\n\\nSee an automated demo of how to use the Websockets package to connect and send \\nmessages to the chat application by running with the chat flag at a command prompt.\\n\\npython websocket_chat.py chat\\n\\nNote: The Lambda handler for the chat application writes to an\\nAmazon CloudWatch log. Checking this log can help you troubleshoot issues and give \\nadditional insight into the application.\\n\\nDestroy resources\\n\\nDestroy example resources by running the script with the destroy flag at a command \\nprompt.\\n\\npython websocket_chat.py destroy\\n\\nExample structure\\n\\nThe example contains the following files.\\n\\nlambda_chat.py\\n\\nShows how to implement an AWS Lambda function as part of a websocket chat application.\\nThe function handles messages from an Amazon API Gateway websocket API and uses an\\nAmazon DynamoDB table to track active connections by taking the following actions.\\n\\nA $connect request adds a connection ID and the associated user name to the\\nDynamoDB table.\\n\\nA sendmessage request scans the table for connections and uses the API \\nGateway Management API to post the message to all other connections.\\n\\nA $disconnect request removes the connection record from the table.\\n\\nwebsocket_chat.py\\n\\nShows how to use API Gateway V2 to create a websocket API that is backed by a \\nLambda function. The usage_demo and chat_demo scripts in this file show how to \\naccomplish the following actions.\\n\\nCreate a websocket API served by API Gateway.\\n\\nAdd resources to the websocket API that represent websocket connections and \\nchat messages.\\n\\nAdd integration methods so the websocket API uses a Lambda function to handle \\nincoming requests.\\n\\nUse the Websockets package to connect users to the chat application and send \\nmessages to other chat participants.\\n\\nsetup.yaml\\n\\nContains a CloudFormation script that is used to create the resources needed for \\nthe demo. Pass the deploy or destroy flag to the websocket_chat.py script to \\ncreate or remove these resources:\\n\\nA DynamoDB table\\n\\nA Lambda function\\n\\nAn IAM role\\n\\nThe setup.yaml file was built from the \\nAWS Cloud Development Kit (AWS CDK) \\nsource script here: \\n/resources/cdk/python_example_code_apigateway_websocket/setup.ts.\\n\\nRunning the tests\\n\\nThe unit tests in this module use the botocore Stubber. The Stubber captures requests \\nbefore they are sent to AWS, and returns a mocked response. To run all of the tests, \\nrun the following command in your \\n[GitHub root]/python/example_code/apigateway/websocket folder.\\n\\npython -m pytest\\n\\nAdditional information\\n\\nBoto3 Amazon API Gateway V2 service reference\\n\\nBoto3 Amazon API Gateway Management API service reference\\n\\nAmazon API Gateway documentation\\n\\nAWS Lambda documentation\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0', metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\apigateway_websocket_chat.md'}),\n",
       " Document(page_content='Track work items in an Aurora Serverless database with the SDK for Python\\n\\nOverview\\n\\nThis example shows you how to use the AWS SDK for Python (Boto3) to create a REST \\nservice that lets you do the following:\\n\\nBuild a Flask REST service that integrates with AWS services.\\n\\nRead, write, and update work items that are stored in an Amazon Aurora Serverless database.\\n\\nCreate an AWS Secrets Manager secret that contains database credentials and use it\\n  to authenticate calls to the database.\\n\\nUse Amazon Simple Email Service (Amazon SES) to send email reports of work items.\\n\\nThe REST service is used in conjunction with the Elwing React client\\nto present a fully functional web application.\\n\\n⚠️ Important\\n\\nRunning this code might result in charges to your AWS account.\\n\\nRunning the tests might result in charges to your AWS account.\\n\\nWe recommend that you grant your code least privilege. At most, grant only the minimum \\n  permissions required to perform the task. For more information, see \\n  Grant least privilege.\\n\\nThis code is not tested in every AWS Region. For more information, see \\n  AWS Regional Services.\\n\\nPrerequisites\\n\\nPrerequisites for running examples can be found in the \\nREADME in the Python folder.\\n\\nIn addition to the standard prerequisites, this example also requires:\\n\\nFlask 2.2.0 or later\\n\\nFlask-Cors 3.0.10 or later\\n\\nwebargs 8.2.0 or later\\n\\nYou can install all of the prerequisites by running the following in a virtual environment:\\n\\npython -m pip install -r requirements.txt\\n\\nCreate the resources\\n\\nAurora Serverless DB cluster and Secrets Manager secret\\n\\nThis example requires an Aurora DB cluster with the Data API feature enabled. As of March 2024,\\nthe available choices are:\\n* Aurora PostgreSQL using Aurora Serverless v2 or provisioned instances in the cluster.\\n* Aurora MySQL or Aurora PostgreSQL using a Serverless v1 cluster.\\n\\nFor this example, we assume that the Aurora cluster uses a combination of Aurora PostgreSQL\\nand Aurora Serverless v2.\\n\\nThe database must be configured to use credentials that are contained in a Secrets Manager secret.\\n\\nFollow the instructions in the \\nREADME for the Aurora Serverless application \\nto use the AWS Cloud Development Kit (AWS CDK) or AWS Command Line Interface\\n(AWS CLI) to create and manage the resources.\\n\\nCreate the work items table\\n\\nAfter you have created the Aurora DB cluster and database, you must create a table to\\ncontain work items. You can do this by using either the AWS Command Line Interface \\n(AWS CLI) or the AWS Management Console.\\n\\nAWS CLI\\n\\nUse the AWS CLI to create the work_items table by running the following command at a \\ncommand prompt. Before you run, replace the following values with the output from the \\nCloudFormation setup script:\\n\\nCLUSTER_ARN — Replace with the ARN of the Aurora DB cluster, such as \\narn:aws:rds:us-west-2:123456789012:cluster:doc-example-aurora-app-docexampleauroraappcluster-15xfvaEXAMPLE.\\n\\nSECRET_ARN — Replace with the ARN of the secret that contains your database\\ncredentials, such as arn:aws:secretsmanager:us-west-2:123456789012:secret:docexampleauroraappsecret8B-xI1R8EXAMPLE-hfDaaj.\\n\\nDATABASE — Replace with the name of the database, such as auroraappdb.\\n\\nTip: The caret \\\\ is the line continuation character for a Linux or Mac command prompt.\\nIf you run this command on another platform, replace the backslash with the line continuation\\ncharacter for that platform.\\n\\naws rds-data execute-statement \\\\\\n    --resource-arn \"CLUSTER_ARN\" \\\\\\n    --database \"DATABASE\" \\\\\\n    --secret-arn \"SECRET_ARN\" \\\\\\n    --sql \"create table work_items (iditem SERIAL PRIMARY KEY, description TEXT, guide VARCHAR(45), status TEXT, username VARCHAR(45), archived BOOL DEFAULT false);\"\\n\\nAWS Management Console\\n\\nUse the Console to create the work_items table with the following steps:\\n\\nBrowse to the Amazon RDS console.\\n\\nSelect Query Editor.\\n\\nFor Database instance or cluster, choose your database instance. If you used the \\nCloudFormation script to create your AWS resources, the name begins with \\ndoc-example-aurora-app-.\\n\\nFor Database username, choose Connect with a Secrets Manager ARN.\\n\\nEnter the ARN of the secret that contains your database credentials, such as \\narn:aws:secretsmanager:us-west-2:123456789012:secret:docexampleauroraappsecret8B-xI1R8EXAMPLE-hfDaaj.\\n\\nFor Enter the name of the database or schema, enter the name of your database, such\\nas auroraappdb.\\n\\nSelect Connect to database.\\n\\nThis opens a SQL query console. You can run any SQL queries here that you want. Run the \\nfollowing to create the work_items table:\\n\\nFor a PostgreSQL-compatible database:\\n```sql\\ncreate table work_items ( \\n  iditem SERIAL PRIMARY KEY, \\n  description TEXT, \\n  guide VARCHAR(45), \\n  status TEXT, \\n  username VARCHAR(45), \\n  archived BOOL DEFAULT false\\n);\\n\\nFor a MySQL-compatible database:sql\\ncreate table work_items ( \\n  iditem INT AUTO_INCREMENT PRIMARY KEY, \\n  description TEXT, \\n  guide VARCHAR(45), \\n  status TEXT, \\n  username VARCHAR(45), \\n  archived BOOL DEFAULT 0\\n);\\n```\\n\\nVerified email address\\n\\nTo email reports from the app, you must register at least one email address with \\nAmazon SES. This verified email is specified as the sender for emailed reports.\\n\\nIn a browser, navigate to the Amazon SES console.\\n\\nIf necessary, select your AWS Region.\\n\\nSelect Verified identities.\\n\\nSelect Create identity.\\n\\nSelect Email address.\\n\\nEnter an email address you own.\\n\\nSelect Create identity.\\n\\nYou will receive an email from Amazon Web Services that contains instructions on how\\nto verify the email with Amazon SES. Follow the instructions in the email to complete\\nverification.\\n\\nTip: For this example, you can use the same email account for both the sender and \\nthe recipient.\\n\\nRun the example\\n\\nREST service\\n\\nConfigure the service\\n\\nBefore you run the service, enter your AWS resource values and verified email address\\nin config.py, similar to the following:\\n\\nCLUSTER_ARN — Replace with the ARN of the Aurora DB cluster, such as \\narn:aws:rds:us-west-2:123456789012:cluster:doc-example-aurora-app-docexampleauroraappcluster-15xfvaEXAMPLE.\\n\\nSECRET_ARN — Replace with the ARN of the secret that contains your database\\ncredentials, such as arn:aws:secretsmanager:us-west-2:123456789012:secret:docexampleauroraappsecret8B-xI1R8EXAMPLE-hfDaaj.\\n\\nDATABASE — Replace with the name of the database, such as auroraappdb.\\n\\nTABLE_NAME — Replace with the name of the work item table, such as work_items.\\n\\nSENDER_EMAIL — Replace with an email address that is registered with Amazon SES.\\n\\nRun the service\\n\\nThis example uses Flask to host a local \\nweb server and REST service. With the web server running, you can send HTTP requests to\\nthe service endpoint to list, add, and update work items and to send email reports.\\n\\nRun the app at a command prompt to start the Flask web server. Specify the --debug\\nflag for more detailed output during development, and specify a port of 8080 to work\\nwith the Elwing client.\\n\\nflask --debug run -p 8080\\n\\nWebpage\\n\\nThe REST service is designed to work with the item tracker plugin in the Elwing web\\nclient. The item tracker plugin is a JavaScript application that lets you manage work \\nitems, send requests to the REST service, and see the results.\\n\\nRun Elwing and select the item tracker\\n\\nRun Elwing by following the instructions in the Elwing README.\\n\\nWhen Elwing starts, a web browser opens and browses to http://localhost:3000/.\\n\\nRun the item tracker plugin by selecting Item Tracker in the left navigation bar.\\n\\nThis sends a request to the REST service to get any existing active items:\\n   GET http://localhost:8080/api/items?archived=false\\n\\nAt first, the table is empty.\\n\\nSelect Add item, fill in the values, and select Add to add an item.\\n\\nThis sends a POST request to the REST service with a JSON payload that contains the\\n   work item.\\n\\nPOST http://localhost:8080/api/items\\n   {\"name\":\"Me\",\\n    \"guide\":\"python\",\\n    \"description\":\"Show how to add an item\",\\n    \"status\":\"In progress\",\\n    \"archived\":false}\\n\\nAfter you add items, they\\'re displayed in the table. You can archive an active \\n   item by selecting the Archive button next to the item.\\n\\nThis sends a PUT request to the REST service, specifying the item ID and the\\n   archive action.\\n\\nPUT http://localhost:8080/api/items/8db8aaa4-6f04-4467-bd60-EXAMPLEGUID:archive\\n\\nSelect a filter in the dropdown list, such as Archived, to get and display\\nonly items with the specified status.\\n\\nThis sends a GET request to the REST service with an archived query parameter.\\n\\nGET http://localhost:8080/api/items?archived=true\\n\\nEnter an email recipient and select Send report to send an email of active items.\\n\\nThis sends a POST request to the REST service with a report action.\\n\\nPOST http://localhost:8080/api/items:report\\n\\nWhen your Amazon SES account is in the sandbox, both the sender and recipient\\n   email addresses must be registered with Amazon SES.\\n\\nUnderstand the example\\n\\nThis example uses the Flask web framework to host a local REST service and respond to\\nHTTP requests.\\n\\nRouting\\n\\nThe app.py file configures the app, creates Boto3 resources, and sets up \\nURL routing. This example uses Flask\\'s MethodView class to help with routing.\\n\\nIn this file, you can find route definitions like the following, which routes a GET\\nrequest to /api/items to the ItemList.get method:\\n\\npython\\nitem_list_view = ItemList.as_view(\\'item_list_api\\', storage)\\napp.add_url_rule(\\n    \\'/api/items\\', defaults={\\'iditem\\': None}, view_func=item_list_view, methods=[\\'GET\\'],\\n    strict_slashes=False)\\n\\nREST methods\\n\\nHTTP requests are routed to methods in the ItemList and \\nReport classes, which use webargs and marshmallow to handle argument \\nparsing and data transformation.\\n\\nFor example, the work item schema includes a field that is named id in the webpage,\\nbut is named iditem in the data table. By defining a data_key, the marshmallow \\nschema transforms this field automatically.\\n\\npython\\nclass WorkItemSchema(Schema):\\n    iditem = fields.Str(data_key=\\'id\\')\\n\\nThe ItemList class contains methods that handle REST requests and use the\\n@use_args and @use_kwargs decorators from webargs to parse incoming arguments.\\n\\nFor example, the get method uses @use_kwargs to parse fields contained in the query \\nstring into arguments in the method signature, and then calls the underlying storage\\nobject to get work items from the DynamoDB table.\\n\\npython\\n@use_kwargs(WorkItemSchema, location=\\'query\\')\\ndef get(self, iditem, archived=None):\\n    work_items = self.storage.get_work_items(archived)\\n\\nAurora Serverless MySQL storage\\n\\nThe storage.py file contains functions that get and set data in an\\nAurora Serverless MySQL database by using a Boto3 Amazon Relational Database \\nService (Amazon RDS) Data Service object. This object wraps low-level Amazon RDS Data \\nService actions.\\n\\nFor example, the get_work_items function constructs a SELECT statement and parameters \\nand sends them to the data client to get work items with a specified archived status:\\n\\n```python\\ndef get_work_items(self, archived=None):\\n    if archived is not None:\\n        sql_where = \"WHERE archived=:archived\"\\n        sql_params = [{\\'name\\': \\'archived\\', \\'value\\': {\\'booleanValue\\': archived}}]\\n    sql = f\"SELECT iditem, description, guide, status, username, archived FROM {self._table_name} {sql_where}\"\\n    results = self._run_statement(sql, sql_params=sql_params)\\n\\ndef _run_statement(self, sql, sql_params=None):\\n    run_args = {\\n        \\'database\\': self._db_name,\\n        \\'resourceArn\\': self._cluster,\\n        \\'secretArn\\': self._secret,\\n        \\'sql\\': sql\\n    }\\n    if sql_params is not None:\\n        run_args[\\'parameters\\'] = sql_params\\n    results = self._rdsdata_client.execute_statement(**run_args)\\n```\\n\\nAmazon SES report\\n\\nThe report.py file contains functions that send an email report of work \\nitems to a specified email address.\\n\\nWhen 10 or fewer work items are included in the report, the work item list is included \\ndirectly in the email body, with both HTML and text versions. This style of report is\\nsent by using the send_email service action, which lets you send the HTML and text\\nbody as plain Python strings.\\n\\nWhen the list is larger than 10 work items, it is rendered in CSV format and included \\nas an attachment to the email. When you use Amazon SES to send an attachment, you must \\nuse the send_raw_email service action and send the email in MIME format.\\n\\nDelete the resources\\n\\nTo avoid charges, delete all the resources that you created for this tutorial.\\n\\nIf you created the example resources by using the AWS CDK or AWS CLI,\\nyou can destroy the resources by following the instructions in the \\nREADME for the Aurora Serverless application.\\n\\nIf you created your resources through the AWS Management Console, or modified them by \\nrunning the app, you must use the console to delete them.\\n\\nNext steps\\n\\nCongratulations! You have built a REST service that reads, writes, and archives \\nwork items that are stored in an Aurora Serverless database, and that uses \\nAmazon SES to send email to a registered user.\\n\\nAdditional information\\n\\nAmazon Aurora User Guide\\n\\nAmazon RDS User Guide\\n\\nAmazon SES Developer Guide\\n\\nAmazon RDS Data Service Boto3 API Reference\\n\\nAmazon SES Boto3 API Reference\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0', metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\aurora_item_tracker.md'}),\n",
       " Document(page_content=\"Amazon Aurora serverless REST API lending library example\\n\\nPurpose\\n\\nShows how to use the AWS SDK for Python (Boto3) with the Amazon Relational Database \\nService (Amazon RDS) API and AWS Chalice to create a REST API backed by an \\nAmazon Aurora database. The web service is fully serverless and represents\\na simple lending library where patrons can borrow and return books. Learn how to:\\n\\nCreate and manage a serverless Amazon Aurora database. This example uses Aurora Serverless v2.\\n\\nUse AWS Secrets Manager to manage database credentials.\\n\\nImplement a data storage layer that uses Amazon RDS Data Service to move data into\\nand out of the database.\\n\\nUse AWS Chalice to deploy a serverless REST API to Amazon API Gateway and AWS Lambda.\\n\\nUse the Requests package to send requests to the web service.\\n\\nPrerequisites\\n\\nYou must have an AWS account, and have your default credentials and AWS Region\\n  configured as described in the AWS Tools and SDKs Shared Configuration and\\n  Credentials Reference Guide.\\n\\nPython 3.7 or later\\n\\nCautions\\n\\nAs an AWS best practice, grant this code least privilege, or only the \\n  permissions required to perform a task. For more information, see \\n  Grant Least Privilege \\n  in the AWS Identity and Access Management \\n  User Guide.\\n\\nThis code has not been tested in all AWS Regions. Some AWS services are \\n  available only in specific Regions. For more information, see the \\n  AWS Region Table\\n  on the AWS website.\\n\\nRunning this code might result in charges to your AWS account.\\n\\nRunning the code\\n\\nInstall prerequisites by running the following at a command prompt.\\n\\npython -m pip install -r requirements.txt\\n\\nThis example contains two deployment steps, a REST demo, and a cleanup step that\\nall must be run separately.\\n\\nFill the database with example data pulled from the Internet Archive's Open Library.\\n\\npython library_demo.py deploy_database\\n\\nThe database is now ready and can be accessed through the \\nAWS Console Query Editor \\nor the Boto3 rds-data client. Or, run the next step to deploy the REST API.\\n\\n3. REST API deployment\\n\\nUses Chalice and AWS CLI commands to deploy routing and data-handling layers \\nto AWS Lambda, set up API Gateway to handle HTTP requests, and\\nestablish AWS Identity and Access Management (IAM) roles and profiles to manage\\npermissions.\\n\\nDeploy the REST API by running the following command at a command \\nprompt.\\n\\npython library_demo.py deploy_rest\\n\\nThe REST API is now deployed and can received HTTP requests. Try it yourself \\nusing your favorite HTTP client or run the next step to see a demonstration\\nof how to use the Requests package to call the web service.\\n\\n4. REST API demonstration\\n\\nUses the Requests package to send a series of HTTP requests to the web service to\\nperform the following tasks.\\n\\nList the books in the library.\\n\\nAdd a library patron.\\n\\nLend a book to the new patron.\\n\\nReturn the book from the patron.\\n\\nSee the demo by running the following command at a command prompt.\\n\\npython library_demo.py demo_rest\\n\\n5. Cleanup\\n\\nRemove all resources created during the demonstration by running the following \\ncommand at a command prompt.\\n\\ncdk destroy\\n\\nBe sure to run cleanup after you're done to avoid additional charges to your \\naccount.\\n\\nExample structure\\n\\nThe example is subdivided into two main sections.\\n\\nThe library_demo.py script and rds_tools folder are used to deploy resources \\nand manage the demo.\\n\\nThe library_api folder contains the REST API code and resource definitions that \\nare deployed to AWS by Chalice.\\n\\nThe example contains the following files.\\n\\nlibrary_demo.py\\n\\nDeploys database and REST API resources, fills the database with example books,\\nruns a REST request demonstration, and cleans up resources.\\n\\nrds_tools/aurora_tools.py\\n\\nWraps parts of the Boto3 RDS and Secrets Manager API to show how to create database\\nclusters and secrets.\\n\\nlibrary_api/app.py\\n\\nContains REST API routes that receive HTTP requests. This file and supporting \\nchalicelib files are deployed to AWS Lambda as part of the Chalice deployment.\\n\\nlibrary_api/requirements.txt\\n\\nLists packages that are required in the AWS Lambda environment.\\n\\nlibrary_api/resources.json\\n\\nDefines an IAM role and policy to grant AWS Lambda permission to perform specific\\nactions on RDS, Secrets Manager, RDS Data Service, and Amazon CloudWatch Logs.\\n\\n.chalice/config.json\\n\\nDefines environment variables that define the cluster, secret, and database used\\nby the data-handling layer.\\n\\nchalicelib/library_data.py\\n\\nHandles calls from the REST routing layer. Runs MySQL statements through RDS\\nData Service to move data into and out of the database.\\n\\nchalicelib/mysql_helper.py\\n\\nA simplified object-relational mapping (ORM) layer that translates between Python \\nstructures and SQL statements.\\n\\nRunning the tests\\n\\nThe unit tests in this module use the botocore Stubber. This captures requests before \\nthey are sent to AWS, and returns a mocked response. To run all of the tests, \\nrun the following in your \\n[GitHub root]/python/example_code/rds/lending_library/rds_tools and\\n[GitHub root]/python/example_code/rds/lending_library/library_api\\nfolders.\\n\\npython -m pytest\\n\\nThe tests in the test_library_api_app.py script use the chalice.test.Client\\nobject to help with route testing. For details, see \\nChalice Testing.\\n\\nAdditional information\\n\\nBoto3 Amazon Relation Database Service service reference\\n\\nBoto3 Amazon RDS Data Service service reference\\n\\nBoto3 AWS Secrets Manager service reference\\n\\nAWS Chalice on GitHub\\n\\nAmazon Aurora User Guide\\n\\nAmazon RDS Data Service API Reference\\n\\nAWS Secrets Manager User Guide\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0\", metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\aurora_rest_lending_library.md'}),\n",
       " Document(page_content='AWS SDK for Python cross-service examples\\n\\nOverview\\n\\nThis README lists the cross-service examples available for the AWS SDK for \\nPython (Boto3). Each folder in this directory contains the following cross-service \\nexamples. A README in each folder describes how to run the example.\\n\\nA cross-service example is an application that works across multiple AWS services \\nusing the AWS SDK for Python.\\n\\n⚠️ Important\\n\\nRunning this code might result in charges to your AWS account.\\n\\nRunning the tests might result in charges to your AWS account.\\n\\nWe recommend that you grant your code least privilege. At most, grant only the minimum permissions required to perform the task. For more information, see Grant least privilege.\\n\\nThis code is not tested in every AWS Region. For more information, see AWS Regional Services.\\n\\nCross-service examples\\n\\nAWS Chalice and AWS Lambda REST API example\\nShows how to use AWS Chalice to create a serverless REST API that uses \\nAmazon API Gateway, AWS Lambda, and Amazon DynamoDB. The REST API simulates a \\nsystem that tracks daily cases of COVID-19 in the United States, using fictional \\ndata.\\n\\nAmazon API Gateway\\nAWS CloudFormation\\nAmazon DynamoDB\\nAWS Lambda\\n\\nAmazon API Gateway websocket chat example\\nShows how to use Amazon API Gateway V2 to create a websocket API that integrates \\nwith AWS Lambda and Amazon DynamoDB.\\n\\nAmazon API Gateway\\nAmazon DynamoDB\\nAWS Lambda\\n\\nTrack work items in an Aurora Serverless database\\nShows how to create a REST service that lets you store work items in an \\nAmazon Aurora Serverless database and use Amazon Simple Email Service (Amazon SES) \\nto send email reports of work items.\\n\\nAurora\\nAmazon SES\\n\\nAmazon Aurora Serverless REST API lending library example\\nShows how to use the Amazon Relational Database Service (Amazon RDS) API and \\nAWS Chalice to create a REST API backed by an Amazon Aurora database. The web \\nservice is fully serverless and represents a simple lending library where patrons \\ncan borrow and return books.\\n\\nAmazon API Gateway\\nAWS Lambda\\nAmazon RDS\\nAWS Secrets Manager\\n\\nTrack work items in a DynamoDB table\\nShows how to create a REST service that lets you store work items in an \\nAmazon DynamoDB table and use Amazon Simple Email Service (Amazon SES) \\nto send email reports of work items.\\n\\nAmazon DynamoDB\\nAmazon SES\\n\\nAnalyzing photos using Amazon Rekognition\\nShows you how to create a web application that lets you upload photos to an \\nAmazon Simple Storage Service (Amazon S3) bucket, use Amazon Rekognition to analyze \\nand label the photos, and use Amazon Simple Email Service (Amazon SES) to send \\nemail reports of image analysis.  \\n\\nAmazon Rekognition\\nAmazon S3\\nAmazon SES\\n\\nModerate content using Amazon Rekognition with URL support\\nShows you how to create a Lambda function that analyzes images with Amazon \\nRekognition to moderate content. The Lambda function is invoked by API Gateway\\nso that you can POST content to the API Gateway URL to receive moderation data.\\n\\nAmazon API Gateway\\nAWS CloudFormation\\nAWS Lambda\\nAmazon Rekognition\\n\\nAWS Step Functions messenger example\\nShows how to use AWS Step Functions to create and run a state machine that \\nretrieves message records from Amazon DynamoDB and sends messages to an \\nAmazon Simple Queue Service (Amazon SQS) queue.\\n\\nAWS CloudFormation\\nAmazon DynamoDB\\nAWS Lambda\\nAmazon SQS\\nAWS Step Functions\\n\\nDetect entities in extracted text using a Jupyter notebook\\nShows how to use a Jupyter notebook to detect entities in text that is extracted \\nfrom an image. This example uses Amazon Textract to extract text from an image \\nstored in Amazon Simple Storage Service (Amazon S3) and Amazon Comprehend to detect \\nentities in the extracted text.\\n\\nAmazon Comprehend\\nAmazon S3\\nAmazon Textract\\n\\nAmazon Textract explorer example\\nShows how to use Amazon Textract to detect text, form, and table elements in a \\ndocument image. The input image and Textract output are shown in a Tkinter \\napplication that lets you explore the detected elements. The application starts\\nasynchronous jobs, publishes notifications to an Amazon Simple Notification \\nService (Amazon SNS) topic when the job completes, and polls an Amazon Simple \\nQueue Service (Amazon SQS) queue for a job completion message and displays the \\nresults. \\n\\nAmazon S3\\nAmazon SNS\\nAmazon SQS    \\nAmazon Textract\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0', metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\cross_service.md'}),\n",
       " Document(page_content='Track work items in a DynamoDB table with the SDK for Python\\n\\nOverview\\n\\nThis example shows you how to use the AWS SDK for Python (Boto3) to create a REST \\nservice that lets you do the following:\\n\\nBuild a Flask REST service that integrates with AWS services.\\n\\nRead, write, and update work items that are stored in an Amazon DynamoDB table.\\n\\nUse Amazon Simple Email Service (Amazon SES) to send email reports of work items.\\n\\nThe REST service is used in conjunction with the Elwing React client\\nto present a fully functional web application.\\n\\n⚠️ Important\\n\\nRunning this code might result in charges to your AWS account.\\n\\nRunning the tests might result in charges to your AWS account.\\n\\nWe recommend that you grant your code least privilege. At most, grant only the minimum \\n  permissions required to perform the task. For more information, see \\n  Grant least privilege.\\n\\nThis code is not tested in every AWS Region. For more information, see \\n  AWS Regional Services.\\n\\nPrerequisites\\n\\nPrerequisites for running examples can be found in the \\nREADME in the Python folder.\\n\\nIn addition to the standard prerequisites, this example also requires:\\n\\nFlask 2.2.0 or later\\n\\nFlask-Cors 3.0.10 or later\\n\\nwebargs 8.2.0 or later\\n\\nYou can install all the prerequisites by running the following in a virtual environment:\\n\\npython -m pip install -r requirements.txt\\n\\nCreate the resources\\n\\nWork item table\\n\\nThis example requires a DynamoDB table that has a String partition key named iditem.\\n\\nAWS CDK and AWS CLI deployment\\n\\nFollow the instructions in the \\nREADME for the DynamoDB item tracker \\nto use the AWS Cloud Development Kit (AWS CDK) or AWS Command Line Interface\\n(AWS CLI) to create and manage the table resource.\\n\\nConsole deployment\\n\\nIf you prefer, you can deploy the resources for the application by using the \\nAWS Management Console.\\nTo deploy with the console, take the following steps:\\n\\nOpen the console in your browser.\\n\\nNavigate to DynamoDB and create a table named doc-example-work-item-tracker\\nwith a partition key of String type named iditem.\\n\\nVerified email address\\n\\nTo email reports from the app, you must register at least one email address with \\nAmazon SES. This verified email is specified as the sender for emailed reports.\\n\\nIn a browser, navigate to the Amazon SES console.\\n\\nIf necessary, select your AWS Region.\\n\\nSelect Verified identities.\\n\\nSelect Create identity.\\n\\nSelect Email address.\\n\\nEnter an email address you own.\\n\\nSelect Create identity.\\n\\nYou will receive an email from Amazon Web Services that contains instructions on how\\nto verify the email with Amazon SES. Follow the instructions in the email to complete\\nverification.\\n\\nTip: For this example, you can use the same email account for both the sender and \\nthe recipient.\\n\\nRun the example\\n\\nREST service\\n\\nConfigure the service\\n\\nBefore you run the service, enter your DynamoDB table name and verified email address\\nin config.py, similar to the following:\\n\\nTABLE_NAME = \\'doc-example-work-item-tracker\\'\\nSENDER_EMAIL = \\'your_name@example.com\\'\\n\\nRun the service\\n\\nThis example uses Flask to host a local \\nweb server and REST service. With the web server running, you can send HTTP requests to\\nthe service endpoint to list, add, and update work items and to send email reports.\\n\\nRun the app at a command prompt to start the Flask web server. Specify the --debug\\nflag for more detailed output during development, and specify a port of 8080 to work\\nwith the Elwing client.\\n\\nflask --debug run -p 8080\\n\\nWebpage\\n\\nThe REST service is designed to work with the item tracker plugin in the Elwing web\\nclient. The item tracker plugin is a JavaScript application that lets you manage work \\nitems, send requests to the REST service, and see the results.\\n\\nRun Elwing and select the item tracker\\n\\nRun Elwing by following the instructions in the Elwing README.\\n\\nWhen Elwing starts, a web browser opens and browses to http://localhost:3000/.\\n\\nRun the item tracker plugin by selecting Item Tracker in the left navigation bar.\\n\\nThis sends a request to the REST service to get any existing active items:\\n   GET http://localhost:8080/api/items?archived=false\\n\\nAt first, the table is empty.\\n\\nSelect Add item, fill in the values, and select Add to add an item.\\n\\nThis sends a POST request to the REST service with a JSON payload that contains the\\n   work item.\\n\\nPOST http://localhost:8080/api/items\\n   {\"name\":\"Me\",\\n    \"guide\":\"python\",\\n    \"description\":\"Show how to add an item\",\\n    \"status\":\"In progress\",\\n    \"archived\":false}\\n\\nAfter you\\'ve added items, they\\'re displayed in the table. You can archive an active \\n   item by selecting the Archive button next to the item.\\n\\nThis sends a PUT request to the REST service, specifying the item ID and the\\n   archive action.\\n\\nPUT http://localhost:8080/api/items/8db8aaa4-6f04-4467-bd60-EXAMPLEGUID:archive\\n\\nSelect a filter in the dropdown list, such as Archived, to get and display\\nonly items with the specified status.\\n\\nThis sends a GET request to the REST service with an archived query parameter.\\n\\nGET http://localhost:8080/api/items?archived=true\\n\\nEnter an email recipient and select Send report to send an email of active items.\\n\\nThis sends a POST request to the REST service with a report action.\\n\\nPOST http://localhost:8080/api/items:report\\n\\nWhen your Amazon SES account is in the sandbox, both the sender and recipient\\n   email addresses must be registered with Amazon SES.\\n\\nUnderstand the example\\n\\nThis example uses the Flask web framework to host a local REST service and respond to\\nHTTP requests.\\n\\nRouting\\n\\nThe app.py file configures the app, creates Boto3 resources, and sets up \\nURL routing. This example uses Flask\\'s MethodView class to help with routing.\\n\\nIn this file, you can find route definitions like the following, which routes a GET\\nrequest to /api/items to the ItemList.get method:\\n\\npython\\nitem_list_view = ItemList.as_view(\\'item_list_api\\', storage)\\napp.add_url_rule(\\n    \\'/api/items\\', defaults={\\'iditem\\': None}, view_func=item_list_view, methods=[\\'GET\\'],\\n    strict_slashes=False)\\n\\nREST methods\\n\\nHTTP requests are routed to methods in the ItemList and \\nReport classes, which use webargs and marshmallow to handle argument \\nparsing and data transformation.\\n\\nFor example, the work item schema includes a field that is named id in the webpage,\\nbut is named iditem in the data table. By defining a data_key, the marshmallow \\nschema transforms this field automatically.\\n\\npython\\nclass WorkItemSchema(Schema):\\n    iditem = fields.Str(data_key=\\'id\\')\\n\\nThe ItemList class contains methods that handle REST requests and use the\\n@use_args and @use_kwargs decorators from webargs to parse incoming arguments.\\n\\nFor example, the get method uses @use_kwargs to parse fields contained in the query \\nstring into arguments in the method signature, and then calls the underlying storage\\nobject to get work items from the DynamoDB table.\\n\\npython\\n@use_kwargs(WorkItemSchema, location=\\'query\\')\\ndef get(self, iditem, archived=None):\\n    work_items = self.storage.get_work_items(archived)\\n\\nDynamoDB storage\\n\\nThe storage.py file contains functions that get and set data in DynamoDB\\nby using a Boto3 Table object. This object is a high-level object that wraps low-level\\nDynamoDB service actions.\\n\\nFor example, the get_work_items function scans the table for work items with a \\nspecified archived status:\\n\\npython\\ndef get_work_items(self, archived=None):\\n    work_items = self.table.scan(\\n        FilterExpression=Attr(\\'archived\\').eq(archived)).get(\\'Items\\', [])\\n\\nAmazon SES report\\n\\nThe report.py file contains functions that send an email report of work \\nitems to a specified email address.\\n\\nWhen 10 or fewer work items are included in the report, the work item list is included \\ndirectly in the email body, with both HTML and text versions. This style of report is\\nsent by using the send_email service action, which lets you send the HTML and text\\nbody as plain Python strings.\\n\\nWhen the list is larger than 10 work items, it is rendered in CSV format and included \\nas an attachment to the email. When you use Amazon SES to send an attachment, you must \\nuse the send_raw_email service action and send the email in MIME format.\\n\\nDelete the resources\\n\\nTo avoid charges, delete all the resources that you created for this tutorial.\\n\\nIf you created the example resources by using the AWS CDK or AWS CLI,\\nyou can destroy the resources by following the instructions in the \\nREADME for the DynamoDB item tracker.\\n\\nIf you created your resources through the AWS Management Console, or modified them by \\nrunning the app, you must use the console to delete them.\\n\\nNext steps\\n\\nCongratulations! You have built a REST service that reads, writes, and archives \\nwork items that are stored in an Amazon DynamoDB table. It also uses \\nAmazon SES to send email to a registered user.\\n\\nAdditional information\\n\\nDynamoDB Developer Guide\\n\\nAmazon SES Developer Guide\\n\\nBoto3 DynamoDB service reference\\n\\nAmazon SES Boto3 API Reference\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0', metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\dynamodb_item_tracker.md'}),\n",
       " Document(page_content=\"Analyzing photos using Amazon Rekognition with the AWS SDK for Python\\n\\nOverview\\n\\nShows you how to use the AWS SDK for Python (Boto3) to create a web application that\\nlets you do the following:\\n\\nUpload photos to an Amazon Simple Storage Service (Amazon S3) bucket.\\n\\nUse Amazon Rekognition to analyze and label the photos.\\n\\nUse Amazon Simple Email Service (Amazon SES) to send email reports of image analysis.\\n\\nThis example contains two main components: a webpage written in JavaScript that is built\\nwith React, and a REST service written in Python that is built with Flask-RESTful.\\n\\nYou can use the React webpage to:\\n\\nDisplay a list of images that are stored in your S3 bucket.\\n\\nUpload images from your computer to your S3 bucket.\\n\\nDisplay images and labels that identify items that are detected in the image.\\n\\nGet a report of all images in your S3 bucket and send an email of the report.\\n\\nThe webpage calls the REST service. The service sends requests to AWS to\\nperform the following actions:\\n\\nGet and filter the list of images in your S3 bucket.\\n\\nUpload photos to your S3 bucket.\\n\\nUse Amazon Rekognition to analyze individual photos and get a list of labels that\\n  identify items that are detected in the photo.\\n\\nAnalyze all photos in your S3 bucket and use Amazon SES to email a report.\\n\\n⚠️ Important\\n\\nRunning this code might result in charges to your AWS account.\\n\\nRunning the tests might result in charges to your AWS account.\\n\\nWe recommend that you grant your code least privilege. At most, grant only the minimum permissions required to perform the task. For more information, see Grant least privilege.\\n\\nThis code is not tested in every AWS Region. For more information, see AWS Regional Services.\\n\\nPrerequisites\\n\\nWebpage\\n\\nThe requirements for the webpage are listed in the accompanying package.json file.\\nYou will need recent versions of Node.js and npm to install the requirements.\\n\\nNode.js 16.14.2\\n\\nnpm 8.5.0\\n\\nInstall the webpage requirements by running the following in the frontend folder\\nat a command prompt:\\n\\nnpm install\\n\\nThis installs components like React, React-Bootstrap, and jQuery.\\n\\nREST service\\n\\nThe requirements for the REST service are listed in the accompanying requirements.txt\\nfile. You will need recent versions of Python and pip to the install the requirements.\\n\\nPython 3.8.8\\n\\npip 21.1.2\\n\\nInstall the REST service components by running the following in the api folder in \\na virtual environment:\\n\\npython -m pip install -r requirements.txt\\n\\nThis installs components like Boto3, Flask, and Flask-RESTful.\\n\\nCreating the resources\\n\\nPhoto bucket\\n\\nThe example requires an S3 bucket to store photos. You can create an S3 bucket by\\nusing the AWS Management Console.\\n\\nAfter you create your bucket, update the BUCKET_NAME field in api\\\\config.py with\\nthe name of your bucket.\\n\\nEmail addresses\\n\\nThe example sends an email report by using Amazon SES. When your account is in the\\nsandbox, you must register both the sender and recipient email addresses. You can\\ndo this by using the console.\\n\\nTip: For this example, you can register the same email account as both the sender and \\nthe recipient.\\n\\nRunning the code\\n\\nREST service\\n\\nStart the REST service by running the following at a command prompt in the api folder:\\n\\npython app.py\\n\\nThis starts the Flask web server in debug mode on http://localhost:5000.\\n\\nWebpage\\n\\nStart the webpage in development mode by running the following at a command prompt\\nin the frontend folder:\\n\\nnpm start\\n\\nThis compiles the project and browses to it at http://localhost:3000.\\n\\nUsing the app\\n\\nBy default, the application shows that your bucket contains no images.\\n\\nSelect Browse, choose a JPG or PNG file, and select Upload to upload it to\\nyour S3 bucket.\\n\\nSelect the image in the list to display it along with labels detected by Amazon\\nRekognition. When a label includes bounding boxes, it is added to a list of clickable\\nitems. Select one to draw boxes over the image.\\n\\nSelect Analyze all photos in your bucket and send a report to use Amazon Rekognition\\nto analyze all photos in your bucket and display a report.\\n\\nFill out the form with sender address, recipient address, and a message. Select \\nSend report to email the report.\\n\\nDeleting the resources\\n\\nTo avoid charges, use the console to delete all the resources that you created for \\nthis tutorial.\\n\\nEmpty and delete your S3 bucket.\\n\\nDelete your email from Amazon SES.\\n\\nNext steps\\n\\nCongratulations! You have built a web application that uploads and lists photos in\\nan S3 bucket, uses Amazon Rekognition to analyze photos, and uses Amazon SES to send\\nemail to a registered user.\\n\\nAdditional resources\\n\\nCross-service examples\\n\\nAmazon Rekognition Developer Guide\\n\\nAmazon S3 User Guide\\n\\nAmazon SES Developer Guide'\\n\\nAmazon Rekognition API Reference\\n\\nAmazon S3 API Reference\\n\\nAmazon SES API Reference\\n\\nAmazon Rekognition Boto3 API Reference\\n\\nAmazon S3 Boto3 API Reference\\n\\nAmazon SES Boto3 API Reference\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0\", metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\photo_analyzer.md'}),\n",
       " Document(page_content='Amazon Rekognition Content Moderation Solution with URL Support\\n\\nToday, Amazon Rekognition doesn\\'t support URLs as input for its APIs. This solution provides a way to invoke Amazon Rekognition, using Amazon API Gateway and AWS Lambda.\\n\\nSetup Instructions\\n\\nUsing AWS CloudFormation, create a stack from cloudformation.yaml\\n\\nOnce it\\'s done running, copy the Invoke URL from the Outputs tab\\n\\nRun a POST request to that URL with a request body containing a \"url\" element with the image as its value.\\n\\nFor example:\\n{\\n  \"url\": \"\"\\n}\\n\\nLicense Summary\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0', metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\rekognition_content_moderation.md'}),\n",
       " Document(page_content=\"Build and manage a resilient service using the SDK for Python\\n\\nOverview\\n\\nThis example shows how to use the AWS SDK for Python (Boto3) to create a load-balanced\\nweb service that returns book, movie, and song recommendations. It shows\\nhow the service responds to failures, and how to restructure the service for\\nmore resilience when failures occur.\\n\\nSeveral components are used to demonstrate the resilience of the example web service:\\n\\nAmazon EC2 Auto Scaling \\n  is used to create \\n  Amazon Elastic Compute Cloud (Amazon EC2) \\n  instances based on a launch template and to keep the number of instances \\n  in a specified range.\\n\\nElastic Load Balancing \\n  handles HTTP requests, monitors the health of instances in the Auto Scaling group, and \\n  dispatches requests to healthy instances.\\n\\nA Python web server runs on each EC2 instance to handle HTTP requests. It responds\\n  with recommendations and health checks.\\n\\nAn Amazon DynamoDB \\n  table simulates a recommendation service that the web server depends on to get recommendations.\\n\\nA set of AWS Systems Manager \\n  parameters control web server response to requests and health checks to \\n  simulate failures and demonstrate resiliency.\\n\\nEach of these components is created and managed with the SDK for Python as part of\\nan interactive demo that runs at a command prompt.\\n\\nAmazon EC2 Auto Scaling and EC2 instances\\n\\nAn Auto Scaling group starts EC2 instances in a specified set of Availability Zones. \\nThis example uses an Auto Scaling group to keep the number of running instances \\nwithin a specified range and to make them available across multiple Availability Zones. The Auto Scaling group\\nis set as a load balancer target so that HTTP requests are handled by a single endpoint\\nand dispatched equally to the instances in the group.\\n\\nAn AWS Identity and Access Management (IAM) \\ninstance profile specifies permissions that are granted to EC2 instances created \\nduring the demo. When you associate an instance profile with an instance, AWS SDK code \\nthat runs on the instance can assume the profile's role to get the permissions that are specified\\nby the role's attached policies.\\n\\nAn Amazon EC2 launch template specifies how instances are created. This example creates\\na launch template that specifies the instance type, Amazon Machine Image (AMI), instance \\nprofile, and a Bash script that runs when the instance is started. The Bash\\nscript installs required Python packages and starts a demo Python web server that listens\\nfor HTTP requests on port 80. The Python web server uses the SDK for Python to get \\nrecommendation data from a DynamoDB table and to get parameter values from Systems \\nManager to control the flow of the demonstration.\\n\\nElastic Load Balancing\\n\\nElastic Load Balancing is used to distribute incoming HTTP traffic across multiple instances.\\nThis example creates an \\nApplication Load Balancer. \\nIt also adds a listener that forwards requests\\nfrom the load balancer endpoint to the EC2 instances that are managed by the Auto Scaling\\ngroup. The target group performs health checks on the instances and pulls unhealthy\\ninstances out of the rotation.\\n\\n⚠ Important\\n\\nRunning this code might result in charges to your AWS account.\\n\\nRunning the tests might result in charges to your AWS account.\\n\\nWe recommend that you grant your code least privilege. At most, grant only the minimum permissions required to perform the task. For more information, see Grant least privilege.\\n\\nThis code is not tested in every AWS Region. For more information, see AWS Regional Services.\\n\\nScenario\\n\\nPrerequisites\\n\\nAll the components used in this example need to access one another. Therefore, you must\\ncreate all components in the same virtual private cloud (VPC). A VPC is a logically isolated network provided by\\nAmazon Virtual Private Cloud (Amazon VPC).\\nYou can use the default VPC that's included with your account, or \\ncreate a new VPC.\\n\\nTo access the load balancer endpoint, you must allow inbound traffic\\non port 80 from your computer's IP address to your VPC. If this rule doesn't exist, the \\nexample tries to add it. Alternately, you can \\nadd a rule to the default security group for your VPC \\nand specify your computer's IP address\\nas a source.\\n\\nFor general prerequisites, see the README in the python folder.\\n\\nInstructions\\n\\nFor general instructions to run the examples, see the\\nREADME in the python folder.\\n\\nRun this example by running the following command in the folder that contains this README:\\n\\npython runner.py --action all\\n\\nThis starts an interactive scenario that walks you through several aspects of creating a \\nresilient web service and lets you send requests to the load balancer endpoint and verify\\ninstance health along the way.\\n\\nBuild and manage a resilient service\\n\\nYou can run the entire example by specifying --action all. Alternatively, run each of the sections \\nseparately by specifying actions of deploy, demo, or destroy.\\n\\nDeploy resources\\n\\nUse the SDK for Python to create the following AWS resources:\\n\\nA DynamoDB table that acts as a service that recommends books, movies, and songs.\\n\\nAn instance profile and an associated role and policy that grants permission to\\n   instances to access DynamoDB and Systems Manager.\\n\\nA launch template that specifies the instance profile and a startup script\\n   that starts a Python web server on each instance.\\n\\nAn Auto Scaling group that starts EC2 instances, one in each of three \\n   Availability Zones.\\n\\nAn Application Load Balancer that handles HTTP requests to a single endpoint.\\n\\nA target group that connects the load balancer to instances in the Auto Scaling group.\\n\\nA listener that is added to the load balancer and forwards requests to the target group.\\n\\nDemonstrate resiliency\\n\\nThis part of the example demonstrates resiliency by simulating several kinds of failures.\\nIt uses Systems Manager parameters to update how the web server responds to requests. It\\nalso uses health checks to show how to make your web server more resilient to failure.\\n\\nAlong with recommendations returned by the DynamoDB table, the web service includes the\\ninstance ID and Availability Zone so you can see how the load balancer distributes\\nrequests among the instances in the Auto Scaling group.\\n\\nThe scenario takes the following steps:\\n\\nInitial state: healthy — Sends requests to the endpoint to get recommendations and verify that instances \\n   are healthy.\\n\\nBroken dependency — Sets a parameter that specifies a nonexistent DynamoDB table name. This simulates a\\n   failure of the recommendation service. Requests for recommendations now return a failure\\n   code. All instances still report as healthy because they only implement shallow health checks. For this\\n   example, a shallow health check means the web server always reports itself as healthy as long as the\\n   load balancer can connect to it.\\n\\nStatic response — Updates a parameter that prompts the web server to return a static response when the\\n   recommendation service fails. Requests for recommendations now return a static response, \\n   which is a better customer experience.\\n\\nBad credentials — Sets the table name parameter so the recommendations service succeeds, but also\\n   updates one of the instances to use an instance profile that contains bad credentials.\\n   Now, when the load balancer selects the bad instance to serve a request, it returns\\n   a static response because it cannot access the recommendation service, but the other\\n   instances return real recommendations.\\n\\nDeep health checks — Sets a parameter that instructs the web server to perform a deep health check.\\n   For this example, a deep health check means that the web server reports itself as unhealthy if it can't \\n   access the recommendations service. The instance with bad credentials reports as unhealthy and the load \\n   balancer takes it out of rotation. Now, requests are forward only to healthy instances.\\n\\nReplace the failing instance — Terminates the unhealthy instance and lets Amazon EC2 Auto Scaling start \\n   a new instance in its place. During this process, the stopping and starting instances are unhealthy so they\\n   don't receive any requests, but the load balancer continues to forward requests to healthy\\n   instances. When the new instance is ready, it is added to the rotation and starts receiving\\n   requests.\\n\\nFail open — Sets the table name parameter so the recommendations service fails for all instances.\\n   Because all instances are using deep health checks, they all report as unhealthy. In this\\n   case, the load balancer continues to forward requests to all instances. This lets the\\n   system fail open and lets the instances return static responses, rather than fail closed\\n   and report failure.\\n\\nDestroy resources\\n\\nUse the SDK for Python to clean up all resources created for this example.\\n\\nDelete the load balancer and target group.\\n\\nStop all instances and delete the Auto Scaling group.\\n\\nDelete the launch template and instance profile.\\n\\nDelete the DynamoDB recommendations table.\\n\\nAdditional resources\\n\\nApplication Load Balancers user guide\\n\\nAmazon EC2 Auto Scaling user guide\\n\\nAmazon Elastic Compute Cloud (Amazon EC2) user guide\\n\\nSDK for Python Elastic Load Balancing v2 reference\\n\\nSDK for Python Amazon EC2 Auto Scaling reference\\n\\nSDK for Python Amazon EC2 reference\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0\", metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\resilient_service.md'}),\n",
       " Document(page_content='AWS Step Functions messenger example\\n\\nPurpose\\n\\nShows how to use the AWS SDK for Python (Boto3) with AWS Step Functions to\\ncreate and run state machines.\\n\\nCreate a state machine that retrieves and updates message records from an \\nAmazon DynamoDB table.\\n\\nUpdate the state machine definition to also send messages to Amazon Simple Queue Service \\n(Amazon SQS).\\n\\nStart and stop state machine runs.\\n\\nConnect to AWS Lambda, DynamoDB, and Amazon SQS from a state machine by using service\\nintegrations.\\n\\nPrerequisites\\n\\nYou must have an AWS account, and have your default credentials and AWS Region\\n  configured as described in the AWS Tools and SDKs Shared Configuration and\\n  Credentials Reference Guide.\\n\\nPython 3.7 or later\\n\\nBoto3 1.14.47 or later\\n\\nPyTest 5.3.5 or later (to run unit tests)\\n\\nCautions\\n\\nAs an AWS best practice, grant this code least privilege, or only the \\n  permissions required to perform a task. For more information, see \\n  Grant Least Privilege \\n  in the AWS Identity and Access Management \\n  User Guide.\\n\\nThis code has not been tested in all AWS Regions. Some AWS services are \\n  available only in specific Regions. For more information, see the \\n  AWS Region Table\\n  on the AWS website.\\n\\nRunning this code might result in charges to your AWS account.\\n\\nRunning the code\\n\\nDeploy resources\\n\\nDeploy prerequisite resources by running the example script with the deploy flag at \\na command prompt.\\n\\npython stepfunctions_demo.py deploy\\n\\nRun the usage demonstration\\n\\nRun the usage example with the demo flag at a command prompt.\\n\\npython stepfunctions_demo.py demo\\n\\nDestroy resources\\n\\nDestroy example resources by running the script with the destroy flag at a command \\nprompt.\\n\\npython stepfunctions_demo.py destroy\\n\\nExample structure\\n\\nThe example contains the following files.\\n\\nstate_definitions.py\\n\\nConstructs state definitions used for the demonstration by inserting resource \\nidentifiers that are retrieved from the CloudFormation stack and \\noptionally including a state that sends messages to Amazon SQS. Definitions are built \\nas Python dicts and must be transformed to JSON format before they are used in \\nStep Functions.\\n\\nstepfunctions_statemachine.py\\n\\nShows how to use AWS Step Functions state machine APIs.\\n\\nstepfunctions_demo.py\\n\\nShows how to create a Step Functions state machine that continuously reads message \\nrecords from an Amazon DynamoDB database and sends them to an Amazon Simple Queue \\nService (Amazon SQS) queue.\\n\\nCreates a state machine that calls a Lambda function to get messages and update\\nthem as sent.\\n\\nRuns the state machine and verifies that it updates items as expected in the \\nDynamoDB table.\\n\\nUpdates the state machine with a new definition that includes a state that sends\\nmessage to Amazon SQS.\\n\\nRuns the state machine again and verifies that it now sends messages to Amazon SQS.\\n\\nsetup.yaml\\n\\nContains a CloudFormation script that is used to create the resources needed for \\nthe demo. Pass the deploy or destroy flag to the stepfunctions_demo.py script to \\ncreate or remove these resources:\\n\\nA DynamoDB table.\\n\\nA Lambda function.\\n\\nAn Amazon SQS queue.\\n\\nAWS Identity and Access Management (IAM) roles.\\n\\nThe setup.yaml file was built from the \\nAWS Cloud Development Kit (AWS CDK) \\nsource script here: \\n/resources/cdk/python_example_code_stepfunctions_demo/setup.ts.\\n\\nRunning the tests\\n\\nThe unit tests in this module use the botocore Stubber. This captures requests before \\nthey are sent to AWS, and returns a mocked response. To run all of the tests, \\nrun the following in your [GitHub root]/python/example_code/stepfunctions \\nfolder.\\n\\npython -m pytest\\n\\nAdditional information\\n\\nBoto3 AWS Step Functions service reference\\n\\nAWS Step Functions documentation\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0', metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\stepfunctions_messenger.md'}),\n",
       " Document(page_content='Amazon Textract explorer example\\n\\nPurpose\\n\\nShows how to use the AWS SDK for Python (Boto3) with Amazon Textract to detect text, \\nform, and table elements in a document image. The input image and Textract output are\\nshown in a Tkinter application that lets you explore the detected elements.\\n\\nSubmit a document image to Textract and explore the output of detected elements.\\n\\nSubmit images directly to Textract or through an Amazon Simple Storage Service \\n(Amazon S3) bucket.\\n\\nUse asynchronous APIs to start a job that publishes a notification to an Amazon \\nSimple Notification Service (Amazon SNS) topic when the job completes.\\n\\nPoll an Amazon Simple Queue Service (Amazon SQS) queue for a job completion message\\nand display the results.\\n\\nPrerequisites\\n\\nYou must have an AWS account, and have your default credentials and AWS Region\\n  configured as described in the AWS Tools and SDKs Shared Configuration and\\n  Credentials Reference Guide.\\n\\nPython 3.8.8 or later\\n\\nBoto3 1.16.49 or later\\n\\nPillow 8.1.1 or later\\n\\nPyTest 6.0.2 or later (to run unit tests)\\n\\nCautions\\n\\nAs an AWS best practice, grant this code least privilege, or only the \\n  permissions required to perform a task. For more information, see \\n  Grant Least Privilege \\n  in the AWS Identity and Access Management \\n  User Guide.\\n\\nThis code has not been tested in all AWS Regions. Some AWS services are \\n  available only in specific Regions. For more information, see the \\n  AWS Region Table\\n  on the AWS website.\\n\\nRunning this code might result in charges to your AWS account.\\n\\nRunning the code\\n\\nThe asynchronous APIs used in this example require an Amazon S3 bucket to contain \\ninput images, an Amazon SNS topic to publish notifications, and an Amazon SQS queue\\nthat the application can poll for notification messages. These resources are managed by\\nan AWS CloudFormation stack that is defined in the accompanying setup.yaml file.\\n\\nDeploy resources\\n\\nDeploy prerequisite resources by running the example script with the deploy flag at \\na command prompt.\\n\\npython textract_demo_launcher.py deploy\\n\\nRun the usage demonstration\\n\\nRun the usage example with the demo flag at a command prompt.\\n\\npython textract_demo_launcher.py demo\\n\\nDestroy resources\\n\\nDestroy example resources by running the script with the destroy flag at a command \\nprompt.\\n\\npython textract_demo_launcher.py destroy\\n\\nExample structure\\n\\nThe example contains the following files.\\n\\ntextract_app.py\\n\\nA Tkinter application that displays document images, starts Textract synchronous and\\nasynchronous detection processes, and shows the hierarchy of detected elements.\\nElements can be clicked to explore the hierarchy and draw bounding polygons on the\\ninput image.\\n\\ntextract_demo_launcher.py\\n\\nLaunches the Textract demo.\\n\\nRun with the deploy option to deploy prerequisite\\nresources defined in the setup.yaml CloudFormation stack.\\n\\nRun with the demo option to show the Tkinter application.\\n\\nRun with the destroy option to destroy prerequisite resources.\\n\\ntextract_wrapper.py\\n\\nWraps Textract, Amazon S3, Amazon SNS, and Amazon SQS functions that are used by the \\napplication.\\n\\nsetup.yaml\\n\\nContains a CloudFormation script that is used to create the resources needed for \\nthe demo.\\n\\nAn Amazon S3 bucket that contains input document images.\\n\\nAn Amazon SNS topic that receives notification of job completion.\\n\\nAn IAM role that grants permission to publish to the topic.\\n\\nAn Amazon SQS queue that is subscribed to receive messages from the topic.\\n\\nThe setup.yaml file was built from the \\nAWS Cloud Development Kit (AWS CDK) \\nsource script here: \\n/resources/cdk/textract_example_s3_sns_sqs/setup.ts.\\n\\nRunning the tests\\n\\nThe unit tests in this module use the botocore Stubber. The Stubber captures requests \\nbefore they are sent to AWS, and returns a mocked response. To run all of the tests, \\nrun the following command in your \\n[GitHub root]/python/cross_service/textract_explorer folder.\\n\\npython -m pytest\\n\\nAdditional information\\n\\nBoto3 Amazon Textract reference\\n\\nAmazon Textract Documentation\\n\\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nSPDX-License-Identifier: Apache-2.0', metadata={'source': 'd:\\\\projects\\\\testbed\\\\ragas_cookbook\\\\sample_documents\\\\textract_explorer.md'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
