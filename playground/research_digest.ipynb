{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feedparser html2text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\testbed\\ragas_cookbook\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from agents.basic_agents import BaseAgent\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "\n",
    "from feedparser.util import FeedParserDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSDigestAgent(BaseAgent):\n",
    "    def find_useful_keys_in_dict(self, entry: FeedParserDict):\n",
    "        prompt = f\"\"\"This is an RSS feed entry. Determine which keys in the json will be useful in evaluating the entry for usefulness for a reader to skim. Feed entry: {entry}\"\"\"\n",
    "\n",
    "        return self.llm.invoke(prompt, config=self.langchain_config)\n",
    "    \n",
    "    def find_content_links(self, entry: FeedParserDict):\n",
    "        prompt = \"\"\"This is an RSS feed entry. Determine how to get the links to referenced content.\"\"\"\n",
    "        prompt += f'Feed entry: {entry}'\n",
    "        \n",
    "        return self.llm.invoke(prompt, config=self.langchain_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from feedparser.util import FeedParserDict\n",
    "\n",
    "import ssl\n",
    "if hasattr(ssl, '_create_unverified_context'):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Define the RSS feed URL\n",
    "feed_url = \"https://hnrss.org/newest?points=300&count=100\"\n",
    "\n",
    "# Parse the RSS feed\n",
    "feed: FeedParserDict = feedparser.parse(feed_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RSSDigestAgent(config='stdout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.find_useful_keys_in_dict(feed.entries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following keys in the JSON object are useful in evaluating the entry for usefulness for a reader to skim:\n",
      "\n",
      "* `title`: This gives a quick overview of the topic of the post.\n",
      "* `summary`: This provides a brief description of the post. In this case, it includes the article URL, comments URL, points, and number of comments.\n",
      "* `published`: This indicates when the post was published.\n",
      "* `authors` or `author`: These keys provide the name of the author(s), which can help the reader determine if they are interested in the post.\n",
      "* `comments`: This link can help the reader quickly access the comments section, if they want to see other readers' reactions or engage in discussion.\n",
      "\n",
      "The `links` key could also be useful if the reader wants to explore the topic further, as it provides an alternate link to the post. However, the `link` key already includes the direct link to the post, so `links` may not be necessary for skimming purposes.\n",
      "\n",
      "The `title_detail`, `summary_detail`, `published_parsed`, and `guidislink` keys are not directly useful for skimming, but they provide additional context or technical details that might be relevant in other contexts.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def strip_rss_entries(entries: List[FeedParserDict], useful_keys):\n",
    "    feed = []\n",
    "    for i in entries:\n",
    "        stripped_entry = {k:v for k,v in i.items() if k in useful_keys}\n",
    "        feed.append(stripped_entry)\n",
    "    return feed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_feed = strip_rss_entries(\n",
    "    feed.entries, ['title', 'summary', 'published', 'links', 'comments'])\n",
    "\n",
    "stripped_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "@tool\n",
    "def search_online(urls: str, content_limit:Optional[int]=4000):\n",
    "    \"\"\"Look up things online.\"\"\"\n",
    "    loader = AsyncHtmlLoader(urls)\n",
    "    docs = loader.load()\n",
    "\n",
    "    html2text = Html2TextTransformer()\n",
    "    docs = html2text.transform_documents(docs)\n",
    "    \n",
    "    if content_limit:\n",
    "        for i in docs:\n",
    "            if len(i.page_content) > content_limit:\n",
    "                i.page_content = i.page_content[:content_limit] + '...'\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.basic_agents import BaseAgent\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "class RSSTechAgent(BaseAgent):\n",
    "    def find_interesting_tech_from_feed(self, entry: FeedParserDict):\n",
    "        article = entry['links'][0].get('href')\n",
    "        if article:\n",
    "            try:\n",
    "                articles = search_online.invoke(article)\n",
    "                entry['linked_article'] = articles\n",
    "            except Exception as e:\n",
    "                print(f'unable to get article: {e}')\n",
    "                return\n",
    "                \n",
    "        prompt = \"\"\"Read an entry in an RSS feed and determine which if it contains useful new programming technologies or tools.\n",
    "Your response should only be about the intended output. Ignore anything irrelevant to the output. Your response must follow the format of the examples given.\n",
    "Examples:\n",
    "Two useful techs: {\n",
    "    \"technology\": [\n",
    "        {\n",
    "            \"name\": \"Kaniko\",\n",
    "            \"summary\": \"Kaniko is an open-source tool that allows you to build container images in Kubernetes without needing to run a Docker daemon inside the cluster.\",\n",
    "            \"link\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"needle-in-a-needlestack\",\n",
    "            \"summary\": \"Needle in a Needlestack is a new benchmark to measure how well LLMs pay attention to the information in their context window. NIAN creates a prompt that includes thousands of limericks and the prompt asks a question about one limerick at a specific location.\",\n",
    "            \"link\": \"https://github.com/llmonpy/needle-in-a-needlestack\"\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "No useful techs:\n",
    "{\"technology\": []}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        prompt += f\"RSS feed entry:{entry}\"\n",
    "        \n",
    "        return self.llm.invoke(prompt,response_format={\"type\": \"json_object\"})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = RSSTechAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import print_long_text\n",
    "\n",
    "responses = []\n",
    "\n",
    "for i in stripped_feed:\n",
    "    response = agent2.find_interesting_tech_from_feed(i)\n",
    "    if response:\n",
    "        print_long_text(response.content)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    responses.append(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'technology': [{'name': 'coq-of-rust', 'summary': \"coq-of-rust is a tool for translating Rust code to the formal proof system Coq, which is used for formal verification of Rust programs. It has been used to translate the core and alloc crates of Rust, resulting in a 'best effort' translation of these large code bases.\", 'link': 'https://formal.land/blog/2024/04/26/translation-core-alloc-crates'}], 'summary': \"Introduction and explanation of the translation of the Rust's core and alloc crates using the coq-of-rust tool for formal verification.\"}\n",
      "{'technology': [{'name': 'Gemini Flash', 'summary': \"Google DeepMind's Gemini Flash is a lightweight, fast, and cost-efficient AI model featuring multimodal reasoning and a breakthrough long context window of up to one million tokens.\", 'link': 'https://deepmind.google/technologies/gemini/flash/'}]}\n",
      "{'technology': [{'name': 'Veo', 'summary': \"Veo is Google DeepMind's most capable generative video model, capable of producing high-quality, 1080p resolution videos in a wide range of cinematic and visual styles. It interprets text prompts with advanced understanding of natural language and visual semantics.\", 'link': 'https://deepmind.google/technologies/veo/'}], 'useful': True}\n",
      "{'technology': [{'name': 'Needle in a Needlestack', 'summary': 'Needle in a Needlestack is a new benchmark to measure how well large language models (LLMs) pay attention to the information in their context window. It creates a prompt that includes thousands of limericks and asks a question about one limerick at a specific location.', 'link': 'http://nian.llmonpy.ai/'}]}\n",
      "{'technology': [{'name': 'Pipecat', 'summary': 'An open source framework for building voice and multimodal conversational AI', 'link': 'https://github.com/pipecat-ai/pipecat'}]}\n",
      "{'technology': [{'name': 'Cap', 'summary': 'Cap is an open-source alternative to Loom for effortless, instant screen sharing.', 'link': 'https://Cap.so'}]}\n",
      "{'technology': [{'name': 'Thread', 'summary': \"A wireless communication protocol for low-power devices that provides meshed networking and uses IPv6 for communication. It is based on the IEEE 802.15.4 physical layer and can connect to a regular home network via a bridge called 'Border Router'.\", 'link': 'https://overengineer.dev/blog/2024/05/10/thread/'}], 'useful': True}\n",
      "{'technology': [{'name': 'Superfile', 'summary': 'A fancy, pretty terminal file manager.', 'link': 'https://github.com/MHNightCat/superfile'}]}\n",
      "{'technology': [{'name': 'Jam', 'summary': 'A web debugger tool by an ex-Cloudflare team that captures and reports bugs with dev tools + video in a link. It syncs to a video recording of the session and can share an instant replay of a bug. It also writes automatic repro steps and parses GraphQL responses.', 'link': 'https://jam.dev'}], 'useful': True}\n",
      "{\",\n",
      "\": [\",\n",
      "{\", \",\n",
      "\", \"Popover API\", \",\", \",\n",
      "\", \"The Popover API provides developers with a standard, consistent, flexible mechanism for displaying popover content on top of other page content. Popover content can be controlled either declaratively using HTML attributes, or via JavaScript.\", \",\n",
      "\", \"https://developer.mozilla.org/en-US/docs/Web/API/Popover_API\", 0, 1432, 0, 0], \",\n",
      "\": []}\n",
      "[{\"name\": \"Algebraic Data Types for C99\", \"summary\": \"A C99 library that provides safe, intuitive algebraic data types with exhaustive pattern matching and compile-time introspection facilities. Does not require any external tools.\", \"link\": \"https://github.com/Hirrolot/datatype99\"}]\n",
      "{'technology': [{'name': 'OverflowAPI', 'summary': 'A new API developed by Stack Overflow in partnership with OpenAI, providing accurate and verified data from Stack Overflow to OpenAI users.', 'link': ''}]}\n",
      "{'technology': [{'name': 'Consistency LLM', 'summary': 'Consistency Large Language Models (CLLMs) are a new family of parallel decoders that can reduce inference latency by efficiently decoding an $n-token sequence per inference step. This process mimics the human cognitive process of forming complete sentences in mind before articulating word by word.', 'link': 'https://hao-ai-lab.github.io/blogs/cllm/'}], 'useful': True}\n",
      "{'technology': [{'name': 'grafychat', 'summary': 'A Visual notebook for ChatGPT, Google AI, Local Llama 3 and more. Expand ideas on canvas and leverage every AI feature. Bring your own API keys and start chatting.', 'link': 'https://www.grafychat.com'}]}\n",
      "{'technology': [{'name': 'M4 chip', 'summary': \"M4 is a system on a chip (SoC) that advances the industry-leading power-efficient performance of Apple silicon. It features an entirely new display engine, a new CPU with up to 10 cores, a 10-core GPU with next-generation architecture, and Apple's fastest Neural Engine ever.\", 'link': 'https://www.apple.com/newsroom/2024/05/apple-introduces-m4-chip/'}]}\n",
      "{'technology': [{'name': 'Pyspread', 'summary': 'Pyspread is a non-traditional spreadsheet application that is based on and written in the programming language Python. It expects Python expressions in its grid cells, which makes a spreadsheet specific language obsolete.', 'link': 'https://pyspread.gitlab.io/'}], 'useful': True}\n",
      "{'technology': [{'name': 'CQ2', 'summary': 'A free and open-source tool for complex discussions that is in its early stages of development. It aims to make discussions enjoyable and increase productivity.', 'link': 'https://cq2.co/blog/the-best-way-to-have-complex-discussions'}], 'useful': True}\n",
      "{'technology': [{'name': 'SABR', 'summary': \"An AI model that helps visualize the beta/technique for climbing routes based on a user's body parameters. The model creates a virtual avatar of the user's body shape and uses it to climb the route, allowing for comparison and contrast.\", 'link': 'https://www.youtube.com/watch?v=cnvNPWoYZz4'}]}\n",
      "{'technology': [{'name': 'Traefik', 'summary': 'Traefik is a load balancer and reverse proxy that is known for its ability to automatically discover and configure services, such as Docker containers, and its robust and well-thought-out design. It also supports configuration files and can be used outside of container environments.', 'link': 'https://j6b72.de/article/why-you-should-take-a-look-at-traefik/'}], 'useful': True}\n",
      "{'technology': [{'name': 'sqlite-vec', 'summary': \"sqlite-vec is a new SQLite extension for vector search, written purely in C. It will provide custom SQL functions and virtual tables for fast vector search, as well as other tools and utilities for working with vectors. It will be an embeddable 'fast enough' vector search tool, that can run anywhere SQLite runs - including WASM!\", 'link': 'https://alexgarcia.xyz/blog/2024/building-new-vector-search-sqlite/index.html'}], 'useful': True}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for i in responses:\n",
    "    try:\n",
    "        content = json.loads(i.content)\n",
    "        if content and content.get('technology'):\n",
    "            print(content)\n",
    "    except:\n",
    "        print(i.content)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "reearch_journal_feed_url = \"https://feeds.feedburner.com/blogspot/gJZg\"\n",
    "\n",
    "# Parse the RSS feed\n",
    "research_feed: FeedParserDict = feedparser.parse(reearch_journal_feed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_digest_agent = RSSDigestAgent(config='stdout')\n",
    "\n",
    "result = rss_digest_agent.find_useful_keys_in_dict(research_feed.entries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following keys in the JSON may be useful in evaluating the entry for usefulness for a reader to skim:\n",
      "\n",
      "1. 'title': This key provides the title of the post, which can give a quick overview of the content.\n",
      "2. 'published': This key gives the date and time the post was published, which can help the reader determine if the post is relevant to current events.\n",
      "3. 'tags': This key provides a list of tags related to the post, which can give an idea of the topics covered in the post.\n",
      "4. 'summary': This key provides a summary of the post, which can give a more detailed overview of the content.\n",
      "5. 'links': This key provides links to related resources, such as comments or the full post, which can provide more information if the reader is interested.\n",
      "\n",
      "These keys can help a reader quickly determine if a post is relevant and interesting to them, and if they want to read more.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'published': '2024-03-29T11:03:00.000-07:00',\n",
       " 'tags': [{'term': 'Climate',\n",
       "   'scheme': 'http://www.blogger.com/atom/ns#',\n",
       "   'label': None},\n",
       "  {'term': 'Machine Learning',\n",
       "   'scheme': 'http://www.blogger.com/atom/ns#',\n",
       "   'label': None},\n",
       "  {'term': 'Weather',\n",
       "   'scheme': 'http://www.blogger.com/atom/ns#',\n",
       "   'label': None}],\n",
       " 'title': 'Generative AI to quantify uncertainty in weather forecasting',\n",
       " 'summary': '<span class=\"byline-author\">Posted by Lizao (Larry) Li, Software Engineer, and Rob Carver, Research Scientist, Google Research</span>\\n\\n<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s1000/image3.gif\" style=\"display: none;\" />\\n\\n<p>\\nAccurate weather forecasts can have a direct impact on people’s lives, from helping make routine decisions, like what to pack for a day’s activities, to informing urgent actions, for example, protecting people in the face of hazardous weather conditions. The importance of accurate and timely weather forecasts will only increase as the climate changes. Recognizing this, we at Google have been investing in weather and climate research to help ensure that the forecasting technology of tomorrow can meet the demand for reliable weather information. Some of our recent innovations include <a href=\"https://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html\">MetNet-3</a>, Google\\'s high-resolution forecasts up to 24-hours into the future, and <a href=\"https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/\">GraphCast</a>, a weather model that can predict weather up to 10 days ahead.\\n</p>\\n<a name=\"more\"></a> \\n\\n<p>\\nWeather is inherently stochastic. To quantify the uncertainty, traditional methods rely on physics-based simulation to generate an ensemble of forecasts. However, it is computationally costly to generate a large ensemble so that rare and extreme weather events can be discerned and characterized accurately.  \\n</p>\\n<p>\\nWith that in mind, we are excited to announce our latest innovation designed to accelerate progress in weather forecasting, <a href=\"https://www.science.org/doi/10.1126/sciadv.adk4489\">Scalable Ensemble Envelope Diffusion Sampler</a> (SEEDS), recently published in <em><a href=\"https://www.science.org/journal/sciadv\">Science Advances</a></em>. SEEDS is a generative AI model that can efficiently generate ensembles of weather forecasts <em>at scale </em>at a small fraction of the cost of traditional physics-based forecasting models. This technology opens up novel opportunities for weather and climate science, and it represents one of the first applications to weather and climate forecasting of probabilistic diffusion models, a generative AI technology behind recent advances in media generation.\\n</p>\\n<br /> \\n\\n<h2>The need for probabilistic forecasts: the butterfly effect</h2>\\n\\n<p>\\nIn December 1972, at the <a href=\"https://www.aaas.org/\">American Association for the Advancement of Science</a> meeting in Washington, D.C., MIT meteorology professor <a href=\"https://en.wikipedia.org/wiki/Edward_Norton_Lorenz\">Ed Lorenz</a> gave a talk entitled, “Does the Flap of a Butterfly\\'s Wings in Brazil Set Off a Tornado in Texas?” which contributed to the term “<a href=\"https://en.wikipedia.org/wiki/Butterfly_effect\">butterfly effect</a>”. He was building on his earlier, landmark 1963 paper where he examined the feasibility of “very-long-range weather prediction” and described how errors in initial conditions grow exponentially when integrated in time with numerical weather prediction models. This exponential error growth, known as chaos, results in a deterministic predictability limit that restricts the use of individual forecasts in decision making, because they do not quantify the inherent uncertainty of weather conditions. This is particularly problematic when forecasting extreme weather events, such as hurricanes, heatwaves, or floods.\\n</p>\\n<p>\\nRecognizing the limitations of deterministic forecasts, weather agencies around the world issue <em>probabilistic forecasts</em>. Such forecasts are based on ensembles of deterministic forecasts, each of which is generated by including synthetic noise in the initial conditions and stochasticity in the physical processes. Leveraging the fast error growth rate in weather models, the forecasts in an ensemble are purposefully different: the initial uncertainties are tuned to generate runs that are as different as possible and the stochastic processes in the weather model introduce additional differences during the model run. The error growth is mitigated by averaging all the forecasts in the ensemble and the variability in the ensemble of forecasts quantifies the uncertainty of the weather conditions.\\n</p>\\n<p>\\nWhile effective, generating these probabilistic forecasts is computationally costly. They require running highly complex numerical weather models on massive supercomputers multiple times. Consequently, many operational weather forecasts can only afford to generate ~10–50 ensemble members for each forecast cycle. This is a problem for users concerned with the likelihood of rare but high-impact weather events, which typically require much larger ensembles to assess beyond a few days. For instance, one would need a 10,000-member ensemble to forecast the likelihood of events with 1% probability of occurrence with a relative error less than 10%. Quantifying the probability of such extreme events could be useful, for example, for emergency management preparation or for energy traders.\\n</p>\\n<br /> \\n\\n<h2>SEEDS: AI-enabled advances</h2>\\n\\n<p>\\nIn the aforementioned <a href=\"https://www.science.org/doi/10.1126/sciadv.adk4489\">paper</a>, we present the Scalable Ensemble Envelope Diffusion Sampler (SEEDS), a generative AI technology for weather forecast ensemble generation. SEEDS is based on <a href=\"https://blog.research.google/2021/07/high-fidelity-image-generation-using.html\">denoising diffusion probabilistic</a> models, a state-of-the-art generative AI method pioneered in part by Google Research.\\n</p>\\n<p>\\nSEEDS can generate a large ensemble conditioned on as few as one or two forecasts from an operational numerical weather prediction system. The generated ensembles not only yield plausible real-weather–like forecasts but also match or exceed physics-based ensembles in skill metrics such as the <a href=\"https://www.jstor.org/stable/26201352\">rank histogram</a>, the <a href=\"https://en.wikipedia.org/wiki/Root-mean-square_deviation\">root-mean-squared error</a> (RMSE), and the <a href=\"https://www.tandfonline.com/doi/abs/10.1198/016214506000001437\">continuous ranked probability score</a> (CRPS). In particular, the generated ensembles assign more accurate likelihoods to the tail of the forecast distribution, such as ±2σ and ±3σ weather events. Most importantly, the computational cost of the model is negligible when compared to the hours of computational time needed by supercomputers to make a forecast. It has a throughput of 256 ensemble members (at 2° resolution) per 3 minutes on Google Cloud TPUv3-32 instances and can easily scale to higher throughput by deploying more accelerators. \\n</p>\\n\\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s1000/image3.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s16000/image3.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">SEEDS generates an order-of-magnitude more samples to in-fill distributions of weather patterns.</td></tr></tbody></table>\\n<div style=\"line-height: 40%;\">\\n    <br />\\n</div>\\n\\n<h2>Generating plausible weather forecasts</h2>\\n\\n\\n<p>\\nGenerative AI is known to generate very detailed images and videos. This property is especially useful for generating ensemble forecasts that are consistent with plausible weather patterns, which ultimately result in the most added value for downstream applications.  As Lorenz points out, “The [weather forecast] maps which they produce should look like real weather maps.\" The figure below contrasts the forecasts from SEEDS to those from the operational U.S. weather prediction system (<a href=\"https://www.emc.ncep.noaa.gov/emc/pages/numerical_forecast_systems/gefs.php\">Global Ensemble Forecast System</a>, GEFS) for a particular date during the <a href=\"https://en.wikipedia.org/wiki/2022_European_heatwaves\">2022 European heat waves</a>. We also compare the results to the forecasts from a Gaussian model that predicts the univariate mean and standard deviation of each atmospheric field at each location, a common and computationally efficient but less sophisticated data-driven approach. This Gaussian model is meant to characterize the output of pointwise post-processing, which ignores correlations and treats each grid point as an independent random variable. In contrast, a real weather map would have detailed <em>correlational</em> structures. \\n</p>\\n<p>\\nBecause SEEDS directly models the joint distribution of the atmospheric state, it realistically captures both the spatial covariance and the correlation between mid-tropospheric geopotential and mean sea level pressure, both of which are closely related and are commonly used by weather forecasters for evaluation and verification of forecasts. Gradients in the mean sea level pressure are what drive winds at the surface, while gradients in mid-tropospheric geopotential create upper-level winds that move large-scale weather patterns. \\n</p>\\n<p>\\nThe generated samples from SEEDS shown in the figure below (frames Ca–Ch) display a geopotential trough west of Portugal with spatial structure similar to that found in the operational U.S. forecasts or the reanalysis based on observations. Although the Gaussian model predicts the marginal univariate distributions adequately, it fails to capture cross-field or spatial correlations. This hinders the assessment of the effects that these anomalies may have on hot air intrusions from North Africa, which can exacerbate heat waves over Europe.\\n</p>\\n\\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQE94TGK404COMAKKxaPwUO9bD8gIzQfu6A0u5c-5xbGKhlUtBW_0KAj-Ur8kpgt5_f-IjAuFzeecpRbbWVujZNQVExTsl0UuDRtOb84Y8uFWc4G1UYYZos6gLVtIHQ3AZ7ojRqoMSmt8IHdTOSx365AaoNyUfNMi1ksC0Wh_axeD_THB6sOmnZZHhrvHQ/s1999/image2.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQE94TGK404COMAKKxaPwUO9bD8gIzQfu6A0u5c-5xbGKhlUtBW_0KAj-Ur8kpgt5_f-IjAuFzeecpRbbWVujZNQVExTsl0UuDRtOb84Y8uFWc4G1UYYZos6gLVtIHQ3AZ7ojRqoMSmt8IHdTOSx365AaoNyUfNMi1ksC0Wh_axeD_THB6sOmnZZHhrvHQ/s16000/image2.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Stamp maps over Europe on 2022/07/14 at 0:00 UTC. The contours are for the mean sea level pressure (dashed lines mark isobars below 1010 hPa) while the heatmap depicts the geopotential height at the 500 hPa pressure level. (A) The&nbsp;<a href=\"https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5\">ERA5</a>&nbsp;reanalysis, a proxy for real observations. (Ba-Bb) 2 members from the 7-day U.S. operational forecasts used as seeds to our model. (Ca-Ch) 8 samples drawn from SEEDS. (Da-Dh) 8 non-seeding members from the 7-day U.S. operational ensemble forecast. (Ea-Ed) 4 samples from a pointwise Gaussian model parameterized by the mean and variance of the entire U.S. operational ensemble.</td></tr></tbody></table>\\n\\n<div style=\"line-height: 40%;\">\\n    <br />\\n</div>\\n<h2>Covering extreme events more accurately  </h2>\\n\\n<p>\\nBelow we show the joint distributions of temperature at 2 meters and total column water vapor near Lisbon during the extreme heat event on 2022/07/14, at 1:00 local time. We used the 7-day forecasts issued on 2022/07/07. For each plot, we generate 16,384-member ensembles with SEEDS. The observed weather event from ERA5 is denoted by the star. The operational ensemble is also shown, with squares denoting the forecasts used to seed the generated ensembles, and triangles denoting the rest of ensemble members.\\n</p>\\n\\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVbbmrrrJ5L1NVb_O7WPUD-d6ULlTJTSns6ZaqjxOqZ4YAi4zOiT72rfMBf8EGTe0kdofIrWAMESq1m2v9IBjnd_k6UAIDM7LvhbxdVr41FOQ0fqkKeERF_QqXbxs94qKLdMxR-A7Hbxkjd4zZn07AlldAsuvn7jsYCu-V3UVAatovY1ELbrcLQz5I1ppX/s1999/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVbbmrrrJ5L1NVb_O7WPUD-d6ULlTJTSns6ZaqjxOqZ4YAi4zOiT72rfMBf8EGTe0kdofIrWAMESq1m2v9IBjnd_k6UAIDM7LvhbxdVr41FOQ0fqkKeERF_QqXbxs94qKLdMxR-A7Hbxkjd4zZn07AlldAsuvn7jsYCu-V3UVAatovY1ELbrcLQz5I1ppX/s16000/image1.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">SEEDS provides better statistical coverage of the 2022/07/14 European extreme heat event, denoted by the brown star . Each plot shows the values of the total column-integrated water vapor (TCVW) vs. temperature over a grid point near Lisbon, Portugal from 16,384 samples generated by our models, shown as green dots, conditioned on 2 seeds (blue squares) taken from the 7-day U.S. operational ensemble forecasts (denoted by the sparser brown triangles). The valid forecast time is 1:00 local time. The solid contour levels correspond to iso-proportions of the kernel density of SEEDS, with the outermost one encircling 95% of the mass and 11.875% between each level.</td></tr></tbody></table>\\n<br />\\n\\n<p>\\nAccording to the U.S. operational ensemble, the observed event was so unlikely seven days prior that none of its 31 members predicted near-surface temperatures as warm as those observed. Indeed, the event probability computed from a Gaussian kernel density estimate is lower than 1%, which means that ensembles with less than 100 members are unlikely to contain forecasts as extreme as this event. In contrast, the SEEDS ensembles are able to extrapolate from the two seeding forecasts, providing an envelope of possible weather states with much better statistical coverage of the event. This allows both quantifying the probability of the event taking place and sampling weather regimes under which it would occur. Specifically, our highly scalable generative approach enables the creation of very large ensembles that can characterize very rare events by providing samples of weather states exceeding a given threshold for any user-defined diagnostic.\\n</p>\\n<br /> \\n\\n<h2>Conclusion and future outlook</h2>\\n\\n<p>\\nSEEDS leverages the power of generative AI to produce ensemble forecasts comparable to those from the operational U.S. forecast system, but at an accelerated pace. The results reported in this paper need only 2 seeding forecasts from the operational system, which generates 31 forecasts in its current version. This leads to a hybrid forecasting system where a few weather trajectories computed with a physics-based model are used to seed a diffusion model that can generate additional forecasts much more efficiently. This methodology provides an alternative to the current operational weather forecasting paradigm, where the computational resources saved by the statistical emulator could be allocated to increasing the resolution of the physics-based model or issuing forecasts more frequently.\\n</p>\\n<p>\\nWe believe that SEEDS represents just one of the many ways that AI will accelerate progress in operational numerical weather prediction in coming years. We hope this demonstration of the  utility of generative AI for weather forecast emulation and post-processing will spur its application in research areas such as climate risk assessment, where generating a large number of ensembles of climate projections is crucial to accurately quantifying the uncertainty about future climate.\\n</p>\\n<br /> \\n\\n<h2>Acknowledgements</h2>\\n\\n<p>\\n<em>All SEEDS authors, Lizao Li, Rob Carver, Ignacio Lopez-Gomez, Fei Sha and John Anderson, co-authored this blog post, with Carla Bromberg as Program Lead. We also thank Tom Small who designed the animation. Our colleagues at Google Research have provided invaluable advice to the SEEDS work. Among them, we thank Leonardo Zepeda-Núñez, Zhong Yi Wan, Stephan Rasp, Stephan Hoyer, and Tapio Schneider for their inputs and useful discussion. We thank Tyler Russell for additional technical program management, as well as Alex Merose for data coordination and support. We also thank Cenk Gazen, Shreya Agrawal, and Jason Hickey for discussions in the early stage of the SEEDS work. </em>\\n</p>',\n",
       " 'links': [{'href': 'http://blog.research.google/feeds/1569605132526995799/comments/default',\n",
       "   'rel': 'replies',\n",
       "   'title': 'Post Comments',\n",
       "   'type': 'application/atom+xml'},\n",
       "  {'href': 'http://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html#comment-form',\n",
       "   'rel': 'replies',\n",
       "   'title': '0 Comments',\n",
       "   'type': 'text/html'},\n",
       "  {'href': 'http://www.blogger.com/feeds/8474926331452026626/posts/default/1569605132526995799',\n",
       "   'rel': 'edit',\n",
       "   'type': 'application/atom+xml'},\n",
       "  {'href': 'http://www.blogger.com/feeds/8474926331452026626/posts/default/1569605132526995799',\n",
       "   'rel': 'self',\n",
       "   'type': 'application/atom+xml'},\n",
       "  {'href': 'http://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html',\n",
       "   'rel': 'alternate',\n",
       "   'title': 'Generative AI to quantify uncertainty in weather forecasting',\n",
       "   'type': 'text/html'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_research_feed = strip_rss_entries(research_feed.entries,\n",
    "                                           ['title','published','tags','summary','links'])\n",
    "stripped_research_feed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  3.76it/s]\n",
      "C:\\Program Files\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"technology\": []}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  3.10it/s]\n",
      "C:\\Program Files\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"technology\": []}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  3.20it/s]\n",
      "C:\\Program Files\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"technology\": []}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  2.91it/s]\n",
      "C:\\Program Files\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"technology\": []}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  3.24it/s]\n",
      "C:\\Program Files\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"technology\": []}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  2.83it/s]\n",
      "C:\\Program Files\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"technology\": []}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  2.96it/s]\n",
      "C:\\Program Files\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"technology\": []}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  3.06it/s]\n",
      "C:\\Program Files\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"technology\": []}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  3.16it/s]\n",
      "C:\\Program Files\\Python310\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    }
   ],
   "source": [
    "for i in stripped_research_feed:\n",
    "    response = agent2.find_interesting_tech_from_feed(i)\n",
    "    if response:\n",
    "        print_long_text(response.content)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
