{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.basic_agents import BasicAgent\n",
    "from utilities import print_long_text\n",
    "\n",
    "from config.project_paths import synthetic_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook holds a function to run rag, evaluation and requerying \n",
    "\n",
    "code should be folded into an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = BasicAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(name)s-%(levelname)s|%(lineno)d:  %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def reretrieve__toc(question):\n",
    "    documents = agent.retriever._get_relevant_documents(question)\n",
    "\n",
    "    answer = agent.answer_question(\n",
    "        question, documents)\n",
    "\n",
    "    evaluation = agent.evaluate_answer(question, answer)\n",
    "    \n",
    "    \n",
    "    match = re.search(r\"Score \\(1-5\\): (\\d+)\", evaluation.content)\n",
    "\n",
    "    if match:\n",
    "        score = match.group(1)\n",
    "        print(f\"Extracted score: {score}\")\n",
    "\n",
    "        try:\n",
    "            if int(score.strip()) < 4:\n",
    "                new_answer = agent.regenerate_new_answer_from_evaluation(\n",
    "                    question, answer, evaluation)\n",
    "                print('\\n\\n regenerated answer:')\n",
    "                print_long_text(new_answer.content)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    else:\n",
    "        print(\"Score not found in the string.\")\n",
    "\n",
    "    print('\\n\\ndocs:')\n",
    "\n",
    "    for i in documents:\n",
    "        print(i.metadata)\n",
    "        print_long_text(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = synthetic_data_dir / 'search_language_qna'\n",
    "\n",
    "qna_files = list(data_dir.rglob('*.csv'))\n",
    "len(qna_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data_generation.generate_syn_data import csv_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_qna_sample = random.sample(qna_files, 15)\n",
    "\n",
    "qnas = []\n",
    "for i in random_qna_sample:\n",
    "    qnas.extend(random.sample(csv_to_list(i), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.rag_agents import RagAgent\n",
    "\n",
    "\n",
    "rag_agent = RagAgent()\n",
    "responses = []\n",
    "\n",
    "for question, answer in qnas:\n",
    "    print('question:')\n",
    "    print(question)\n",
    "    \n",
    "    print('\\n\\nground truth:')\n",
    "    print(answer)\n",
    "\n",
    "    response = rag_agent.answer_with_rag(question)\n",
    "    responses.append(response)\n",
    "    \n",
    "    print(f'\\n\\ngenerated answer:{response.answer}')\n",
    "    print(f'\\n\\nevaluation score:{response.evaluation_score}')\n",
    "\n",
    "    \n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "t = ''\n",
    "for response, qna in zip(responses, qnas):\n",
    "    for k,v in asdict(response).items():\n",
    "        t += k.capitalize() + '\\n'\n",
    "        t += '-'*40 + '\\n'\n",
    "        if k.lower().strip() == 'documents':\n",
    "            t += '\\n\\n'.join([i.page_content for i in v]) + '\\n\\n'\n",
    "        elif k.lower().strip() == 'evaluation': \n",
    "            t += v.content + '\\n\\n'\n",
    "        elif k.lower().strip() == 'answer':\n",
    "            t += v.content + '\\n\\n'\n",
    "        else:\n",
    "            t += str(v) + '\\n\\n'\n",
    "        \n",
    "    t += 'ground truth answer'.capitalize() + '\\n'\n",
    "    t += qna[1] + '\\n\\n'\n",
    "    \n",
    "    t += '='*100\n",
    "    t += '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.project_paths import project_root\n",
    "\n",
    "o = project_root/'synthetic_data_generation'/'trial_search_engine_lang.txt'\n",
    "o.write_text(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_agent.answer_with_rag('implement eks observability')\n",
    "print_long_text(response.answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.evaluation_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
